[[["c7ca8b35-7232-421a-9c75-41b3b94d83df",{"pageContent":"Cloud Chaser:  Real Time Deep Learning Computer Vision on\nLow Computing Power Devices\nZhengyi Luo, Austin Small, Liam Dugan, Stephen Lane\nDepartment of Computer and Information Science, University of Pennsylvania\nABSTRACT\nInternet of Things(IoT) devices, mobile phones, and robotic systems are often denied the power of deep learning\nalgorithms due to their limited computing power.  However, to provide time critical services such as emergency\nresponse,  home assistance,  surveillance,  etc,  these devices often need real time analysis of their camera data.\nThis paper strives to offer a viable approach to integrate high-performance deep learning based computer vision\nalgorithms  with  low-resource  and  low-power  devices  by  leveraging  the  computing  power  of  the  cloud.   By  of-\nfloading the computation work to the cloud, no dedicated hardware is needed to enable deep neural networks on","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":1,"lines":{"from":1,"to":11}}}}],["7ac7433d-88c4-407c-9ab3-3b57f37b75ab",{"pageContent":"floading the computation work to the cloud, no dedicated hardware is needed to enable deep neural networks on\nexisting low computing power devices.  A Raspberry Pi based robot, Cloud Chaser, is built to demonstrate the\npower of using cloud computing to perform real time vision tasks.  Furthermore, to reduce latency and improve\nreal time performance, compression algorithms are proposed and evaluated for streaming real-time video frames\nto the cloud.\nKeywords:  Deep Learning, Computer Vision, real-time computing, Cloud Computing, Mobile Computing\n1. INTRODUCTION\nWith  the  growing  success  of  deep  learning  in  the  field  of  computer  vision,  it  is  natural  for  developers  and\nresearchers alike to be interested in deploying these new vision algorithms in devices such as smart home cameras,\nrobots, drones, etc.  However, popular computer vision algorithms that leverage deep neural nets often require","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":1,"lines":{"from":11,"to":20}}}}],["38ab9d22-7305-4630-845f-dd09b457b4b1",{"pageContent":"robots, drones, etc.  However, popular computer vision algorithms that leverage deep neural nets often require\nhigh end GPUs (Graphics Processing Units) to achieve desired performance.  This constraint heavily limits the\nnumber of algorithms researchers can experiment with. To overcome this problem, researchers have tried different\napproaches, such as designing more efficient deep learning networks like MobileNet,\n1\nmaking computing add-on\nmodules like Intel Movidius,\n2\nor creating dedicated chips and processors such as NeuPro by CEVA\n3\nThe idea of using Cloud infrastructure to aid low resource devices,  is first introduced by James Kuffner,\n4\nhas  inspired  a  number  of  research  projects\n5\n.\n6\nAs  more  and  more  Cloud  Computing  services  provide  deep\nlearning  frameworks,  deep  learning  algorithms  are  becoming  more  accessible,  especially  when  GPU  enabled\ncloud machines provided by Amazon AWS, Google Cloud, Paperspace etc.  are offering substantial computing","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":1,"lines":{"from":20,"to":38}}}}],["d9006d3e-7f59-4c93-b035-66fa203b0329",{"pageContent":"cloud machines provided by Amazon AWS, Google Cloud, Paperspace etc.  are offering substantial computing\npower at affordable prices.  Moreover, these cloud machines tend to have relatively high-bandwidth networks,\nmaking them suitable for real-time applications.  For instance, the Nvidia Gaming as a Service (GaaS)\n7\noffers\nplayers who do not have access to high-end GPUs the opportunity to run their games in the cloud and render\nthem on their personal devices.  In this paper, we are interested in using these resources to enable low resource\ndevices to run computationally intensive real-time computer vision algorithms.  Specifically, object detection is\nchosen as the task for investigation.\nAs more and more internet of things, robotics, and mobile devices become equipped with cameras,  object\ndetection will be increasingly important as it is the foundation on which higher level tasks such as navigation,","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":1,"lines":{"from":38,"to":48}}}}],["a7fe010f-6ab3-425f-8aac-2de113d38aef",{"pageContent":"detection will be increasingly important as it is the foundation on which higher level tasks such as navigation,\nplanning, and surveillance can be built.  State of the art object detection algorithms all heavily rely on powerful\nGPUs to achieve a desirable accuracy and frame-rate.  In order to achieve similar performance in low computing\npower devices, we offload the computing tasks to a GPU enabled cloud instance.\nThis paper makes the following contributions:\n•Proposing a software architecture to allow resource constrained devices to run state of the art deep learning\nobject detection algorithms that require a GPU to achieve real time performance.\narXiv:1810.01069v2  [cs.CV]  8 Nov 2020","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":1,"lines":{"from":48,"to":55}}}}],["eb8e340c-12bf-4f8c-a686-738862df44a3",{"pageContent":"•Utilizing multi-thread and asynchronous programming to coordinate real-time video streaming and object\ndetection between cloud and local devices.\n•Designing and evaluating compression algorithms to reduce latency induced by offloading computing work\nto the cloud.\nThe paper is organized as follows:  section II reviews the current research in the field of deep learning for\nobject  detection  and  other  efforts  in  running  deep  neural  nets  on  low  resource  devices.   Section  III  describes\nthe research objective and problem formulations.  Section IV gives the system architecture, and Section V will\ndescribe  our  technical  approach  and  methodology.   System  experiments  and  performance  analysis  will  be  in\nsection VI, while section VII concludes the paper.\n2. RELATED WORKS\n2.1  Deep Learning Object Detection\nObject  Detection,  different  from  image  classification,  is  the  task  of  detecting  possible  objects  in  the  current","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":2,"lines":{"from":1,"to":12}}}}],["5b7ce767-569c-45a3-948f-17de88c5427f",{"pageContent":"2. RELATED WORKS\n2.1  Deep Learning Object Detection\nObject  Detection,  different  from  image  classification,  is  the  task  of  detecting  possible  objects  in  the  current\nimage frame,  as well as producing a bounding box that indicates the location of the object.  State of the art\nobject detection algorithms consists of two main approaches:  region Based, such Regional-based convolutional\nnetwork  (R-CNN)\n8\nand  Fast-RCNNs,\n9\nand  single-shot  based  You  only  look  once  (YOLO)\n10\nand  Single  shot\nmultibox Detector (SSD).\n11\nFor regional proposal based approaches,  they often consist of two steps:  regional\nproposal and object classification.  At first, the potential objects’ locations (bounding box) in the current scene\nare formulated, then a deep neural network such as a Convolutional Neural Network (CNN) are used to predict\nthe objects’ classes.  R-CNN,\n8\nwas the first model to adopt this procedure for object detection.  Fast R-CNN\n9","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":2,"lines":{"from":10,"to":30}}}}],["5613c8d7-020e-4d67-bbf2-9b108d788d7f",{"pageContent":"the objects’ classes.  R-CNN,\n8\nwas the first model to adopt this procedure for object detection.  Fast R-CNN\n9\nspeeds up the R-CNN model by using a single network for the whole image rather than dedicating a separate one\nto each region.  Faster R-CNN\n12\nbrings in near real-time performance at 5 FPS frame rate on a GPU. However,\nthe separation of the object detection problem into both a regional proposal stage and classification stage results\nin complicated pipelines that significantly slow down the algorithm.\nYOLO, a state of the art object detection algorithm that was introduced in 2015, pioneered the approach to\nunify the steps of regional proposal and object classification.\n13\nDue to its simple and effective architecture, YOLO\ncan achieve comparable accuracy at more than 30 FPS. Since YOLO was first released, several improvements in\nspeed and accuracy have since been realized in YoloV3.\n14","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":2,"lines":{"from":27,"to":43}}}}],["8e85918e-4210-45d9-8b03-22d68dafc6a5",{"pageContent":"can achieve comparable accuracy at more than 30 FPS. Since YOLO was first released, several improvements in\nspeed and accuracy have since been realized in YoloV3.\n14\nThe advent of these real time object detection algorithms has encouraged numerous researchers to attempt\nto integrate them into real-time systems.  However,  all of these algorithms rely on powerful GPUs to achieve\nreal-time performance.  For instance, to achieve real time performance, the YOLO algorithm requires 4 GB of\nGPU Random-access memory (RAM), which is only available on higher gaming PCs and Laptops.  On typical\nresource constraint devices such as mobile phones and IoT devices, most of the time discrete GPU is unavailable.\n2.2  Real Time Computer Vision On Resource Constraint Devices\nSeveral  Neural  Network  based  approaches  have  been  devised  to  cater  towards  real  time  object  detection  on\nresource constrained platforms. Many of these approaches either compress a pre-trained network like\n15","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":2,"lines":{"from":41,"to":52}}}}],["b446c9d9-724d-49f4-a18c-11a3b170ebe7",{"pageContent":"resource constrained platforms. Many of these approaches either compress a pre-trained network like\n15\nor directly\ntrain a small network such as.\n16\nFor instance, MobileNet proposes the use of depth-wise separable convolutions\nin order to reduce computation and model size by reducing the complexity of the convolution operation.\n1\nWhile\nthese works do reduce space and computational complexity, a significant trade off in accuracy is present.\nThough there are a few online machine learning service provider like\n1718\n,\n19\nexisting services focus on image\nanalysis and object detection for business purposes.  These services mainly depends on deriving business insight\nderived from image analysis, so they do not provide object detection in a desired frame rate that is suitable for\nreal time tasks.\nWith state of the art object detection unable to run in real time on low resource platforms without com-","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":2,"lines":{"from":51,"to":69}}}}],["891cdfa0-d0d4-4c97-b11f-cd749edd8633",{"pageContent":"real time tasks.\nWith state of the art object detection unable to run in real time on low resource platforms without com-\npromising on accuracy,  an opportunity exists to offload the computing workload to the cloud.  Existing work\nattempts to measure and contrast the performance of state of the art object detection algorithms as it pertains to\nreal time object tracking with aerial drones.\n20\nWhile this work can act as a proof of concept, practical concerns","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":2,"lines":{"from":68,"to":74}}}}],["624e7d3b-f3f9-4362-acf6-b935cb9bb2c5",{"pageContent":"such as selection of communication protocols, image compression, and threading of tasks on the mobile platform\nto minimize communication latency all require significant investigation before cloud computing becomes a viable\nmainstream solution for real time object detection.\n3. RESEARCH OBJECTIVE AND PROBLEM FORMULATION\nTo  enable  deep  learning  based  computer  vision  on  resource  constrained  devices,  we  leverage  the  computing\npower of the cloud.  Using the cloud as a secondary computing unit has many advantages.  First of all, cloud\ncomputing allows any device with basic WiFi capabilities to run computationally intensive deep learning neural\nnetworks.  This is especially useful as popular high performance vision libraries often need a significant amount\nof GPU power to produce satisfying results.  Another advantage is the fact that incorporating a cloud computing","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":3,"lines":{"from":1,"to":9}}}}],["54e22e5c-360f-49bb-8e0f-c9d845c33392",{"pageContent":"of GPU power to produce satisfying results.  Another advantage is the fact that incorporating a cloud computing\ncomponent  requires  minimal  additional  setup  but  can  drastically  improve  the  device’s  capabilities.   The  only\nhardware requirement for using cloud computing in an existing device will be a reliable WiFi module.\nThus, our goal is to provide a way to integrate deep learning computer vision algorithms into low resource\ndevices with minimal effort using cloud technologies. We are assuming the constraint of a computing unit that has\nthe capability of communicating through WiFi but has limited bandwidth and little to no on-board computing\npower with which to run highly parallel deep learning algorithms.  To offload real time computer vision tasks to\nthe cloud, a low-latency communication architecture is designed, implemented, and tested.  In order to test the\nperformance of the system, we have selected object detection tasks as the main objective for this work.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":3,"lines":{"from":9,"to":17}}}}],["23b0bca8-27fe-4263-962d-50f85685a0dd",{"pageContent":"performance of the system, we have selected object detection tasks as the main objective for this work.\n4. SYSTEM ARCHITECTURE\nFigure 1. System Architecture\nTo enable offloading the heavy computation tasks to the cloud, a novel architecture is proposed, as shown\nin Fig.  1.  The architecture consists of two main parts:  the client side, which resides on the mobile device and\nhandles the image data transmission,  and the server side,  which runs on the cloud instance and executes the\nobject detection algorithms.\n4.0.1  Server Side\nThe server side is made of three main components, the Asyncio Server, the Local Asyncio Rebroadcast Server,\nand the Deep Learning Object Detector.\n•Asyncio Server:  runs two separate websocket servers for communicating with the client.  The first server\nreceives the incoming JPEG stream from the device,  while the second server reads the results from the\nobject detector and sends data to the client device.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":3,"lines":{"from":17,"to":29}}}}],["ecab1199-7aaa-4080-b39c-fd10bebc1df6",{"pageContent":"receives the incoming JPEG stream from the device,  while the second server reads the results from the\nobject detector and sends data to the client device.\n•Local Asyncio Rebroadcast Server:  receives the JPEG stream from the Asyncio Server and rebroadcasts\nthe images using MJPEG protocol.\n•Deep  Learning  Object  Detector:  analyzes  the  rebroadcast  MJPEG  frames,  and  uses  YOLO-based  deep\nlearning object detection algorithms to recognize objects in the current frame.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":3,"lines":{"from":28,"to":33}}}}],["52874206-8dca-44fa-8538-ff5d380bb7fb",{"pageContent":"4.0.2  Client Side\nThe client side is made of four main components, the OpenCV Camera Capture component, the on device image\ncompression component (Local Compression), the Asyncio Client component, and the device Action component.\n•OpenCV Camera Capture:  uses OpenCV library to capture image output from the device camera.\n•Local  Compression:  processes  raw  image  data  from  camera  and  compresses  it  to  reduce  file  size,  then\nencodes the raw image data into JPEG images.\n•Asyncio Client:  runs two separate websocket clients and communicates with the server.  The first client\nsends the image data to the server continuously, and the second receives object detection data.\n•Action:  reacts to the detected object in the scene.\n4.0.3  Pipeline Workflow Overview\nOur proposed pipeline contains the above components and uses multiple threads to ensure parallel execution.\nEach image frame captured by OpenCV Camera Capture from the device camera is compressed by Local Com-","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":4,"lines":{"from":1,"to":12}}}}],["2987c63c-1130-4cb7-a90f-2b77bb706305",{"pageContent":"Each image frame captured by OpenCV Camera Capture from the device camera is compressed by Local Com-\npression.   Then  the  compressed  image  is  encoded  in  JPEG  format  and  sent  to  the  server  by  the  websocket\nthrough the Asyncio Client.  After the cloud Asyncio Server receives the image frame, it shares the image with\nLocal Asyncio Rebroadcast Server for rebroadcasting the frame as a MJPEG stream.  The Deep Learning Object\nDetector accepts the image frames and detect the objects in it.  After detection is finished, the cloud Asyncio\nServer reads it from the object detection algorithm, and sends the detection as a string through websocket to\nthe client.  The device Asyncio Client receives the object information and sends it to the Action component for\nreaction.\n5. TECHNICAL APPROACH\n5.1  Real Time Video Streaming and Compression\nTo  stream  real  time  video  frames  to  the  cloud  from  a  local  device  to  the  cloud  for  analysis,  we  propose  the","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":4,"lines":{"from":12,"to":22}}}}],["c52b4fd8-5b31-4d51-8002-e1866f8b5ea6",{"pageContent":"5. TECHNICAL APPROACH\n5.1  Real Time Video Streaming and Compression\nTo  stream  real  time  video  frames  to  the  cloud  from  a  local  device  to  the  cloud  for  analysis,  we  propose  the\nfollowing approaches to the corresponding challenges:\n5.1.1  Communication Protocol\nTo enable real-time performance, the communication protocol needs to be low level enough that it introduces\nminimal  latency.   Specifically,  we  chose  websocket  as  our  main  communication  protocol.   Another  option  to\nensure the lowest possible network latency is raw TCP, but the direct use of TCP has limited speed advantage.\nWebsocket allows multiple connections to a single server, and is easy to use in an existing web framework like\nthe python aiohttp library.  Since websocket ensures easy interfacing with the rest of the application, while also\nproviding a more secure interface for the streamed video frames (WSS protocol encrypts streams using HTTPS)\nit was the best choice for our use case.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":4,"lines":{"from":20,"to":31}}}}],["7d220bb9-8829-46af-93d6-7f076bbead6c",{"pageContent":"providing a more secure interface for the streamed video frames (WSS protocol encrypts streams using HTTPS)\nit was the best choice for our use case.\n5.1.2  Network Address Translation\nIn  order  to  stream  a  MJPEG  video  stream  through  the  network,  existing  programs  usually  set  up  a  HTTP\nweb server on local devices and access the video stream through the device’s assigned IP address.  However, for\nsecurity reasons, Internet of Things devices and mobile devices are often assigned private IP addresses that are\nunreachable from outside of the private network.  Naturally, a cloud machine is outside of the local network, thus\nmaking traditional streaming schemes unusable for our purposes.\nSome  users  with  administrative  privileges  on  their  routers  can  circumvent  this  problem  through  enabling\nport forwarding, which is a router setting that allows a user to open up specific ports on their routers to allow","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":4,"lines":{"from":30,"to":39}}}}],["7b66ed69-cc01-48a0-baf5-a184b754d7a4",{"pageContent":"port forwarding, which is a router setting that allows a user to open up specific ports on their routers to allow\npublic  access.   However,  this  approach  is  not  possible  on  public  networks  where  common  users  do  not  have\nadministrative privileges.  Additionally, some routers do not support port forwarding.\nTo make the device’s video stream accessible from the cloud machine, as well as to avoid running a separate\nMJPEG  server  on  our  device,  our  system  proposes  the  following  solutions:  instead  of  setting  up  a  MJPEG","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":4,"lines":{"from":39,"to":43}}}}],["8c035d1c-23b8-4161-8142-f19397a92b87",{"pageContent":"server directly on the device, camera frames captured from the device are sent to the server via an established\nwebsocket connection between server and device.  Local devices, though not accessible from public networks, can\nestablish bi-directional websocket connections with the server.  Thus, the client can send video frames through\nthe websocket directly to the server, and the server can then rebroadcast the frames using the MJPEG protocol,\nwhich is then consumed by object detection algorithms.\n5.1.3  Network Latency and Compression\nDue to the limited bandwidth of WiFi networks, we propose several compression approaches in order to reduce\nthe size of the data transmitted and reduce latency.\n•Camera resolution:  by reducing the camera capturing resolution from 960 * 540 to 480 * 270, we cut down\nthe number of pixels transmitted by a factor of 4.\n•Blurring the image:  by applying a blur effect on the image, we improve the JPEG compression algorithm’s","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":5,"lines":{"from":1,"to":11}}}}],["6dcfbc69-0942-4fbd-9fd4-ed70ecff6342",{"pageContent":"the number of pixels transmitted by a factor of 4.\n•Blurring the image:  by applying a blur effect on the image, we improve the JPEG compression algorithm’s\nefficiency, and effectively cut down the data size transmitted.\n•Blacking  out:  by  using  the  temporal  data  from  the  previously  processed  frame,  we  are  able  to  identify\nknown objects and black out all regions of the image which do not contain objects.  This approach assumes\nthat objects do not change their relative position drastically between frames, and by leaving a buffer region\non the object bounding box (15px margin), the detection algorithm can still identify the existing objects\nin the new frame.  To detect new objects that may appear in the current scene, a complete, not blacked\nout frame is periodically sent for detecting new objects.  File size reduction and latency reduction from the\nabove approaches are outlined in the experiment section.\n5.2  Asynchronous and Multi-threaded Computing","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":5,"lines":{"from":10,"to":20}}}}],["0dcba4f8-d14a-4d1c-8481-4938fa64c4b9",{"pageContent":"above approaches are outlined in the experiment section.\n5.2  Asynchronous and Multi-threaded Computing\nIn our architecture, the local device needs to send a video stream continuously while receiving detection results\nfrom previous frames.  The cloud service also receives and rebroadcasts each frame to localhost for deep learning\nobject detection algorithms.  On top of that, the cloud needs to start the object detection process on demand\nand read results from the process.  To achieve these goals, asynchronous programming and multi-threading are\nheavily utilized in our system.\nOn  the  cloud/server  side,  three  different  threads  run  in  parallel.   One  thread  runs  a  websocket  server  for\nreceiving  the  video  stream.   Within  this  thread,  the  rebroadcast  server  asynchronously  broadcasts  the  video\nstream, exposing it as a MJPEG stream on localhost. These two processes communicate through shared variables.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":5,"lines":{"from":19,"to":28}}}}],["f0ab3cb1-0032-40b0-8d22-c408831e9355",{"pageContent":"stream, exposing it as a MJPEG stream on localhost. These two processes communicate through shared variables.\nA second thread runs a websocket server which starts the deep learning object detector as the third thread.  This\nwebsocket server then reads the output from the deep learning object detector, and sends it back to the client.\nOn the client/device side, three different threads are also initiated to ensure parallel computing.  The first\nthread runs a websocket client dedicated to capturing, compressing, and sending a continuous real-time video\nstream to the server.  The second thread also hosts a websocket client that receives the object detection result\nfrom the server in a non-ending loop.  Finally, a third thread reads the information from the second websocket\nthread, and reacts to the results from the cloud object detector.\n5.3  Deep Learning Object Detection\nDeep learning based object detection algorithms have gained significant traction due to their rapidly improving","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":5,"lines":{"from":28,"to":37}}}}],["c5c0ee5d-f92b-443d-80f5-ccff8bf425e5",{"pageContent":"5.3  Deep Learning Object Detection\nDeep learning based object detection algorithms have gained significant traction due to their rapidly improving\nperformance on some of the most well-known image classification datasets such as ImageNet and COCO. For\nthis work, we mainly focus on adapting the YOLO\n13\nobject detection algorithm to a cloud computing friendly\nconfiguration, which is shown as the Deep Learning Object Detector in Fig.  1.\nTaking an input frame from the MJPEG stream provided by the Local Asyncio Rebroadcast Server as shown\nin Fig.  1, the image frame will first be resized to fit the network parameters, and then go through the following\ncalculations to predict the current objects and their location in the scene.  The problem formulation is as follows.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":5,"lines":{"from":36,"to":45}}}}],["6ba3e1fc-f330-49e3-87de-f1fc1252bf3f",{"pageContent":"Figure 2. Neural Network Architecture for Deep Learning Object Detector\n1)  An  image  is  broken  up  into  an  SxS  grid  of  cells  (S  =  7  in  our  case),  where  each  cell  is  responsible  for\nproducing B (B = 5) bounding boxes.  A bounding box represents a potential object in the scene.  If the\ncenter of an object lies within a cell, that cell is responsible for generating the object’s bounding box.\n2)  Each cell predicts the conditional probabilityP r(Class\ni\n|Object) which represents the probability that the\nobject in a grid cell is ofClass\ni\n, given that an object is present in the cell.\n3)  For each bounding box, five predictions, (x, y, w, h, c) are produced, where (x, y) represents the center of\nthe predicted bounding box of an object, (w, h) represents the width and height of the bounding box, and\nc represents the confidence.  The confidence score c is calculated as formula (1), where c represents object","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":6,"lines":{"from":1,"to":13}}}}],["5cf98ee2-b0f3-4795-812c-db94a1508574",{"pageContent":"c represents the confidence.  The confidence score c is calculated as formula (1), where c represents object\nclass specific confidence in a bounding box, meaning how likely a bounding box is to contain class i as well\nas how well the bounding box fits the object.\nc=P r(Class\ni\n|Object)∗P r(Object)∗IOU\ntruth\npred\n=P r(Class\ni\n)∗IOU\ntruth\npred\n(1)\nwhere IOU stands for Intersection over Union, which is the area of overlap divided by the area of union\nbetween the detected bounding box and the ground truth bounding box.\nAn image frame goes through a Convolutional Neural Network (CNN) shown in Fig.  2 to predict the (x, y,\nw, h, c) as formulated above.  The neural network contains 24 convolutional layers and then 2 fully connected\nlayers.  This network was inspired by GoogleLeNet model for image classification.\n21\nUnlike GoogLeNet, 1×1\nreduction layers followed by 3×3 convolutional layers are used.  The output of the CNN is then fed into to the","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":6,"lines":{"from":13,"to":34}}}}],["4f411c25-4029-4528-9e2d-7b6962406aa5",{"pageContent":"21\nUnlike GoogLeNet, 1×1\nreduction layers followed by 3×3 convolutional layers are used.  The output of the CNN is then fed into to the\nAsyncio Server, where it will be transferred to the client.\nFor our application, the Deep Learning Object Detector is trained on the Microsoft COCO dataset, which\ncontains pictures of 80 objects.  The object detector is run as a subprocess of the server side program, as shown\nin Fig.  2, and the output of the detector is framed as (x, y, c, w, h).  The confidence threshold is set at 50%.\nEssentially, for each detected object in each frame that has a confidence greater than 50%, the subprocess will\noutput a text line giving its prediction.\nThe  advantage  of  using  this  construction  lies  in  its  speed  and  accuracy.   By  unifying  the  step  of  regional\nproposal and object classification, the final classification can be computed by one pass of the network, making","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":6,"lines":{"from":32,"to":42}}}}],["f7a55481-4f2b-485d-aeaf-2281041a3a30",{"pageContent":"proposal and object classification, the final classification can be computed by one pass of the network, making\nthe algorithm applicable to real-time applications with state of the art accuracy. Combining the high performance\nof YOLO with our cloud computing system, resource constrained devices can make use of this object detection\nsystem with minimal additional setup or hardware.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":6,"lines":{"from":42,"to":45}}}}],["f1a09851-3fee-4b38-aaf2-28d73cad67ca",{"pageContent":"Figure 3. Cloud Chaser and cloud based real time object detection.  The right image showcases a screenshot of real time\nobjects being recognized from the Paperspace cloud instance.\nTable 1. Time for round trip websocket communication, data size 26 bytes, taking the average of 2000 round trips\nTiming DifferenceSunnyvale to San\nFrancisco (41.5 mi)\nSunnyvale to New York\n(2,937.8 mi)\nTCP0:00:00.0230890:00:00.092086\nWebsocket0:00:00.0246190:00:00.094308\n6. EXPERIMENTS\nCloud  Chaser,\n22\nshown  in  Fig.   3.   was  built  to  demonstrate  the  viability  of  our  approach.   It  is  our  custom\nRaspberry Pi based robot capable of following voice commands, recognizing 80 different kinds of common objects\nin real time, and tracking these objects in real-time (thus the name ”Cloud Chaser”).\nUsing Cloud Chaser, three sets of experiment are conducted to demonstrate the viability of our approach.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":7,"lines":{"from":1,"to":16}}}}],["529e80db-30cc-4123-b0b2-93dec3382a6a",{"pageContent":"in real time, and tracking these objects in real-time (thus the name ”Cloud Chaser”).\nUsing Cloud Chaser, three sets of experiment are conducted to demonstrate the viability of our approach.\nFirst, the communication latency introduced by offloading critical computation to the cloud is timed and calcu-\nlated.  Further, we timed the compression algorithms that are intended to reduce communication latency.  Three\ncompression algorithms are considered:  averaging the captured image without reducing resolution, reducing cap-\ntured image resolution, and blacking out parts of the image that are considered ”uninteresting”.  Last but not\nleast,  to demonstrate the adaptability of our platform,  an iOS app is developed to showcase our architecture\nrunning on different devices.\n6.1  Experiment Settings\nOur cloud instance is a Paperspace GPU+ instance, equipped with Nvidia Quadro P4000.  Cloud Chaser, our\ncustom built robot shown in Fig.  3, is built from a Raspberry Pi 3 B.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":7,"lines":{"from":15,"to":25}}}}],["1f4e0570-e797-484e-a50e-e1a8188f0377",{"pageContent":"6.1  Experiment Settings\nOur cloud instance is a Paperspace GPU+ instance, equipped with Nvidia Quadro P4000.  Cloud Chaser, our\ncustom built robot shown in Fig.  3, is built from a Raspberry Pi 3 B.\n6.2  Real-Time Communication Latency\nThe latency induced by enabling our program can be broken into three parts:  latency introduced by traveling\nthrough physical space when sending packets, latency introduced by limited bandwidth when sending data to\ncloud instance and sending back prediction, and the time it took for YOLO to process the image and produce\nprediction.  We tested these latencies separately and in combination.\nTable 1 makes clear the time for a small packet to be sent from the local device (Raspberry Pi) to the server\nby calculating the round trip delay.  From the data we conclude that using websocket against TCP introduces\nnegligible additional delay(1ms) compared to the delay introduced by YOLO running at 15 FPS (60ms).","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":7,"lines":{"from":23,"to":33}}}}],["16249c2b-57a5-4233-ac57-866664658c44",{"pageContent":"negligible additional delay(1ms) compared to the delay introduced by YOLO running at 15 FPS (60ms).\nTable 2. Time for round trip websocket communication, data size 13500 bytes, taking the average of 2000 round trips\nTiming DifferenceSunnyvale to San\nFrancisco (41.5 mi)\nSunnyvale to New York\n(2,937.8 mi)\nWebsocket0:00:00.0766120:00:00.210735","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":7,"lines":{"from":33,"to":39}}}}],["0bfa2a55-611f-47cf-8875-31fe67eb1a09",{"pageContent":"Table 3. Time between frame of person showing up and detection\nTimeTotal DelayTimeTotal Delay\n8:00 AM0:00:00.5622036:00 PM0:00:00.651300\n11:00 AM0:00:00.6879199:00 PM0:00:00.756316\n3:00 PM0:00:00.58543512:00 AM0:00:00.494050\nFigure 4. Prediction By Different Compression Approaches\nTo time the latency introduced by limited bandwidth, we encoded a string of length 13500 bytes, which is\nthe average size of a 480 * 270 byte JPEG image.  The delay introduced by sending this data packet through the\nnetwork is shown in Table 2.\nTo time the combined latency of object detection and network communication, a picture of a person shown\nin Fig.  4(a).  is used as a token.  First the camera is facing a black plane where no object is detected in the scene.\nAfter a few seconds the program swaps out the camera feed for the JPEG picture which is read from memory\n(counting the time to recompress the picture into JPEG format) and sent for detection.  This effectively simulates","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":8,"lines":{"from":1,"to":13}}}}],["4a48f83b-3393-4d92-84bd-7341c1ca43f0",{"pageContent":"(counting the time to recompress the picture into JPEG format) and sent for detection.  This effectively simulates\nthe procedure of capturing, sending, and detecting a person in the current scene.  The time when YOLO first\ndetects the presence of a person is recorded.  The results from five different times of day are shown in Table 3.\nWhile YOLO can run at an average frame rate of 15 FPS on our machine, the additional delay time between the\npicture event occurring on the local machine and cloud detection is caused by the following factors:  1) Network\nCongestion, where the cloud instance can not process the image frames sent from local devices as quickly as the\nlocal devices can send them, and 2) The video compression and decompression time taken by OpenCV on both\nthe cloud instance and local device.  Overall, by offloading the object detection from the local device, less than\nhalf of a second delay is introduced on the original performance of the object detection algorithm.  Equipped","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":8,"lines":{"from":13,"to":21}}}}],["f026bbf4-f207-4b7c-8e1e-9662dc1da111",{"pageContent":"half of a second delay is introduced on the original performance of the object detection algorithm.  Equipped\nwith better GPU enabled cloud instance, the latency can be reduced even further.\n6.3  Compression Algorithms Evaluation\nIn order to reduce the bottle-neck of transmission, we tried a few different compression schemes to reduce the\namount of data that needed to be transmitted.\n6.3.1  Averaging\nApplying an averaging filter to the picture has the effect of reducing the sharpness of the picture, as shown in\nFig.4(b).  The effect on file size and latency reduction of applying an averaging filter with each pixel weighted by\n0.04 and averaging every 5 by 5 grids is shown in Table 4.  Even though the image remains the same resolution\nTable 4. Image resolution 480 * 270, file size in bytes\nCompressed (averaging)Original\nLatency0:00:00.5225360:00:00.651300\nAverage File Size1498917740","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":8,"lines":{"from":21,"to":33}}}}],["8a935b2c-081e-4f48-8855-182503180168",{"pageContent":"Table 5. Image resolution 960 * 540 vs 480 * 270, file size in bytes\n960 * 540480 * 270\nLatency0:00:00.5225360:00:00.651300\nAverage File Size7319817740\nTable 6. Image size original vs blacked out 480 * 270, file size in bytes\nOriginalBlacked Out\nLatency0:00:00.6513000:00:00.651300\nAverage File Size1774014418\nafter  averaging,  a  blurred  image  will  benefit  more  from  JPEG  compression  and  will  thus  take  less  data  to\ntransmit.\nAs a side effect of blurring this image we observe a reduction in accuracy on the our object detector.  Our\nexperiment showed that blurring using this kernel will reduce the mAP (Mean Average Precision) for YOLOv3\nfrom 0.7465 to 0.6426 on the 2007 Pascal VOC dataset.\n6.3.2  Reduction on Resolution\nReducing the resolution from 960 * 540 to 480 * 270 results in both a file size reduction and latency reduction\nas shown in Table 5.  The affect of this reduction is shown in Fig.4(c).\n6.3.3  Blacking out","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":9,"lines":{"from":1,"to":17}}}}],["cd985357-ebc3-416c-9b4d-c35a59c4ea40",{"pageContent":"as shown in Table 5.  The affect of this reduction is shown in Fig.4(c).\n6.3.3  Blacking out\nThe size reduction effect of blacking out an image is shown in Table 6.  As shown in Fig.  4(d), blacking out the\nuninteresting part of the image still results in the correct detection of the object in the current frame, assuming\nthe location of objects do not change drastically between frames.\n6.4  Platform Adaption\nTo test the viability of this approach on other platforms, an iOS app is developed.  Combined with the spacial\nmapping capability of the ARkit, the app demonstrates the ability to identify and label objects in a scene in real\ntime.  To demonstrate the real time object localization capability of the app, please refer to our short video.\n23\nThrough these experiments, we have demonstrated that using the GPU-enabled cloud machines to carry out\nthe deep learning object detection tasks is a viable,  easy to setup,  and easy to adapt approach.  A variety of","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":9,"lines":{"from":16,"to":27}}}}],["0090c70b-c32f-4913-b334-054d296fface",{"pageContent":"the deep learning object detection tasks is a viable,  easy to setup,  and easy to adapt approach.  A variety of\ndevices and platforms will be able to implement this architecture and make use of state of the art deep leaning\nobject detection algorithms in real time.\n7. CONCLUSION AND FUTURE WORK\nIn  this  work,  we  present  a  novel  architecture  to  integrate  existing  deep  learning  computer  vision  algorithm\nto  resource  constraint  devices.   As  opposed  to  designing  a  new  neural  network  that  can  be  run  on  resource\nconstraint devices, this approach runs the existing state of the art neural networks in the cloud and utilizes the\nwireless networking capabilities of the device to ensure real time performance.  Since the full neural network is\nrun in the cloud, there is little compromise on accuracy.  Also, by utilizing multi-threading and asynchronous\ncomputing, coupled with compression algorithms for image size reduction, the latency introduced by streaming","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":9,"lines":{"from":27,"to":36}}}}],["8c9375a8-4faa-4b76-8d7c-66cfc575ad11",{"pageContent":"computing, coupled with compression algorithms for image size reduction, the latency introduced by streaming\ndata to the cloud is reduced to minimum.  Finally, as demonstrated in our experiment, devices with camera and\nWiFi capability can be easily adapted to use our approach for deep learning computer vision,  allowing rapid\nprototyping for researchers and developers alike.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":9,"lines":{"from":36,"to":39}}}}],["58edbfa3-fd9d-4cc6-be5b-475ea31dda1c",{"pageContent":"In the future, we hope to develop a more robust communication scheme for streaming camera data from the\ndevice to the cloud.  Also, we look forward to adapting more deep learning algorithms to this system, allowing\nresource constrained devices to utilize real time deep learning for a greater range of tasks.\nREFERENCES\n[1]  Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco\nAndreetto, and Hartwig Adam. Mobilenets:  Efficient convolutional neural networks for mobile vision appli-\ncations.CoRR, abs/1704.04861, 2017.\n[2]  Intel  Movidius:  cutting  edge  solutions  for  deploying  deep  learning  and  computer  vision  algorithms  right\non-device at ultra-low power.  https://www.movidius.com.  Accessed:  2018-06-02.\n[3]  CEVA   NeuPro:a   family   of   ai   processors   for   deep   learning   at   the   edge.https://www.ceva-\ndsp.com/product/ceva-neupro/.  Accessed:  2018-06-02.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":10,"lines":{"from":1,"to":11}}}}],["3e1212b0-1725-4565-8f15-7f3c2929ac89",{"pageContent":"[3]  CEVA   NeuPro:a   family   of   ai   processors   for   deep   learning   at   the   edge.https://www.ceva-\ndsp.com/product/ceva-neupro/.  Accessed:  2018-06-02.\n[4]  James Kuffner, Cloud Robotics, The Google, Kiva Systems, Steve Cousins, Willow Garage, Online Robots,\nNetworked Robots, Mechanical Turk, The Cloud, World Wide Web, The Roboearth, and Industrial Internet.\nCloud Robotics and Automation.  pages 1–9, 2012.\n[5]  Guoqiang Hu, Wee Peng Tay, and Yonggang Wen. Cloud robotics: Architecture, challenges and applications.\nIEEE Network, 26(3):21–28, 2012.\n[6]  Ken Goldberg and Ben Kehoe. Cloud robotics and automation:  A survey of related work. Technical Report\nUCB/EECS-2013-5, EECS Department, University of California, Berkeley, Jan 2013.\n[7]  Nvidia Cloud Gaming:  cloud gaming – gaming as a service (gaas).  http://www.nvidia.com/object/cloud-\ngaming.html.  Accessed:  2018-06-02.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":10,"lines":{"from":10,"to":20}}}}],["9e56b406-586e-4df9-bdb8-436825bacaac",{"pageContent":"[7]  Nvidia Cloud Gaming:  cloud gaming – gaming as a service (gaas).  http://www.nvidia.com/object/cloud-\ngaming.html.  Accessed:  2018-06-02.\n[8]  Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik.  Rich feature hierarchies for accurate\nobject detection and semantic segmentation.CoRR, abs/1311.2524, 2013.\n[9]  Ross B. Girshick.  Fast R-CNN.CoRR, abs/1504.08083, 2015.\n[10]  Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You Only Look Once: Unified, Real-Time\nObject Detection.  2015.\n[11]  Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng Yang Fu, and Alexan-\nder C. Berg.  SSD: Single shot multibox detector.Lecture Notes in Computer Science (including subseries\nLecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 9905 LNCS:21–37, 2016.\n[12]  Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.  Faster R-CNN: Towards Real-Time Object De-","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":10,"lines":{"from":19,"to":29}}}}],["1b6425b9-1937-43ca-b943-5daf3d0192c3",{"pageContent":"[12]  Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.  Faster R-CNN: Towards Real-Time Object De-\ntection with Region Proposal Networks.IEEE Transactions on Pattern Analysis and Machine Intelligence,\n39(6):1137–1149, 2017.\n[13]  Joseph Redmon, Santosh Kumar Divvala, Ross B. Girshick, and Ali Farhadi.  You only look once:  Unified,\nreal-time object detection.CoRR, abs/1506.02640, 2015.\n[14]  Joseph Redmon and Ali Farhadi.  Yolov3:  An incremental improvement.CoRR, abs/1804.02767, 2018.\n[15]  Min  Wang,  Baoyuan  Liu,  and  Hassan  Foroosh.   Factorized  convolutional  neural  networks.2017  IEEE\nInternational Conference on Computer Vision Workshops (ICCVW), pages 545–553, 2017.\n[16]  Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf, Song Han, William J. Dally, and Kurt Keutzer.\nSqueezenet: Alexnet-level accuracy with 50x fewer parameters and<1mb model size.CoRR, abs/1602.07360,\n2016.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":10,"lines":{"from":29,"to":39}}}}],["18874489-d027-4c4f-9f29-5e2505feaa5d",{"pageContent":"Squeezenet: Alexnet-level accuracy with 50x fewer parameters and<1mb model size.CoRR, abs/1602.07360,\n2016.\n[17]  Google Vision API : cloud vision api provides a comprehensive set of capabilities including object detection,\nocr, explicit content, face, logo, and landmark detection. https://cloud.google.com/vision/. Accessed: 2018-\n06-02.\n[18]  Amazon Rekognition – Video and Image - AWS: amazon rekognition makes it easy to add image and video\nanalysis to your applications.  aws.amazon.com/rekognition.  Accessed:  2018-06-02.\n[19]  Clarifai :  drive revenue with computer vision ai.  www.clarifai.com/.  Accessed:  2018-06-02.\n[20]  Jangwon Lee, Jingya Wang, David J. Crandall, Selma Sabanovic, and Geoffrey C. Fox.  Real-time, cloud-\nbased object detection for unmanned aerial vehicles.2017 First IEEE International Conference on Robotic\nComputing (IRC), pages 36–43, 2017.\n[21]  Christian  Szegedy,  Wei  Liu,  Yangqing  Jia,  Pierre  Sermanet,  Scott  Reed,  Dragomir  Anguelov,  Dumitru","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":10,"lines":{"from":38,"to":49}}}}],["11967836-32c2-4da7-81ee-7a548114bf4a",{"pageContent":"Computing (IRC), pages 36–43, 2017.\n[21]  Christian  Szegedy,  Wei  Liu,  Yangqing  Jia,  Pierre  Sermanet,  Scott  Reed,  Dragomir  Anguelov,  Dumitru\nErhan, Vincent Vanhoucke, and Andrew Rabinovich.  Going deeper with convolutions.Proceedings  of  the\nIEEE  Computer  Society  Conference  on  Computer  Vision  and  Pattern  Recognition,  07-12-June-2015:1–9,\n2015.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":10,"lines":{"from":48,"to":52}}}}],["8c4c2298-6a30-4d61-a421-0c0efe1eea60",{"pageContent":"[22]  Cloud Chaser:  cloudchaser is a custom built,  cloud-based robot that uses computer vision to target and\nchase user-specified objects.  https://devpost.com/software/cloudchaser.  Accessed:  2018-06-02.\n[23]  CloudChaseriOS:objectdetection+localizationthroughdeeplearning+arkit.\nhttps://youtu.be/hB1dj2lU5Tk.  Accessed:  2018-06-02.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\1810.01069.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.21","CreationDate":"D:20201110022738Z","ModDate":"D:20201110022738Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":11},"loc":{"pageNumber":11,"lines":{"from":1,"to":4}}}}],["8e48ec68-9f1c-4166-a683-073cf3273429",{"pageContent":"Received 5 August 2022, accepted 28 August 2022, date of publication 31 August 2022, date of current version 12 September 2022.\nDigital Object Identifier 10.1109/ACCESS.2022.3203053\nDistributed Real-Time Object Detection Based on\nEdge-Cloud Collaboration for Smart Video\nSurveillance Applications\nYUNG-YAO CHEN\n1\n, (Member, IEEE), YU-HSIU LIN\n2\n,\nYU-CHEN HU\n3\n, (Senior Member, IEEE), CHIH-HSIEN HSIA\n4\n, (Member, IEEE),\nYI-AN LIAN\n1\n, AND SIN-YE JHONG\n5\n1\nDepartment of Electronic and Computer Engineering, National Taiwan University of Science and Technology, Taipei 106335, Taiwan\n2\nGraduate Institute of Automation Technology, National Taipei University of Technology, Taipei 106344, Taiwan\n3\nDepartment of Computer Science and Information Management, Providence University, Taichung 43301, Taiwan\n4\nDepartment of Computer Science and Information Engineering, National Ilan University, Ilan 260007, Taiwan\n5\nDepartment of Engineering Science, National Cheng Kung University, Tainan 701, Taiwan","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":1,"lines":{"from":1,"to":29}}}}],["80db3145-0421-40c3-b0b3-e016c025496b",{"pageContent":"4\nDepartment of Computer Science and Information Engineering, National Ilan University, Ilan 260007, Taiwan\n5\nDepartment of Engineering Science, National Cheng Kung University, Tainan 701, Taiwan\nCorresponding author: Yu-Hsiu Lin (yhlin@ntut.edu.tw)\nThis work was supported by the Ministry of Science and Technology, Taiwan, under Grant MOST 111-2221-E-027-050-, Grant MOST\n111-3116-F-006-005-, and Grant MOST 111-3116-F-027-001-.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\nABSTRACTInternet of Things (IoT) and artificial intelligence (AI) can realize the concept of ‘‘smart city.’’\nVideo surveillance in smart cities is, usually, based on a centralized framework in which large amounts of\nreal-time media data are transmitted to and processed in the cloud. However, the cloud relies on network\nconnectivity of the Internet that is sometimes limited or unavailable; thus, the centralized framework is not","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":1,"lines":{"from":26,"to":49}}}}],["96c71250-87b9-4335-953e-5d498db93fe0",{"pageContent":"connectivity of the Internet that is sometimes limited or unavailable; thus, the centralized framework is not\nsufficient for real-time processing of media data needed for smart video surveillance. To tackle this problem,\nedge computing - a technique for accelerating the development of AIoT (AI across IoT) in smart cities -\ncan be conducted. In this paper, a distributed real-time object detection framework based on edge-cloud\ncollaboration for smart video surveillance is proposed. When collaborating with the cloud, edge computing\ncan serve as converged computing through which media data from distributed edge devices of the network\nare consolidated by AI in the cloud. After AI discovers global knowledge in the cloud, it to be shared at\nthe edge is deployed remotely on distributed edge devices for real-time smart video surveillance. First, the\nproposed framework and its preliminary implementation are described. Then, the performance evaluation is","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":1,"lines":{"from":49,"to":57}}}}],["3c9714a4-1ac2-40dd-bf22-fc7990fccfe6",{"pageContent":"proposed framework and its preliminary implementation are described. Then, the performance evaluation is\nprovided regarding potential benefits, real-time responsiveness and low-throughput media data transmission.\n14\n15\nINDEX TERMSCloud computing, edge computing, edge-cloud collaboration, object detection, video\nsurveillance.\nI. INTRODUCTION16\nRecent breakthrough technologies are trending up for today’s17\ntechnologically driven society, i.e., from fundamental con-18\nstituents of a city (e.g., smart homes, buildings and factories)19\nto smart cities that promote our daily living by providing20\nsmart surveillance, healthcare, intelligent transportation and21\nso on [1]. The technical combination of advances in Inter-22\nnet of Things (IoT) collecting data around a city [2] and23\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Liang-Bi Chen\n.\nartificial intelligence (AI) using a wide range of algorithms24","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":1,"lines":{"from":57,"to":74}}}}],["436f606a-2c8c-4208-b0e9-bf611c314214",{"pageContent":"The associate editor coordinating the review of this manuscript and\napproving it for publication was Liang-Bi Chen\n.\nartificial intelligence (AI) using a wide range of algorithms24\nin soft computing, machine learning, deep learning, image25\nprocessing and computer vision to analyze collected data has26\nbeen developed; this brings novel insights to turn the concept27\nof ‘‘smart city’’ from hype to reality. Being the prominent28\ntechnologies nowadays, IoT and cloud computing (i.e., the29\ncloud) have received significant attention from both academia30\nand industry, as they are used in numerous applications in31\nsmart homes including healthcare devices, manufacturing and32\ncities [3]. Video surveillance is one of the main and important33\nbuilding blocks of smart cities, which offers people more34\nVOLUME 10, 2022\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":1,"lines":{"from":71,"to":87}}}}],["41071151-2c51-4225-836c-6af6cde547a9",{"pageContent":"VOLUME 10, 2022\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n93745","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":1,"lines":{"from":85,"to":88}}}}],["953b814d-0bed-4ff8-b103-079db817898d",{"pageContent":"Y.-Y. Chenet al.: Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration\ntools and applications to monitor environments (e.g., pedes-35\ntrians for security surveillance and management) and serves36\nas a security force for improving a city’s stability [2], [4].37\nSmart cities providing intelligent urban environments that can38\ndeliver a high quality of life to citizens to co-work with local39\ngovernment authorities to launch initiatives to be designed40\nand implemented with smart technologies used use a variety41\nof software, user interfaces and communication networks42\nalongside the AIoT (AI across IoT) to deliver connected solu-43\ntions for the public. In the past decade, for video surveillance44\napplications in different fields ranging from traffic monitor-45\ning to national security, it was a significant aspect to transmit46\ndirectly huge amounts of real-time media data from end47\ndevices to the cloud via the Internet (or dedicated high-speed48","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":2,"lines":{"from":1,"to":15}}}}],["d89bf787-c2f6-4ca8-8f69-70a3f07cb3b6",{"pageContent":"ing to national security, it was a significant aspect to transmit46\ndirectly huge amounts of real-time media data from end47\ndevices to the cloud via the Internet (or dedicated high-speed48\nfiber networks) [5], [6]. The cloud with rich storage and com-49\nputing resources can then perform massive-scale and com-50\nplex data processing, to detect and identify valuable insights51\nfrom vast amounts of received media data for surveillance52\napplications. In such a centralized cloud-centered framework,53\nthis leads to a high investment (including annual maintenance54\ncosts) in network deployment (simultaneously transmitting55\nIoT data to the cloud requires a large number of bandwidth56\nresources, where the existing network infrastructure cannot57\nsupport enough bandwidth at a reasonable price [7]). Also, it58\nposes great challenges to the cloud in terms of high network59\nlatency and congestion from heavy communication burden60\n[5], [7], [8]. Such a centralized cloud-centered framework61","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":2,"lines":{"from":13,"to":28}}}}],["ba1114d2-4e24-4968-bdf7-2d7c02113de6",{"pageContent":"poses great challenges to the cloud in terms of high network59\nlatency and congestion from heavy communication burden60\n[5], [7], [8]. Such a centralized cloud-centered framework61\nhas been gradually changed in recent years, as the current62\nbreakthrough technologies are driving us into the era of AIoT)63\n(AI across IoT). Limited by the network connectivity where64\nthe Internet is not always available, transmitting all media65\ndata from end devices to the cloud for data processing is66\nno longer a wise approach for real-time video surveillance67\napplications (latency-sensitive video surveillance applica-68\ntions) [9], [10]. Edge computing, a developed complement69\nof cloud computing, can perform real-time media data pro-70\ncessing at the edge of the Internet, while relieving computing71\nand storage pressure of the cloud; this results in remedying72\nhigh network latency and congestion [6], [8], [11], [12].73\nBy leveraging storage and computing resources at the edge74","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":2,"lines":{"from":26,"to":41}}}}],["d250ca61-cc0d-47da-8d84-becb94f5415b",{"pageContent":"and storage pressure of the cloud; this results in remedying72\nhigh network latency and congestion [6], [8], [11], [12].73\nBy leveraging storage and computing resources at the edge74\nof the Internet, edge computing can provide distributed, real-75\ntime media data processing and analysis for surveillance76\napplications [5], [8], [13]. Meanwhile, collaborating with the77\ncloud to form edge-cloud computing, edge computing can78\nalso serve as converged computing through which data from79\ndistributed edge devices are consolidated through AI. After80\nAI achieves global knowledge discovering, it is then deployed81\nremotely as global knowledge sharing on distributed edge82\ndevices for real-time smart video surveillance [14]. Where,83\nAI that is deployed at the edge of the network is trained in84\nthe cloud. The aforementioned advantages have promoted85\nthe significance of the edge collaborating with the cloud for86\ndeveloping next-generation video surveillance applications,87","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":2,"lines":{"from":39,"to":54}}}}],["accc2481-3df4-4f46-9a3f-5fa23561604e",{"pageContent":"the cloud. The aforementioned advantages have promoted85\nthe significance of the edge collaborating with the cloud for86\ndeveloping next-generation video surveillance applications,87\nwhere the cloud consolidates data from and interacts with88\nedge devices to update their knowledge (trained local AI89\nmodel).90\nSmart video surveillance is a new initiative that sets a91\nhigher ceiling than that of traditional video surveillance for92\nthe future of smart cities [3]. Yet, not much attention has93\nbeen paid to smart video surveillance applications based on94\nedge-cloud collaboration for smart cities in traffic monitor-95\ning, national security, healthcare and many others. In [15],96\nan edge computing-based surveillance framework, a Convo-97\nlutional Neural Network (CNN)-implemented edge device98\nbased on Raspberry Pi, for real-time human activity recog-99\nnition is presented. The authors of [16] develop a face recog-100\nnition system based on a fog computing platform, where a101","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":2,"lines":{"from":52,"to":68}}}}],["40cfece0-d741-4958-9986-4731f8498d60",{"pageContent":"based on Raspberry Pi, for real-time human activity recog-99\nnition is presented. The authors of [16] develop a face recog-100\nnition system based on a fog computing platform, where a101\nsmartphone’s photos are proceeded at the edge instead of102\nthe cloud, to achieve fast response of face recognition. The103\nauthors of [17] and [18] propose a framework for video104\nstream acquisition, storage and analysis using a cloud based105\nGPU (Graphic Processing Unit) cluster accelerating the video106\nprocessing process to reduce the computational complexity.107\nHowever, the studies only considered a centralized video108\nanalytics framework. In [19], [20], and [21], cloud-based109\nsmart video surveillance systems are shown, which were110\na centralized smart video surveillance paradigm. In [22],111\nan edge computing-based video analytics for public safety112\nis presented, which can distribute computing workloads in113\nboth the edge, including a smartphone, Raspberry Pi and114","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":2,"lines":{"from":66,"to":81}}}}],["d68889e3-d94e-4639-b017-287e49710109",{"pageContent":"an edge computing-based video analytics for public safety112\nis presented, which can distribute computing workloads in113\nboth the edge, including a smartphone, Raspberry Pi and114\nbody-worn cameras, and the cloud in an optimized way.115\nAn edge-cloud cooperation framework is proposed in [23],116\nwhich aims at designing and implementing the framework117\nin a heterogeneous converged communication network for118\nedge computing in a coal mine environment. The authors119\nof [24] investigate an edge-cloud computing based smart120\nparking surveillance system for parking occupancy detection.121\nWhere, two detection methods are implemented at the edge122\nand their detection results can be combined, for occlusion or123\nextreme lighting conditions occurring for parking occupancy124\ndetection in a parking lot scene, by a rule or a metric on125\nthe cloud-centric server side for enhanced performance in126\nparking occupancy detection. In [25], a cloud-based object127","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":2,"lines":{"from":79,"to":94}}}}],["2f0d1d1e-7985-457b-bd08-d370c225c91c",{"pageContent":"detection in a parking lot scene, by a rule or a metric on125\nthe cloud-centric server side for enhanced performance in126\nparking occupancy detection. In [25], a cloud-based object127\ntracking and behavior identification architecture performing128\nabnormal fall detection based on a deep learning approach,129\na CNN, for a smart healthcare video surveillance applica-130\ntion is proposed. As demonstrated in the study, the archi-131\ntecture purely depends on a cloud-based platform for object132\ndetection and fall recognition which is restricted to network133\nbandwidth utilization. Where, the limitation can be easily134\nresolved as a more robust process can be enforced in the135\narchitecture, utilizing the edge computing capability, within136\nlimited network bandwidth utilization. As reviewed above,137\nit is vital to develop a dedicated edge-cloud computing archi-138\ntecture, utilizing an edge-cloud collaboration mechanism, for139","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":2,"lines":{"from":92,"to":106}}}}],["391db8bf-135c-45e3-806a-d20c412df7af",{"pageContent":"limited network bandwidth utilization. As reviewed above,137\nit is vital to develop a dedicated edge-cloud computing archi-138\ntecture, utilizing an edge-cloud collaboration mechanism, for139\nobject detection and classification in smart video surveillance140\n(a video-/image-processing related practical application)141\nsuch that the full use of storage and computing resources of142\nboth the edge and the cloud to be leveraged within limited net-143\nwork bandwidth utilization and be enabled for autonomous144\nAI updates so the whole system can be advanced can be real-145\nized. Therefore, in this paper, a distributed real-time object146\n93746VOLUME 10, 2022","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":2,"lines":{"from":104,"to":114}}}}],["8cf1f8f0-7fc2-4ed1-bde2-c9e9df07c815",{"pageContent":"Y.-Y. Chenet al.: Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration\ndetection and classification framework based on edge-cloud147\ncollaboration for smart video surveillance applications is pro-148\nposed, and its preliminary implementation is demonstrated.149\nWhere, both the edge and the cloud can be leveraged within150\nlimited network bandwidth utilization and be enabled for151\nautonomous AI updates as converged computing for different152\nkinds of objects from different monitoring scenes to possible153\nwarning situations.154\nOur first contribution is that we propose and demonstrate a155\npreliminary implementation of a newly developed distributed156\nreal-time object detection framework based on edge-cloud157\ncollaboration for smart video surveillance applications. Upon158\nthis framework, an edge-cloud collaboration mechanism is159\nrealized based on image matting and You Only Look Once160\n(YOLO)-based AI methodology such as the state-of-the-161","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":3,"lines":{"from":1,"to":16}}}}],["4f548604-bd70-4157-aa4b-ad50ab546ad0",{"pageContent":"this framework, an edge-cloud collaboration mechanism is159\nrealized based on image matting and You Only Look Once160\n(YOLO)-based AI methodology such as the state-of-the-161\nart YOLOR object detection technique; where the cloud162\nachieves global knowledge discovering from and enables163\nglobal knowledge sharing to the edge to advance the whole164\nframework for real-time smart video surveillance. The edge-165\ncloud collaboration mechanism can autonomously update166\nlocal AI models on edge devices having global knowledge167\nfor detecting and classifying unknown objects that appear168\nfrequently (objects are not seen beforehand). Lastly, the169\nproposed framework can reduce/prevent network congestion170\n(i.e., eliminate unnecessary data transmission) for real-time171\nsmart video surveillance when sending/transmitting a small172\nset of images, with unknown latent objects, from the edge173\nto the cloud when the network connectivity is available,174","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":3,"lines":{"from":14,"to":29}}}}],["5d7cf74d-d1f9-4b73-aa1a-e587878877ac",{"pageContent":"smart video surveillance when sending/transmitting a small172\nset of images, with unknown latent objects, from the edge173\nto the cloud when the network connectivity is available,174\ncompared to centralized cloud-centered paradigms.175\nThe remainder of the paper is structured as follows.176\nSection II describes the proposed distributed real-time object177\ndetection framework based on edge-cloud collaboration for178\nreal-time smart video surveillance applications, and its bene-179\nfits. Section III describes the experiments that demonstrate a180\npreliminary implementation of the proposed framework and181\nvalidate its feasibility and effectiveness. Lastly, the conclu-182\nsions are summarized in Section IV.183\nII. METHODOLOGY184\nA. FRAMEWORK185\nFigure 1 depicts the overview of the proposed distributed real-186\ntime object detection framework considering edge-cloud col-187\nlaboration for real-time smart video surveillance at the edge.188","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":3,"lines":{"from":27,"to":43}}}}],["9134b12c-c7af-44be-b24c-489cabd35a0d",{"pageContent":"Figure 1 depicts the overview of the proposed distributed real-186\ntime object detection framework considering edge-cloud col-187\nlaboration for real-time smart video surveillance at the edge.188\nThe framework is composed of three tiers: the end device,189\nedge device and cloud tiers. The process flow for the proposed190\nsmart video surveillance based on edge-cloud collaboration191\nis described as follows. IoT end devices - front-end video192\ncameras - in the end device tier act as the first action that cap-193\ntures IoT data, i.e., media images in real time. The captured194\nreal-time media images are then passed to edge devices in the195\nedge device tier. The edge devices connected to end devices196\nand distributed geographically with storage and computing197\ncapabilities are responsible for processing and analyzing real-198\ntime media images for real-time smart video surveillance199\nwhere they collaborate with the cloud in the cloud tier.200","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":3,"lines":{"from":41,"to":55}}}}],["074cc7ff-c4bb-4d05-97bf-a267832df175",{"pageContent":"capabilities are responsible for processing and analyzing real-198\ntime media images for real-time smart video surveillance199\nwhere they collaborate with the cloud in the cloud tier.200\nThe  distributed  edge  devices  for  real-time  captured,201\nprocessed and analyzed media images can be server-202\nclass machines or systems like Advanced RISC Machine203\n(ARM)\nR\n©processor-based embedded systems deployed204\non-site in different realistic fields of interest, which are205\nwith their trained local AI model for latency-sensitive video206\nsurveillance applications [26], [27]. When distributed on-site207\nedge devices in the edge device tier detect unknown latent208\nobjects with a low confidence level for certain times, they209\ntransmit images to the cloud for object detection. In the cloud210\ntier, the cloud having rich storage and computing resources211\nto store captured image data from distributed on-site edge212\ndevices and consolidate stored image data through AI achiev-213","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":3,"lines":{"from":53,"to":70}}}}],["799b9d11-b8fb-4e12-8223-e6b489bdfc76",{"pageContent":"tier, the cloud having rich storage and computing resources211\nto store captured image data from distributed on-site edge212\ndevices and consolidate stored image data through AI achiev-213\ning global knowledge discovering assists distributed on-site214\nedge devices to detect unknown latent objects. Also, trained215\nlocal AI models - local inference models - to distributed216\non-site edge devices are updated remotely from the cloud217\nenabling global knowledge sharing such that converged com-218\nputing can be deployed to and performed on-line at the edge219\nof the Internet. Such an edge-cloud collaboration mechanism220\nwhere the cloud achieves global knowledge discovering from221\nand enabling global knowledge sharing to the edge to advance222\nthe whole system for smart video surveillance is necessary223\nbecause the performance and detection accuracy of trained224\nlocal AI run on distributed edge devices for real-time video225\nsurveillance applications may be degraded over time with226","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":3,"lines":{"from":68,"to":83}}}}],["5a904cdd-0cae-4a34-a2dd-9873f564c840",{"pageContent":"because the performance and detection accuracy of trained224\nlocal AI run on distributed edge devices for real-time video225\nsurveillance applications may be degraded over time with226\nunknown objects. Additionally, global knowledge sharing227\ncan be triggered under a certain criterion that unknown228\nobjects frequently appeared for a certain number of times.229\nIn the cloud with such a mechanism, unknown latent objects230\nrelated to a computed confidence level are found at the edge;231\nfurther, queries in the form of requesting class labels for232\nunlabeled images are sent over the Internet to an adminis-233\ntrator overcoming the labeling bottleneck, where a practical234\nparadigm of LINE Notify push notification service [14] is235\nachieved. Owing to the distributed edge devices collaborating236\nwith the cloud, they can process and analyze captured real-237\ntime video images for real-time object detection without238\ntransmitting all image data to the cloud via the backbone239","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":3,"lines":{"from":81,"to":96}}}}],["ed8709af-1bca-4d88-b272-ee002e7b189c",{"pageContent":"with the cloud, they can process and analyze captured real-237\ntime video images for real-time object detection without238\ntransmitting all image data to the cloud via the backbone239\nInternet [28].240\nB. PIPELINE OF DISTRIBUTED REAL-TIME OBJECT241\nDETECTION UNDER EDGE-CLOUD COLLABORATION242\nFigure 2 illustrates the pipeline of distributed real-time243\nobject detection under edge-cloud collaboration in the pro-244\nposed framework for real-time smart video surveillance at245\nthe edge.246\nIn the pipeline, the end devices are responsible for real-247\ntime image data acquisition and transmission. Real-time248\nvideo images are captured by end devices, and then, trans-249\nmitted to on-site edge devices for further data process-250\ning and analysis. On-site edge devices receiving captured251\nimage data from end devices are responsible for real-time252\nobject detection and classification. They perform real-time253\nobject detection and classification based on their trained local254","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":3,"lines":{"from":94,"to":111}}}}],["2675d56a-2b5e-4045-8d71-ac298b2a28f3",{"pageContent":"image data from end devices are responsible for real-time252\nobject detection and classification. They perform real-time253\nobject detection and classification based on their trained local254\none-stage object detection model, and collaborate with the255\ncloud based on an edge-cloud collaboration mechanism when256\nVOLUME 10, 202293747","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":3,"lines":{"from":109,"to":114}}}}],["ce353f58-b86f-402a-a9be-9dadf5eb2473",{"pageContent":"Y.-Y. Chenet al.: Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration\nFIGURE 1.An overview of the distributed real-time object detection framework considering edge-cloud collaboration proposed for\nreal-time smart video surveillance at the edge.\nFIGURE 2.A pipeline of distributed real-time object detection under edge-cloud collaboration in the framework proposed for real-time\nsmart video surveillance at the edge. Numerous distributed on-site edge devices transmitting their video streams to and collaborating\nwith the cloud to form a smart video surveillance orchestrator are the fundamental constituents of the whole system; they process and\nanalyze video streams in real time for real-time smart video surveillance applications (the cloud is not suitable for tasks that are\nsensitive to network latency and congestion). The cloud uses stored video data, from numerous on-site edge devices distributed in","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":4,"lines":{"from":1,"to":8}}}}],["936c185a-8ec8-4d91-8382-d6b90d03f627",{"pageContent":"sensitive to network latency and congestion). The cloud uses stored video data, from numerous on-site edge devices distributed in\nvarious fields of interest at the edge of the Internet, to train, improve and optimize the model deployed remotely on distributed on-site\nedge devices. With an edge-cloud collaboration mechanism, the cloud can assist distributed on-site edge devices to update their\ninference model to detect unknown objects in images.\nunknown objects exist in images and cannot be detected at257\nthe edge.258\nThe edge-cloud collaboration mechanism in the pipeline259\nof the proposed framework in this paper is described in260\n93748VOLUME 10, 2022","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":4,"lines":{"from":8,"to":16}}}}],["e066782c-e66d-46ad-ae2c-e5cc54d7236b",{"pageContent":"Y.-Y. Chenet al.: Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration\nSection II-Part 1; the one-stage object detection approach is261\npresented in Section II-Part 2. The cloud is responsible for262\nimage data storage and modeling via AI achieving global263\nknowledge discovering and enabling global knowledge264\nsharing. Through the edge-cloud collaboration mechanism,265\non-site edge devices, which serve as a promising complement266\nof the cloud to support real-time smart video surveillance,267\ncollaborate with the cloud to refresh their knowledge (trained268\nlocal AI model to be updated) in order to detect and classify269\nunknown objects at the edge. The collaboration advances the270\nwhole system in real-time object detection and classification271\nfor smart real-time video surveillance applications.272\n1) EDGE-CLOUD COLLABORATION MECHANISM BASED273\nUPON IMAGE MATTING AND YOLOV3 RELATING TO274\nA CONFIDENCE LEVEL OF RA FOR UNKNOWN275\nLATENT OBJECTS276","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":5,"lines":{"from":1,"to":17}}}}],["69ce6afe-f32c-4b50-a598-b53517cd93d6",{"pageContent":"1) EDGE-CLOUD COLLABORATION MECHANISM BASED273\nUPON IMAGE MATTING AND YOLOV3 RELATING TO274\nA CONFIDENCE LEVEL OF RA FOR UNKNOWN275\nLATENT OBJECTS276\nFirst, it is impractical to train a powerful AI model on an edge277\ndevice to detect all objects for real-time video surveillance278\nas an example. Practically, it can be alternated through an279\nedge-cloud collaboration mechanism to adaptively update280\nan AI model according to the environment where the edge281\ndevice is located, as the cloud has large storage and com-282\nputing resources for training a powerful AI model that can283\nbe deployed remotely for AI inference at the edge of the284\nnetwork (the Internet). Thus, in this paper, an edge-cloud285\ncollaboration mechanism developed and used to realize such286\nan alternative is proposed. Figure 3 shows the block diagram287\nof the proposed mechanism (which is mainly based upon288\nimage matting and YOLOv3, relating to a confidence level of289","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":5,"lines":{"from":14,"to":30}}}}],["3d3d7c1b-f905-492e-b2b5-3fc09d084247",{"pageContent":"an alternative is proposed. Figure 3 shows the block diagram287\nof the proposed mechanism (which is mainly based upon288\nimage matting and YOLOv3, relating to a confidence level of289\nresidual area (ra), for unknown latent objects to be learned for290\na new AI model trained in the cloud and deployed remotely on291\nan edge device(s) for real-time smart video surveillance at the292\nedge). We summarize the block diagram using an illustrative293\ncode snippet, as summarized in Table 1 (Algorithm 1).294\nThe developed mechanism can assist an edge device(s)295\nin adapting to the environment where it is located for real-296\ntime video surveillance at the edge. When an unknown latent297\nobject(s) appears frequently at the edge, the corresponding298\nimages are sent/transmitted to the cloud for off-line object299\ndetection and classification where an AI model is trained300\nacross all object classes. Once the cloud has trained a power-301","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":5,"lines":{"from":28,"to":42}}}}],["14b47b7b-9168-4578-b172-a5eb5f75739c",{"pageContent":"images are sent/transmitted to the cloud for off-line object299\ndetection and classification where an AI model is trained300\nacross all object classes. Once the cloud has trained a power-301\nful AI model across all object classes (global knowledge dis-302\ncovering/converged analytics), it deploys/updates the trained303\nmodel (via the network (the Internet)) with good generaliza-304\ntion for AI inference at the edge (global knowledge sharing).305\nWith global knowledge discovering and sharing, the edge can306\nknow exactly if new objects are present or not. In the proposed307\ndistributed real-time object detection framework, the edge-308\ncloud collaboration mechanism computes a confidence level309\nofrarelating to a frequent occurrence to determine whether310\nor not the cloud is requested by the edge to be assisted.311\nSending/transmitting a small set of images with unknown312\nlatent objects to the cloud, whenrais greater thanαforβ313","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":5,"lines":{"from":40,"to":54}}}}],["82468cc7-24d2-4a7e-9ab6-be40db6d7147",{"pageContent":"or not the cloud is requested by the edge to be assisted.311\nSending/transmitting a small set of images with unknown312\nlatent objects to the cloud, whenrais greater thanαforβ313\ntimes and the network is not congested, can reduce/prevent314\nnetwork congestion from the edge to the cloud with network315\nconnectivity from the cloud to the edge for real-time smart316\nvideo surveillance. Image matting and YOLOv3 relating317\nto a confidence level ofrain the mechanism are briefly318\ngiven below. Figure 4 shows the block diagram of image319\nmatting [29], i.e., a process of accurately estimating fore-320\nground objects in images, used in the edge-cloud collabo-321\nration mechanism above; this is a very important technique322\nin image, video editing and composition applications. Fore-323\nground objects found in images through this process are con-324\nsidered and used against YOLOv3’s object detection results325\nto judge if the foreground objects are unknown in images at326","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":5,"lines":{"from":52,"to":67}}}}],["df5f35e5-af58-4944-9d3f-38ad0a81b65a",{"pageContent":"ground objects found in images through this process are con-324\nsidered and used against YOLOv3’s object detection results325\nto judge if the foreground objects are unknown in images at326\nthe edge or not (whererais computed and compared as shown327\nin Table 1 (Algorithm 1)). The output of each function block328\nshown in Figure 4(a) is shown in Figure 4(b).329\nIn Algorithm 1,rais computed by Equation (1), whereW,330\nHandA\ni\n(nwhite pixels) are the width, height and white area331\nof a resulting image through image cutting in Algorithm 1,332\nrespectively. If an unknown latent object(s) that cannot be333\ndetected and classified by YOLOv3 deployed at the edge334\nand applied on a captured image (frame) exists, Equation (1)335\nproduces a very high value ofra; otherwise, it produces a very336\nlow value ofra.337\nWhen an edge device(s) at the edge detects an unknown338\nlatent object(s) that frequently appeared for certain times with339","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":5,"lines":{"from":65,"to":82}}}}],["d229be17-2ee8-4980-b456-505e11e51820",{"pageContent":"low value ofra.337\nWhen an edge device(s) at the edge detects an unknown338\nlatent object(s) that frequently appeared for certain times with339\na confidence level ofrato be compared with a threshold, the340\ncloud handshaking with the edge assists in detecting it.341\nra=\n∑\nn\ni\nA\ni\nW×H\n(1)342\nIn the cloud with the same mechanism, unknown latent343\nobjects relating to a confidence level ofraare found at the344\nedge; also, queries in the form of requesting class labels for345\nunlabeled images are sent over the Internet to an admin-346\nistrator overcoming the labeling bottleneck, where a prac-347\ntical paradigm of LINE Notify push notification service is348\nachieved.349\n2) ONE-STAGE OBJECT DETECTION APPROACH350\nBASED ON YOLOV3351\nConvolutional Neural Networks (CNNs) are analogous to352\nthe organization and connectivity pattern of neurons in the353\nvisual cortex of the human brain [30]. CNN-based object354\ndetection approaches have been receiving a lot of attention355","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":5,"lines":{"from":80,"to":105}}}}],["de195f6b-913f-4731-bb5b-b058ea97967d",{"pageContent":"the organization and connectivity pattern of neurons in the353\nvisual cortex of the human brain [30]. CNN-based object354\ndetection approaches have been receiving a lot of attention355\nfrom researchers for video surveillance applications, due356\nto their superior performance; however, they can only be357\ndeployed on cloud machines having rich storage and com-358\nputing resources [23]. For real-time object detection, one-359\nstage object detection approaches like the YOLO series [31],360\n[32], [33] are widely used. YOLO was first proposed in361\n[24] to enhance the performance of two-stage object detec-362\ntion approaches like the faster Regional Convolutional Neu-363\nral Network (R-CNN); R-CNNs are reliable, but they are364\nslow even when they are running on a graphics process-365\ning unit (GPU). On the other hand, the one-stage object366\ndetection approach (YOLO) is very fast and can achieve367\nvery good real-time efficiency on a GPU [2]. It deals with368\nVOLUME 10, 202293749","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":5,"lines":{"from":103,"to":119}}}}],["b454090e-a1b3-4409-be24-bc787d69eaff",{"pageContent":"Y.-Y. Chenet al.: Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration\nFIGURE 3.A block diagram of the proposed edge-cloud collaboration mechanism.\nTABLE 1.Algorithm 1: Edge-cloud collaboration mechanism, based upon image matting and YOLOv3 relating to a confidence level of, for detecting\nunknown latent objects in real-time smart video surveillance at the edge. YOLO-accommodated AI methodology such as YOLOv4 and YOLOR against\nYOLOv3 can be considered; they are conducted and compared in this paper.\nobject detection as a regression problem, which is faster but369\nless accurate than two-stage approaches like faster R-CNN370\n[34], [35], [36]. Subsequently, the authors who proposed371\nYOLO improved YOLOv1 to propose YOLOv2 [32].372\nYOLOv2 replaces the fully-connected layer of the YOLOv1373\nmodel with a fully convolutional layer, so as to have the374\nability of handling images with different sizes. Also, the375\nYOLOv1 model is upgraded in terms of object localiza-376","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":6,"lines":{"from":1,"to":13}}}}],["07852fbf-e335-480c-b77d-2497875ed8a0",{"pageContent":"model with a fully convolutional layer, so as to have the374\nability of handling images with different sizes. Also, the375\nYOLOv1 model is upgraded in terms of object localiza-376\ntion and multi-scale object detection. In 2018, YOLOv2377\nwas improved and became YOLOv3 [33]. In compari-378\nson with YOLOv2, YOLOv3 greatly improves detection379\naccuracy, while maintaining detection speed. YOLOv3 has380\n93750VOLUME 10, 2022","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":6,"lines":{"from":11,"to":18}}}}],["d84f1ac6-0fa5-4988-854a-35eda5fe3e76",{"pageContent":"Y.-Y. Chenet al.: Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration\nFIGURE 4.A flowchart of image matting used in the developed\nedge-cloud collaboration mechanism: (a) function blocks involved in\nimage matting; (b) output of each function block.\nbeen widely used in object detection owing to its excellent381\ndetection accuracy and speed [23], [33]. To achieve real-382\ntime video surveillance based on edge-cloud collaboration,383\nAI deployed on edge devices should be robust without sacri-384\nficing detection accuracy over time while maintaining detec-385\ntion speed. In the pipeline of the proposed framework in386\nthis paper, real-time object detection and classification is387\nachieved by YOLOv3 [37], which is conducted and used388\nas a benchmark for the object detection method in the pro-389\nposed distributed real-time object detection framework con-390\nsidering edge-cloud collaboration for real-time smart video391\nsurveillance applications.392","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":7,"lines":{"from":1,"to":16}}}}],["eee9450f-624c-4d10-ba6e-e84f6f25cf87",{"pageContent":"posed distributed real-time object detection framework con-390\nsidering edge-cloud collaboration for real-time smart video391\nsurveillance applications.392\nThe backbone of YOLOv3 has a total of 53 layers, and393\nit uses the Residual Network (ResNet) to solve the problem394\nof gradient vanishing. In addition, it uses the Feature Pyra-395\nmid Network (FPN) [37] with multi-scale feature maps to396\ndetect objects with different sizes such that its performance in397\ndetecting small objects can be improved. Figure 5 shows the398\nused YOLOv3 in this paper. Figure 5(a) shows that, it can use399\nmulti-scale feature maps, with different sizes, to strengthen400\nits performance in detecting objects having different sizes;401\nFigure 5(b) shows that, the feature maps with the different402\nsizes are used to extract fine-grained, distinguishable features403\nin distinguishing from objects having different sizes in an404\nimage.405\nIn YOLOv3, the loss function of the model to be trained406","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":7,"lines":{"from":14,"to":30}}}}],["50ce6cf0-3264-45b3-982d-9b0249534640",{"pageContent":"sizes are used to extract fine-grained, distinguishable features403\nin distinguishing from objects having different sizes in an404\nimage.405\nIn YOLOv3, the loss function of the model to be trained406\ncan be defined in Equation (2).407\nLoss=L\nbbox\n+L\nobj\n+L\ncls\n(2)408\nwhereL\nbbox\nis the bounding box regression loss by the sum409\nof squared error (SSE),L\nobj\nrepresents the object confidence410\nloss, andL\ncls\nis the class score based on cross entropy (CE).411\nThe first loss (L\nbbox\n) in Equation (2) can be expressed as412\nEquation (3).413\nL\nbbox\n=λ\ncoord\nS\n∑\ni=0\nS\n∑\nj=0\nB\n∑\nk=0\nI\nobj\nijk\n414\n×\n∑\nr∈(x,y,w,h)\nSSE(P\nr\nbbox\n,G\nr\nbbox\n)   (3)415\nwhereP\nr\nbbox\nandG\nr\nbbox\nrepresent the predicted box and416\nground truth bounding box, respectively. Also,λ\ncoord\nindi-417\ncates the loss weighting,Sdenotes the number of grids, and418\nBaccounts for the box generated by each grid cell.I\nobj\nijk\nshows419\nwhether or not an object exists in thek\nth\ngrid cell (i,j):420\nif it exists,I\nobj\nijk","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":7,"lines":{"from":27,"to":99}}}}],["70c7e9e6-11a5-4374-9f12-27cebc9ed2bf",{"pageContent":"Baccounts for the box generated by each grid cell.I\nobj\nijk\nshows419\nwhether or not an object exists in thek\nth\ngrid cell (i,j):420\nif it exists,I\nobj\nijk\n=1; otherwise, it is 0. In this paper, the421\nDistance-Intersection over Union (DIoU) is considered and422\nused, which is an improvement of the Intersection over Union423\n(IoU)-based loss for object detection bounding box regression424\nto its anchor boxes to be identified. In Equation (3),L\nbbox\nis425\ncomputed through the SSE between the predicted and ground426\ntruth bounding boxes.427\nThe second loss (L\nobj\n) in Equation (2) can be expressed as428\nin Equation (4).429\nL\nobj\n=\nS\n∑\ni=0\nS\n∑\nj=0\nB\n∑\nk=0\nI\nobj\nijk\nSSE(P\nobj\n,I\nm\n)+λ\nnoobj\n430\n×\nS\n∑\ni=0\nS\n∑\nj=0\nB\n∑\nk=0\nI\nnoobj\nijk\nSSE(P\nobj\n,0)   (4)431\nwhereP\nobj\nis the probability that indicates whether or not432\nan object exists in anchor boxes. The third loss (L\ncls\n) in433\nEquation (2) can be expressed as in Equation (5).434\nL\nobj\n=\nS\n∑\ni=0\nS\n∑\nj=0\nA\n∑\nk=0\nI\nobj\nijk\nC\n∑\nc=0\nCE(P\nc","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":7,"lines":{"from":90,"to":177}}}}],["55214845-b92b-40f3-9236-57e6200d4395",{"pageContent":"an object exists in anchor boxes. The third loss (L\ncls\n) in433\nEquation (2) can be expressed as in Equation (5).434\nL\nobj\n=\nS\n∑\ni=0\nS\n∑\nj=0\nA\n∑\nk=0\nI\nobj\nijk\nC\n∑\nc=0\nCE(P\nc\nclass\n,G\nc\nclass\n)(5)435\nVOLUME 10, 202293751","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":7,"lines":{"from":154,"to":183}}}}],["4f072366-0c01-4d62-a050-413e911ee7a4",{"pageContent":"Y.-Y. Chenet al.: Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration\nFIGURE 5.Used YOLOv3: (a) multi-scale feature maps with different sizes are used to strengthen the performance in detecting objects having\ndifferent sizes; (b) image examples are shown, where the feature maps are with the different sizes.\nwhereP\nc\nclass\nrepresents the predicted class labelcand G\nc\nclass\n436\nis the actual class label to be targeted.437\nIn this paper, state-of-the-art object detection techniques -438\nYou Only Learn One Representation (YOLOR) [38] and439\nYOLOv4 [39] - are also used in the experiments in the440\nfollowing section and compared against YOLOv3 in terms441\nof frames per second (FPS), model size (in MB) and mean442\nAverage Precision (mAP) as the evaluation metrics. FPS is443\nused to evaluate the inference speed of the three state-of-the-444\nart object detection techniques.445\nIII. EXPERIMENTAL RESULTS AND DISCUSSION446","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":8,"lines":{"from":1,"to":20}}}}],["cf35ecd1-5380-44a5-aeba-b67790aba52f",{"pageContent":"used to evaluate the inference speed of the three state-of-the-444\nart object detection techniques.445\nIII. EXPERIMENTAL RESULTS AND DISCUSSION446\nIn this section, the proposed distributed real-time object447\ndetection framework based on the edge-cloud collaboration448\nmechanism for real-time smart video surveillance at the edge449\nis experimentally evaluated. Figure 6 shows the preliminary450\nimplementation of the proposed framework for real-time451\nsmart video surveillance at the edge. The three distributed452\non-site edge devices connected with their video camera (their453\nend device) are deployed at the three different locations,454\nand the cloud is deployed in a laboratory environment. The455\ndistributed on-site edge devices are networked across wireless456\nWi-Fi communication with the cloud. They transmit real-time457\nvideo data to the cloud (based on the edge-cloud collabo-458\nration mechanism, they can send the cloud only a certain459","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":8,"lines":{"from":18,"to":33}}}}],["82095ae9-f08c-40c1-bc11-73dc9bdba3e9",{"pageContent":"Wi-Fi communication with the cloud. They transmit real-time457\nvideo data to the cloud (based on the edge-cloud collabo-458\nration mechanism, they can send the cloud only a certain459\nnumber of video data with potential new objects such that460\nthe bandwidth of the backbone network can be saved). They461\nalso process and analyze transmitted video data for real-time462\nobject detection and classification, using their trained local463\nAI model. The cloud stores received video data from the464\ndistributed on-site edge devices. Further, the trained AI model465\n(1) achieves global knowledge discovering from received466\nvideo data captured at the different locations (for various sce-467\nnarios) and (2) enables global knowledge sharing to the video468\ndata sources where distributed on-site edge devices refresh469\ntheir knowledge (AI model) such that the whole video surveil-470\nlance system is advanced with the edge-cloud collaboration471","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":8,"lines":{"from":31,"to":45}}}}],["5fb7ccca-f515-487b-a23d-0cf3e3543615",{"pageContent":"data sources where distributed on-site edge devices refresh469\ntheir knowledge (AI model) such that the whole video surveil-470\nlance system is advanced with the edge-cloud collaboration471\nmechanism for real-time smart video surveillance. Objects472\ntargeted by the distributed on-site edge devices with the focus473\non object detection and classification of real-time smart video474\nsurveillance potential are presented in Table 2. The objects475\ninclude pedestrians, bicycles, cars, motorcycles, trucks and476\nbuses; they are large objects and some of them are potentially477\ndangerous targets (forbidden or restricted) for certain areas478\n93752VOLUME 10, 2022","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":8,"lines":{"from":43,"to":53}}}}],["115c6d91-67a7-4d3a-8b6c-cc26d371751f",{"pageContent":"Y.-Y. Chenet al.: Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration\nFIGURE 6.Preliminary implementation of the proposed distributed real-time object detection framework based on the edge-cloud\ncollaboration mechanism for real-time smart video surveillance at the edge. In the proposed framework, the distributed on-site edge devices\nconnected with their end device and deployed at different locations can transmit different real-time video data to the cloud, and the cloud\ncan optimize AI based on received video data. The edge-cloud collaboration mechanism can advance the whole system.\nTABLE 2.Objects targeted by the distributed on-site edge devices.\nof interest. Table 3 lists the specifications of hardware and479\nsoftware used in the preliminary implementation.480\nIn the preliminary implementation, three power-efficient481\nand   compact   nNVIDIA\nR\n©\nJetson   Xavier\nTM\nNX482\nmodules are used as the distributed on-site edge devices483","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":9,"lines":{"from":1,"to":16}}}}],["40e26b5b-c13e-427c-95e7-248f8cc087b9",{"pageContent":"In the preliminary implementation, three power-efficient481\nand   compact   nNVIDIA\nR\n©\nJetson   Xavier\nTM\nNX482\nmodules are used as the distributed on-site edge devices483\nperforming deployed AI updated from the cloud for real-484\ntime smart video surveillance applications. The nNVIDIA\nR\n©\n485\nJetson Xavier\nTM\nNX benefits from new cloud-native support,486\nand accelerates the nVIDIA software stack in as little as487\n10 W with more than 10 times the performance of its widely488\nadopted predecessor Jetson\nTM\nTX2. With regards to the489\ncloud, the nVIDIA\nR\n©\nGeForce\nTM\nRTX 2080Ti GPU is con-490\nfigured on a computer with an Intel\nR\n©\nCore\nTM\ni7-9700k CPU491\n(@ 4.9GHz), and it is used to model AI from gathered image492\ndata, to be used as converged computing, deployed on-site493\nand performed at the edge. Here, AI trained in the cloud and494\ndeployed at the edge is based on YOLOv3, which is imple-495\nmented in PyTorch (based on Python). In addition, labeled496","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":9,"lines":{"from":9,"to":46}}}}],["ebba686b-efe3-4ddd-8746-b7d11a1384fe",{"pageContent":"and performed at the edge. Here, AI trained in the cloud and494\ndeployed at the edge is based on YOLOv3, which is imple-495\nmented in PyTorch (based on Python). In addition, labeled496\nimage data from the COCO 2017 dataset [40] are used; a total497\nof 4,004 images are randomly selected for training, while498\nthe remaining 1,001 images are used for testing. When col-499\nlaborating with the cloud (which enables global knowledge500\nsharing based on the image matting-based edge-cloud col-501\nlaboration mechanism), the distributed on-site edge devices502\nare capable of identifying unknown latent objects using their503\nlocal updated AI model (in the preliminary implementation504\nof the proposed framework, each on-site edge device owns505\nits trained local AI model identifying certain objects for its506\nspecific task, while the cloud receiving video streams from all507\nthe on-site edge devices leverages them to serve as converged508\ncomputing to be deployed and performed at the edge). The509","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":9,"lines":{"from":44,"to":59}}}}],["f199a4b1-3057-478c-ad5c-4f34cd6bf021",{"pageContent":"specific task, while the cloud receiving video streams from all507\nthe on-site edge devices leverages them to serve as converged508\ncomputing to be deployed and performed at the edge). The509\ntrained local AI models that did not include unknown objects510\nare updated, when unknown objects frequently appeared for511\ncertain times that are related to a confidence level ofra. Here,512\nthe unknown objects frequently appeared forβ=10 times at513\nwhichrais greater than or equal toα=1.0 %. The results514\nof the edge-cloud collaboration mechanism based on image515\nmatting withrafor unknown latent objects can be seen in516\nVOLUME 10, 202293753","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":9,"lines":{"from":57,"to":67}}}}],["de8911af-0dd3-4380-a041-2636212bbf43",{"pageContent":"Y.-Y. Chenet al.: Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration\nTABLE 3.Specifications of hardware and software used in the preliminary implementation.\nAlgorithm 2Implementation of AI by YOLOv3 That is Trained in the Cloud and Deployed at the Edge\n1.  Inputs (416×416,3)\n2.  CBL(Conv2d(size=3×3/2) Batch Normalization, and Leaky ReLU, 416×416,32)\n3.  1×Res Block(CBL(size=3×3/2) and ResNet, 208×208,64)\n4.  2×ResBlock(CBL(size=3×3/2)and ResNet, 104×104,128)\n5.  8×Res Block(CBL(size=3×3/2)and ResNet, 52×52,256)→line 13\n6.  8×Res Block(CBL(size=3×3/2)and ResNet,26×26,512)→line 10\n7.  4×Res Block(CBL(size=3×3/2)and ResNet, 13×13,1024)\n8.  CBL Set(CBL(size=1×1), CBL(size=3×3), CBL(size=1×1), CBL(size=3×3), CBL(size=1×1), 13×\n13,512)→CBL ((size=3×3), 13×13,255)+Conv2d ((size=1×1)13×13,255)→Output prediction\n9.  CBL(size=1×l)+up sampling(26×26,256)\n10.  concatenate(26×26,768) with line 6.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":10,"lines":{"from":1,"to":14}}}}],["44ee1990-9a50-4e6b-a1f3-f435b2da166a",{"pageContent":"13,512)→CBL ((size=3×3), 13×13,255)+Conv2d ((size=1×1)13×13,255)→Output prediction\n9.  CBL(size=1×l)+up sampling(26×26,256)\n10.  concatenate(26×26,768) with line 6.\n11.  CBL Set(CBL(size=1×1), CBL(size=3×3),CBL(size=1×1), CBLfsize=3×3),CBL(size=1×1), 26×\n26,256)→CBL ((size=3×3), 26×26,255)+Conv2d ((size=1×1)26×26,255)→Outputprediction\n12.  CBL(size=1×l)+up sampling(52×52,128)\n13.  concatenate(52×52,384) with line 5.\n14.  CBL Set(CBL(size=1×1), CBL(size=3×3), CBL(size=1×1), CBLfsize=3×3), CBL(size=1×1),52×\n52,255)→CBL ((size=3×3), 52×52,255)+Conv2d ((size=1×1), 52×52,255)→Output prediction\nTable 4. In Table 4 (a) original images with objects are shown;517\nin Table 4 (b) objects are detected through YOLOv3 object518\ndetection; in Table 4 (c) matted images with bounding boxes519\nby YOLOv3 for objects detected are shown (original images520\nare processed through image matting); and in Table 4 (d)521\nunknown objects may exist withra(=1.0(%)) (matted images522","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":10,"lines":{"from":12,"to":26}}}}],["f8d4568a-c85c-4797-b2d6-0cefc54f45c6",{"pageContent":"by YOLOv3 for objects detected are shown (original images520\nare processed through image matting); and in Table 4 (d)521\nunknown objects may exist withra(=1.0(%)) (matted images522\nare processed via image cutting), which can be identified by523\nupdated inference models at the edge. In the cloud with the524\nsame mechanism, unknown latent objects relating toraare525\nfound at the edge, and queries in the form of requesting class526\nlabels for unlabeled images are sent over the Internet to an527\nadministrator overcoming the labeling bottleneck; a practical528\nparadigm of LINE Notify push notification service is imple-529\nmented.530\nFigure 7 shows the detection of unknown objects at the531\nedge. The blue line indicates the situations in which unseen532\ntypes of objects frequently appeared, while the red line533\ndenotes the situations in which unseen objects that frequently534\nappeared (for 10 times) are identified successfully after the535","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":10,"lines":{"from":24,"to":39}}}}],["fa595a57-532e-4d76-800a-6103cc8b5b8a",{"pageContent":"types of objects frequently appeared, while the red line533\ndenotes the situations in which unseen objects that frequently534\nappeared (for 10 times) are identified successfully after the535\ntrained local models on the distributed on-site edge devices536\nare autonomously updated (the edge devices transmit a small537\nset of unknown latent object-contained images to the cloud,538\nwhile the cloud achieves global knowledge discovering from539\nreceived images and enables global knowledge sharing to540\ndeploy the model at the edge). As seen in Figure 7 that shows541\nthe interaction between the cloud and the edge for the model542\nupdates in the proposed edge-cloud collaborative framework,543\nthe local AI models are updated when unknown objects fre-544\nquently appeared forβ=10 times whenra(computed by545\nEquation (1)) is greater than or equal toα=0.01. Once546\nthe local AI models have been updated upon the proposed547\nedge-cloud collaboration mechanism, the edge devices have548","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":10,"lines":{"from":37,"to":52}}}}],["745b52bd-0c9b-42da-aacd-f1d026b41a3a",{"pageContent":"Equation (1)) is greater than or equal toα=0.01. Once546\nthe local AI models have been updated upon the proposed547\nedge-cloud collaboration mechanism, the edge devices have548\nthe ability of identifying the never-before-seen objects. This549\nis evidenced byrathat returns to the normal level, as shown550\nin Figure 7. The autonomous update of the trained local AI551\nmodels from the cloud to the edge through the proposed552\nedge-cloud collaboration mechanism is realized; thus, the553\nedge-cloud collaboration advances the whole system. With554\nthe proposed mechanism, the edge devices can transmit a555\nsmall set of unknown latent object-contained images to the556\ncloud. Figure 8 shows the bandwidth occupancy of the cloud557\nagainst the edge. As shown in the figure, the proposed edge-558\ncloud collaborative framework could save approximately559\n94% bandwidth of the backbone network. Based on this560\nmechanism, the distributed on-site edge devices need to trans-561","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":10,"lines":{"from":50,"to":65}}}}],["c4416f51-659c-4158-bf7c-df9d5ec6629f",{"pageContent":"cloud collaborative framework could save approximately559\n94% bandwidth of the backbone network. Based on this560\nmechanism, the distributed on-site edge devices need to trans-561\nmit only a small number of unknown latent object-contained562\n93754VOLUME 10, 2022","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":10,"lines":{"from":63,"to":67}}}}],["23c458c9-a002-4e2d-b7f1-777ba04327f0",{"pageContent":"Y.-Y. Chenet al.: Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration\nTABLE 4.Results of the edge-cloud collaboration mechanism, Algorithm 1, based on image matting and object detection with a confidence level of ra for\nunknown latent objects to be learned for a new AI model trained in the cloud and then deployed at the edge for real-time smart video surveillance.\nFIGURE 7.Detection of unknown objects at the edge (αspecified forra: 1.0%;βspecified for frequent occurrence of an\nunknown latent object relating tora: 10).\nimages to the cloud building new global AI for the model563\nupdates shown in Figure 7.564\nTo  diminish  the  risk  of  overfitting  that  the  AI565\nmodel  (YOLOv3)  trained  in  the  cloud  consolidates566\nVOLUME 10, 202293755","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":11,"lines":{"from":1,"to":10}}}}],["81f4e2bd-aed6-4abd-ac92-03169e2361a3",{"pageContent":"Y.-Y. Chenet al.: Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration\nFIGURE 8.Bandwidth occupancy of the cloud against the edge.\nTABLE 5.Results of the AI model that serves as converged computing at the edge for real-time smart video surveillance.\nclass-imbalanced image data from the distributed on-site edge567\ndevices, stratified k-fold cross-validation test [41], [42], [43]568\n(i.e., a variant of the commonly usedk-fold cross-validation569\nstrategy) evaluates the AI model. First, it returnskstratified570\nfolds (k=5 here) from the entire dataset split. Then, the571\nfollowing procedure is followed in a loop for each of the572\nkstratified folds: (1) the AI model is trained onk−1 of573\nthe stratified folds as the training data, and (2) the trained574\nmodel is validated on a remaining stratified one as the test set575\nto compute the performance measure (i.e., mAP@.5) [44].576\nFinally, the overall performance measure reported by the577","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":12,"lines":{"from":1,"to":14}}}}],["ea8bd9a0-d198-4d1e-be94-7c902834ddda",{"pageContent":"model is validated on a remaining stratified one as the test set575\nto compute the performance measure (i.e., mAP@.5) [44].576\nFinally, the overall performance measure reported by the577\nstratifiedk-fold cross-validation test is the average of the578\nmeasures computed in the loop. In the stratifiedk-fold cross-579\nvalidation test, the stratification ensures that, as the complete580\nset, each stratified fold contains approximately the same581\npercentage of data instances of each target class (the stratified582\nk-fold cross-validation test preserves approximate class ratios583\nin both training and test datasets).584\nTable 5 shows the results of the AI model that serves as585\nconverged computing at the edge for real-time smart video586\nsurveillance. Table 6 shows the results of the AI model trained587\nthrough the holdout cross-validation test (Table 6 (a)) versus588\nthe stratifiedk-fold cross-validation test (Table 6 (b)), where589\nF\n1","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":12,"lines":{"from":12,"to":28}}}}],["fa178d3d-c073-4a97-9d52-10f717144783",{"pageContent":"through the holdout cross-validation test (Table 6 (a)) versus588\nthe stratifiedk-fold cross-validation test (Table 6 (b)), where589\nF\n1\n-scorewas used as the metric; only four classes were590\nconsidered and shown in Table 6 (a). As shown in Table 5,591\nthe final AI model (model 6) achieves an excellent level of592\ndetection accuracy of 0.921 across all the six object classes,593\nwhich gives a value of F\n1\nscore of 0.812 across them as shown594\nin Table 6 (b). Table 7 presents the evaluation metrics from the595\ncomparison of the three state-of-the-art object detection tech-596\nniques in terms of mAP@.5, model size and FPS. As shown597\nin Table 7, serving as edge computing, YOLOv3 obtains an598\nFPS value of 15.7 under an achieved rate of up to 1.44 and599\n1.33 times to that of YOLOv4 and YOLOR, respectively.600\nAlso, YOLOv3 is a lightweight model with a model size601\nof 453.32 MB, which shrinks by approximately 69% and602\n61% against the model size of 1465.36 MB by YOLOv4603","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":12,"lines":{"from":25,"to":44}}}}],["058b6749-21db-44ea-b1b4-657cb6f585d5",{"pageContent":"Also, YOLOv3 is a lightweight model with a model size601\nof 453.32 MB, which shrinks by approximately 69% and602\n61% against the model size of 1465.36 MB by YOLOv4603\nand model size of 1163.73 MB by YOLOR, respectively.604\nIn summary, YOLOv3 is chosen and used as the primary AI605\ntechnique for the edge devices in the proposed framework606\n93756VOLUME 10, 2022","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":12,"lines":{"from":42,"to":48}}}}],["167c5a6d-a80e-415b-8ed7-deb1135a0972",{"pageContent":"Y.-Y. Chenet al.: Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration\nTABLE 6.Results of the AI model trained with (a) a holdout cross-validation test, (b) a stratified k -fold cross-validation test.\nTABLE 7.Evaluation metrics of the three state-of-the-art object detection techniques.\nas it reaches the minimum model size to be deployed and607\nachieves the highest FPS, where the similar performance in608\nmAP@.5 is given by it; it throwing a value of mAP@.5 of609\n0.944 across all the six object classes achieves an excellent610\nlevel of detection accuracy. Through collaborating with the611\ncloud, the distributed on-site edge devices identifying objects612\nin real time can identify specific target objects or potentially613\ndangerous targets forbidden in certain restricted areas for614\nreal-time smart video surveillance. As demonstrated in this615\nsection, with the edge-cloud collaboration mechanism, the616","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":13,"lines":{"from":1,"to":13}}}}],["03de37a4-b847-4756-9ec5-d083397f8bb5",{"pageContent":"dangerous targets forbidden in certain restricted areas for614\nreal-time smart video surveillance. As demonstrated in this615\nsection, with the edge-cloud collaboration mechanism, the616\ncloud assists on-site edge devices to detect unknown objects,617\nin images, using their updated inference model from the618\ncloud.619\nIV. CONCLUSION AND FUTURE WORK620\nFor the development of smart video surveillance applications,621\nedge computing, which is a promising complement of the622\ncloud, has pushed media data processing from the cloud to the623\nedge of the Internet, and can achieve fast response for latency-624\nsensitive video surveillance tasks. In this paper, a distributed625\nreal-time object detection framework based on edge-cloud626\ncollaboration for smart video surveillance applications has627\nbeen proposed. It achieves fast response for real-time video628\nsurveillance while serving as converged computing by which629\nmedia data from distributed edge devices are consolidated630","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":13,"lines":{"from":11,"to":27}}}}],["0708c79e-d6e3-4794-9c88-bef5a79823f1",{"pageContent":"been proposed. It achieves fast response for real-time video628\nsurveillance while serving as converged computing by which629\nmedia data from distributed edge devices are consolidated630\nthrough AI in the cloud; AI achieving global knowledge dis-631\ncovering in the cloud is then deployed, as global knowledge632\nsharing, remotely on distributed edge devices. Additionally,633\nits preliminary implementation has been demonstrated and634\nvalidated experimentally. In the future, a peer-to-peer offload-635\ning mechanism [22], [25] among the edge devices, to balance636\nreal-time workloads, for distributed/decentralized comput-637\ning in the proposed framework for smart video surveillance638\n(a video-/image-processing related practical application) will639\nVOLUME 10, 202293757","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":13,"lines":{"from":25,"to":37}}}}],["096f1e36-776a-420e-947f-4d4948f1d949",{"pageContent":"Y.-Y. Chenet al.: Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration\nbe developed. The framework proposed in this paper is640\nsuitable for internal networks where data security and privacy641\nprotection are not considered. In the future, some of the642\nmain cryptosystems, such as proxy re-encryption, homomor-643\nphic encryption and searchable encryption [12], [45], [46],644\n[47], [48], that can be implemented for data security and645\nprivacy protection will be investigated and included in the646\nproposed framework for networks that require them.647\nREFERENCES648\n[1] Y.-Y. Chen, M.-H. Chen, C.-M. Chang, F.-S. Chang, and Y.-H. Lin,649\n‘‘A smart home energy management system using two-stage non-intrusive650\nappliance load monitoring over fog-cloud analytics based on Tridium’s651\nNiagara framework for residential demand-side management,’’Sensors,652\nvol. 21, no. 8, p. 2883, Apr. 2021.653\n[2] M. A. Ezzat, M. A. A. E. Ghany, S. Almotairi, and M. A.-M. Salem,654","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":14,"lines":{"from":1,"to":16}}}}],["61e0360f-f3df-4a47-a6fd-360bc0bbb486",{"pageContent":"Niagara framework for residential demand-side management,’’Sensors,652\nvol. 21, no. 8, p. 2883, Apr. 2021.653\n[2] M. A. Ezzat, M. A. A. E. Ghany, S. Almotairi, and M. A.-M. Salem,654\n‘‘Horizontal review on video surveillance for smart cities: Edge devices,655\napplications, datasets, and future trends,’’Sensors, vol. 21, no. 9, p. 3222,656\nMay 2021.657\n[3] S. S. Ahamad and A.-S. K. Pathan, ‘‘A formally verified authentication658\nprotocol in secure framework for mobile healthcare during COVID-19-like659\npandemic,’’Connection Sci., vol. 33, no. 3, pp. 532–554, Jul. 2021.660\n[4] A. Song and M. Zhang, ‘‘Genetic programming for detecting target661\nmotions,’’Connection Sci., vol. 24, nos. 2–3, pp. 117–141, Sep. 2012.662\n[5] J. Ren, Y. Guo, D. Zhang, Q. Liu, and Y. Zhang, ‘‘Distributed and efficient663\nobject detection in edge computing: Challenges and solutions,’’IEEE664\nNetw., vol. 32, no. 6, pp. 137–143, Nov./Dec. 2018.665","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":14,"lines":{"from":14,"to":27}}}}],["3e64197c-c9f8-4201-853c-2b3504c1792e",{"pageContent":"object detection in edge computing: Challenges and solutions,’’IEEE664\nNetw., vol. 32, no. 6, pp. 137–143, Nov./Dec. 2018.665\n[6] Y. Zhang, J. Ren, J. Liu, C. Xu, H. Guo, and Y. Liu, ‘‘A survey on emerging666\ncomputing paradigms for big data,’’Chin. J. Electron., vol. 26, no. 1,667\npp. 1–12, Jan. 2017.668\n[7] H. Zhao, L. Yao, Z. Zeng, D. Li, J. Xie, W. Zhu, and J. Tang, ‘‘An edge669\nstreaming data processing framework for autonomous driving,’’Connec-670\ntion Sci., vol. 33, no. 2, pp. 173–200, Apr. 2021.671\n[8] Q. Xu, Z. Su, Q. Zheng, M. Luo, and B. Dong, ‘‘Secure content delivery672\nwith edge nodes to save caching resources for mobile users in green cities,’’673\nIEEE Trans. Ind. Informat., vol. 14, no. 6, pp. 2550–2559, Jun. 2018.674\n[9] T. Sultana and K. A. Wahid, ‘‘IoT-guard: Event-driven fog-based video675\nsurveillance system for real-time security management,’’IEEE Access,676\nvol. 7, pp. 134881–134894, 2019.677","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":14,"lines":{"from":26,"to":39}}}}],["47b067a4-b1dc-4432-906a-9c3b5fba1548",{"pageContent":"[9] T. Sultana and K. A. Wahid, ‘‘IoT-guard: Event-driven fog-based video675\nsurveillance system for real-time security management,’’IEEE Access,676\nvol. 7, pp. 134881–134894, 2019.677\n[10] T. Sultana and K. A. Wahid, ‘‘Choice of application layer protocols for678\nnext generation video surveillance using internet of video things,’’IEEE679\nAccess, vol. 7, pp. 41607–41624, 2019.680\n[11] W. Shi and S. Dustdar, ‘‘The promise of edge computing,’’Computer,681\nvol. 49, no. 5, pp. 78–81, 2016.682\n[12] X. Yan, P. Yin, Y. Tang, and S. Feng, ‘‘Multi-keywords fuzzy search683\nencryption supporting dynamic update in an intelligent edge network,’’684\nConnection Sci., vol. 34, no. 1, pp. 511–528, Dec. 2022.685\n[13] J. Ren, H. Guo, C. Xu, and Y. Zhang, ‘‘Serving at the edge: A scalable IoT686\narchitecture based on transparent computing,’’IEEE Netw., vol. 31, no. 5,687\npp. 96–105, Aug. 2017.688\n[14] Y.-Y. Chen, Y. H. Lin, C. C. Kung, M. H. Chung, and I. H. Yen, ‘‘Design689","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":14,"lines":{"from":37,"to":51}}}}],["1f0e2b3e-8196-4883-a092-2def281442f1",{"pageContent":"architecture based on transparent computing,’’IEEE Netw., vol. 31, no. 5,687\npp. 96–105, Aug. 2017.688\n[14] Y.-Y. Chen, Y. H. Lin, C. C. Kung, M. H. Chung, and I. H. Yen, ‘‘Design689\nand implementation of cloud analytics-assisted smart power meters con-690\nsidering advanced artificial intelligence as edge analytics in demand-side691\nmanagement for smart homes,’’Sensors, vol. 19, no. 9, p. 2047, 2019.692\n[15] D. Aishwarya and R. I. Minu, ‘‘Edge computing based surveillance693\nframework for real time activity recognition,’’ICT Exp., vol. 7, no. 2,694\npp. 182–186, Jun. 2021.695\n[16] S. Yi, Z. Hao, Z. Qin, and Q. Li, ‘‘Fog computing: Platform and appli-696\ncations,’’ inProc. 3rd IEEE Workshop Hot Topics Web Syst. Technol.697\n(HotWeb), Nov. 2015, pp. 73–78.698\n[17] T. Abdullah, A. Anjum, M. F. Tariq, Y. Baltaci, and N. Antonopoulos,699\n‘‘Traffic monitoring using video analytics in clouds,’’ inProc. IEEE/ACM700\n7th Int. Conf. Utility Cloud Comput., Dec. 2014, pp. 39–48.701","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":14,"lines":{"from":49,"to":63}}}}],["a0bde366-b69b-4544-b852-287e8ae40325",{"pageContent":"‘‘Traffic monitoring using video analytics in clouds,’’ inProc. IEEE/ACM700\n7th Int. Conf. Utility Cloud Comput., Dec. 2014, pp. 39–48.701\n[18] A. Anjum, T. Abdullah, M. F. Tariq, Y. Baltaci, and N. Antonopoulos,702\n‘‘Video stream analysis in clouds: An object detection and classification703\nframework for high performance video analytics,’’IEEE Trans. Cloud704\nComput., vol. 7, no. 4, pp. 1152–1167, Oct. 2019.705\n[19] M. A. Hossain, ‘‘Framework for a cloud-based multimedia surveillance706\nsystem,’’Int. J. Distrib. Sens. Netw., vol. 10, no. 5, p. 135257, May 2014.707\n[20] L. Valentín, S. A. Serrano, R. O. García, A. Andrade, M. A. Palacios-708\nAlonso, and L. E. Sucar, ‘‘A cloud-based architecture for smart video709\nsurveillance,’’Int. Arch. Photogramm., Remote Sens. Spatial Inf. Sci.,710\nvol. XLII-4/W3, pp. 99–104, 2017.711\n[21] C.-S. Yoon, H.-S. Jung, J.-W. Park, H.-G. Lee, C.-H. Yun, and Y. W. Lee,712\n‘‘A cloud-based UTOPIA smart video surveillance system for smart713","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":14,"lines":{"from":62,"to":75}}}}],["f39025be-976c-4091-93e0-663fd8bf8081",{"pageContent":"vol. XLII-4/W3, pp. 99–104, 2017.711\n[21] C.-S. Yoon, H.-S. Jung, J.-W. Park, H.-G. Lee, C.-H. Yun, and Y. W. Lee,712\n‘‘A cloud-based UTOPIA smart video surveillance system for smart713\ncities,’’Appl. Sci., vol. 10, no. 18, p. 6572, Sep. 2020.714\n[22] Q. Zhang, Z. Yu, W. Shi, and H. Zhong, ‘‘Demo abstract: EVAPS: Edge715\nvideo analysis for public safety,’’ inProc. IEEE/ACM Symp. Edge Comput.716\n(SEC), Oct. 2016, pp. 121–122.717\n[23] Z. Xu, J. Li, and M. Zhang, ‘‘A surveillance video real-time analysis system718\nbased on edge-cloud and FL-YOLO cooperation in coal mine,’’IEEE719\nAccess, vol. 9, pp. 68482–68497, 2021.720\n[24] R. Ke, Y. Zhuang, Z. Pu, and Y. Wang, ‘‘A smart, efficient, and reli-721\nable parking surveillance system with edge artificial intelligence on IoT722\ndevices,’’IEEE Trans. Intell. Transp. Syst., vol. 22, no. 8, pp. 4962–4974,723\nAug. 2020.724\n[25] R. Rajavel, S. K. Ravichandran, K. Harimoorthy, P. Nagappan, and725","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":14,"lines":{"from":73,"to":87}}}}],["7dfaf66c-7d3d-4011-bba8-1516dfb4e9f1",{"pageContent":"devices,’’IEEE Trans. Intell. Transp. Syst., vol. 22, no. 8, pp. 4962–4974,723\nAug. 2020.724\n[25] R. Rajavel, S. K. Ravichandran, K. Harimoorthy, P. Nagappan, and725\nK. R. Gobichettipalayam, ‘‘IoT-based smart healthcare video surveillance726\nsystem using edge computing,’’J. Ambient Intell. Hum. Comput., vol. 13,727\nno. 6, pp. 3195–3207, Jun. 2022.728\n[26] L. Duan, Y. Lou, S. Wang, W. Gao, and Y. Rui, ‘‘AI-oriented large-scale729\nvideo management for smart city: Technologies, standards, and beyond,’’730\nIEEE Multimedia Mag., vol. 26, no. 2, pp. 8–20, Apr. 2019.731\n[27] Y.-D. Zhang, Z.-J. Yang, H.-M. Lu, X.-X. Zhou, P. Phillips, Q.-M. Liu, and732\nS.-H. Wang, ‘‘Facial emotion recognition based on biorthogonal wavelet733\nentropy, fuzzy support vector machine, and stratified cross validation,’’734\nIEEE Access, vol. 4, pp. 8375–8385, 2016.735\n[28] T. G. Rodrigues, K. Suto, H. Nishiyama, and N. Kato, ‘‘Hybrid method for736","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":14,"lines":{"from":85,"to":98}}}}],["f117c5ed-af69-4702-9fe3-2ada0dfdcd44",{"pageContent":"IEEE Access, vol. 4, pp. 8375–8385, 2016.735\n[28] T. G. Rodrigues, K. Suto, H. Nishiyama, and N. Kato, ‘‘Hybrid method for736\nminimizing service delay in edge cloud computing through VM migration737\nand transmission power control,’’IEEE Trans. Comput., vol. 66, no. 5,738\npp. 810–819, May 2017.739\n[29] M. B. Dillencourt, H. Samet, and M. Tamminen, ‘‘A general approach740\nto connected-component labeling for arbitrary image representations,’’741\nJ. ACM, vol. 39, no. 2, pp. 253–280, Apr. 1992.742\n[30] E. B. Varghese and S. M. Thampi, ‘‘Towards the cognitive and psychologi-743\ncal perspectives of crowd behaviour: A vision-based analysis,’’Connection744\nSci., vol. 33, no. 2, pp. 380–405, Apr. 2021.745\n[31] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, ‘‘You only look once:746\nUnified, real-time object detection,’’ inProc. IEEE Conf. Comput. Vis.747\nPattern Recognit. (CVPR), Jun. 2016, pp. 779–788.748\n[32] J. Redmon and A. Farhadi, ‘‘YOLO9000: Better, faster, stronger,’’ in749","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":14,"lines":{"from":97,"to":111}}}}],["d46ea7a9-299e-4498-a832-1146f1f4fc2b",{"pageContent":"Pattern Recognit. (CVPR), Jun. 2016, pp. 779–788.748\n[32] J. Redmon and A. Farhadi, ‘‘YOLO9000: Better, faster, stronger,’’ in749\nProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017,750\npp. 7263–7271.751\n[33] J. Redmon and A. Farhadi, ‘‘YOLOv3: An incremental improvement,’’ in752\nProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2018.753\n[34] D. Acharya, K. Khoshelham, and S. Winter, ‘‘Real-time detection and754\ntracking of pedestrians in CCTV images using a deep convolutional neural755\nnetwork,’’ inProc. Therapeutic 4th Annu. Conf. Research@Locate, 2017,756\npp. 3–6.757\n[35] R. Girshick, ‘‘Fast R-CNN,’’ inProc. IEEE Int. Conf. Comput. Vis. (ICCV),758\nDec. 2015, pp. 1440–1448.759\n[36] S. Ren, K. He, R. Girshick, and J. Sun, ‘‘Faster R-CNN: Towards real-760\ntime object detection with region proposal networks,’’IEEE Trans. Pattern761\nAnal. Mach. Intell., vol. 39, no. 6, pp. 1137–1149, Jun. 2017.762","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":14,"lines":{"from":110,"to":124}}}}],["ded9c0ff-fc73-491e-bd7e-523765761d04",{"pageContent":"time object detection with region proposal networks,’’IEEE Trans. Pattern761\nAnal. Mach. Intell., vol. 39, no. 6, pp. 1137–1149, Jun. 2017.762\n[37] T.-Y. Lin, P. Dollar, R. Girshick, K. He, B. Hariharan, and S. Belongie,763\n‘‘Feature pyramid networks for object detection,’’ inProc. IEEE Conf.764\nComput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 2117–2125.765\n[38] C.-Y. Wang, I.-H. Yeh, and H.-Y. M. Liao, ‘‘You only learn one represen-766\ntation: Unified network for multiple tasks,’’ 2021,arXiv:2105.04206.767\n[39] A. Bochkovskiy, C.-Y. Wang, and H.-Y. M. Liao, ‘‘YOLOv4: Optimal768\nspeed and accuracy of object detection,’’ 2020,arXiv:2004.10934.769\n[40] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,770\nand C. L. Zitnick, ‘‘Microsoft COCO: Common objects in context,’’ in771\nProc. Eur. Conf. Comput. Vis. (ECCV), in Lecture Notes in Computer772\nScience, 2014, pp. 740–755.773\n[41] P. Dhivya and S. Vasuki, ‘‘Wavelet based MRI brain image classification774","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":14,"lines":{"from":123,"to":136}}}}],["dc1d3c81-d3c3-42c6-8a9f-0bb2546977a4",{"pageContent":"Proc. Eur. Conf. Comput. Vis. (ECCV), in Lecture Notes in Computer772\nScience, 2014, pp. 740–755.773\n[41] P. Dhivya and S. Vasuki, ‘‘Wavelet based MRI brain image classification774\nusing radial basis function in SVM,’’ inProc. 2nd Int. Conf. Trends775\nElectron. Informat. (ICOEI), May 2018, pp. 1–9.776\n[42] E. M. Dogo, N. I. Nwulu, B. Twala, and C. O. Aigbavboa, ‘‘Empirical com-777\nparison of approaches for mitigating effects of class imbalances in water778\nquality anomaly detection,’’IEEE Access, vol. 8, pp. 218015–218036,779\n2020.780\n[43] D. Zhang, Z. Chen, M. K. Awad, N. Zhang, H. Zhou, and X. S. Shen,781\n‘‘Utility-optimal resource management and allocation algorithm for energy782\nharvesting cognitive radio sensor networks,’’IEEE J. Sel. Areas Commun.,783\nvol. 34, no. 12, pp. 3552–3565, Dec. 2016.784\n[44] Z. Song, J. Yang, D. Zhang, S. Wang, and Z. Li, ‘‘Semi-supervised dim785\nand small infrared ship detection network based on Haar wavelet,’’IEEE786","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":14,"lines":{"from":134,"to":148}}}}],["4abc5523-5596-457c-90ce-c5a7b05d6d3f",{"pageContent":"vol. 34, no. 12, pp. 3552–3565, Dec. 2016.784\n[44] Z. Song, J. Yang, D. Zhang, S. Wang, and Z. Li, ‘‘Semi-supervised dim785\nand small infrared ship detection network based on Haar wavelet,’’IEEE786\nAccess, vol. 9, pp. 29686–29695, 2021.787\n93758VOLUME 10, 2022","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":14,"lines":{"from":146,"to":150}}}}],["9787f160-8493-4070-be3d-fff876f83644",{"pageContent":"Y.-Y. Chenet al.: Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration\n[45] R. Gupta and U. P. Rao, ‘‘A hybrid location privacy solution for mobile788\nLBS,’’Mobile Inf. Syst., vol. 2017, pp. 1–11, Jun. 2017.789\n[46] L. Wu, X. Wei, L. Meng, S. Zhao, and H. Wang, ‘‘Privacy-preserving790\nlocation-based traffic density monitoring,’’Connection Sci., vol. 34, no. 1,791\npp. 874–894, Dec. 2022.792\n[47] P. Yang, X. Gui, J. An, and F. Tian, ‘‘An efficient secret key homomorphic793\nencryption used in image processing service,’’Secur. Commun. Netw.,794\nvol. 2017, pp. 11–13, Mar. 2017.795\n[48] J. Zhang, B. Chen, Y. Zhao, X. Cheng, and F. Hu, ‘‘Data security and796\nprivacy-preserving in edge computing paradigm: Survey and open issues,’’797\nIEEE Access, vol. 6, pp. 18209–18237, 2018.798\nYUNG-YAO CHEN(Member, IEEE) received799\nthe B.S. and M.S. degrees in electrical and con-800\ntrol engineering from the National Chiao Tung801\nUniversity, Hsinchu, Taiwan, in 2004 and 2006,802","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":15,"lines":{"from":1,"to":16}}}}],["d7042b15-de8c-4e03-ba86-eb0a31b75f46",{"pageContent":"YUNG-YAO CHEN(Member, IEEE) received799\nthe B.S. and M.S. degrees in electrical and con-800\ntrol engineering from the National Chiao Tung801\nUniversity, Hsinchu, Taiwan, in 2004 and 2006,802\nrespectively, and the Ph.D. degree in electri-803\ncal engineering from Purdue University, USA,804\nin 2013. Before being a Faculty, he worked with805\nHP Labs–Printing and Content Delivery Labo-806\nratory (HPL–PCDL) about one year. He is cur-807\nrently an Associate Professor with the Department808\nof Electronic and Computer Engineering, National Taiwan University of809\nScience and Technology, Taipei, Taiwan. His current research interests810\ninclude vision-based automation, automated/wisdom factory, self-driving811\ncar, and human–computer interaction. He is a member of the Golden812\nKey International Honor Society and Phi Tau Phi. He was a recipient of813\nthe Ta-Yu Wu Memorial Award from Taiwan’s Ministry of Science and814\nTechnology (MOST).815\nYU-HSIU LINreceived the Ph.D. degree in816","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":15,"lines":{"from":13,"to":30}}}}],["78b4fc8a-06dc-48a2-b662-c7f004e60481",{"pageContent":"the Ta-Yu Wu Memorial Award from Taiwan’s Ministry of Science and814\nTechnology (MOST).815\nYU-HSIU LINreceived the Ph.D. degree in816\nmechanical and electrical engineering from the817\nGraduate Institute of Mechanical and Electrical818\nEngineering, National Taipei University of Tech-819\nnology, Taipei, Taiwan, in 2014. To his work and820\nresearch experience, from October 2014 to August821\n2017, he worked with the Smart Network System822\nInstitute, Institute for Information Industry (III),823\nTaiwan, and worked as a full-time Senior Engineer.824\nFrom September 2017 to July 2018, he was an825\nAssistant Professor with the Department of Computer Science and Infor-826\nmation Management, Providence University, Taichung City, Taiwan. From827\nAugust 2018 to July 2019, he was an Assistant Professor with the Depart-828\nment of Electrical Engineering, Southern Taiwan University of Science and829\nTechnology, Tainan City, Taiwan. Besides, from August 2019 to January830","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":15,"lines":{"from":28,"to":44}}}}],["9e8cd049-1e56-483c-ae15-3de059d07445",{"pageContent":"ment of Electrical Engineering, Southern Taiwan University of Science and829\nTechnology, Tainan City, Taiwan. Besides, from August 2019 to January830\n2021, he was an Assistant Professor with the Department of Electrical831\nEngineering, Ming Chi University of Technology, New Taipei City, Taiwan.832\nSince February 2021, he has been with the Graduate Institute of Automation833\nTechnology, National Taipei University of Technology, where he is cur-834\nrently an Assistant Professor. His current research interests include the IoT835\ntechnologies, fog-/edge-cloud computing, and artificial intelligence/deep836\nlearning/computational intelligence in smart grid.837\nYU-CHEN HU(Senior Member, IEEE) received838\nthe Ph.D. degree in computer science and infor-839\nmation engineering  from the Department  of840\nComputer Science and Information Engineer-841\ning, National Chung Cheng University, Chiayi,842\nTaiwan, in 1999. He is currently a Professor with843","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":15,"lines":{"from":43,"to":57}}}}],["619ff864-9f49-40cb-8a36-f9e1ceb0dff4",{"pageContent":"mation engineering  from the Department  of840\nComputer Science and Information Engineer-841\ning, National Chung Cheng University, Chiayi,842\nTaiwan, in 1999. He is currently a Professor with843\nthe Department of Computer Science and Informa-844\ntion Management, Providence University, Sha-Lu,845\nTaiwan. His research interests include image and846\nsignal processing, data compression, information847\nhiding, information security, computer networks, and machine learning.848\nCHIH-HSIEN HSIA(Member, IEEE) was born in849\nTaipei City, Taiwan, in 1979. He received the Ph.D.850\ndegree from Tamkang University, New Taipei,851\nTaiwan, in 2010.852\nIn 2007, he was a Visiting Scholar with Iowa853\nState University, Ames, IA, USA. From 2010854\nto 2013, he was a Postdoctoral Research Fel-855\nlow with the Department of Electrical Engineer-856\ning, National Taiwan University of Science and857\nTechnology, Taipei. From 2013 to 2015, he was858","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":15,"lines":{"from":54,"to":72}}}}],["eed61c70-6871-49f7-afb8-39823d35e599",{"pageContent":"to 2013, he was a Postdoctoral Research Fel-855\nlow with the Department of Electrical Engineer-856\ning, National Taiwan University of Science and857\nTechnology, Taipei. From 2013 to 2015, he was858\nan Assistant Professor with the Department of Electrical Engineering,859\nChinese Culture University, Taiwan. He was an Associate Professor with860\nthe Chinese Culture University and the National Ilan University, Taiwan,861\nfrom 2015 to 2017. From 2019 to 2020, he was the Director of the Research862\nPlanning Division, Research & Development, National Ilan University. He is863\ncurrently a Professor and the Chairperson with the Department of Computer864\nScience and Information Engineering, National Ilan University. He is also the865\nDirector of the Multimedia & Intelligent Technical Laboratory, National Ilan866\nUniversity. His research interests include DSP IC Design, AI in computer867\nvision, and cognitive learning. He was received the Outstanding Young868","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":15,"lines":{"from":69,"to":82}}}}],["42b09dbd-a9d9-4949-ad36-252587711624",{"pageContent":"University. His research interests include DSP IC Design, AI in computer867\nvision, and cognitive learning. He was received the Outstanding Young868\nScholar Award of the Taiwan Association of Systems Science and Engi-869\nneering in 2020 and the Outstanding Young Scholar Award of the Computer870\nSociety of the Republic of China in 2018. He is the Chapter Chair of the871\nIEEE Young Professionals Group, Taipei Section, and the Director of the IET872\nTaipei Local Network. He has served as an Associate Editor for theJournal873\nof Imaging Science and Technology, theJournal of Imaging, and theJournal874\nof Computers.875\nYI-AN LIANreceived the B.S. degree in elec-876\ntronic engineering from the National Kaohsiung877\nUniversity of Science and Technology, Kaohsiung,878\nTaiwan, in 2019. He is currently pursuing the879\nM.S. degree in electronic and computer engineer-880\ning with the National Taiwan University of Sci-881\nence and Technology, Taipei, Taiwan. His current882","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":15,"lines":{"from":81,"to":96}}}}],["c9f9ad67-a04b-4fc1-85e0-01fd9367b223",{"pageContent":"M.S. degree in electronic and computer engineer-880\ning with the National Taiwan University of Sci-881\nence and Technology, Taipei, Taiwan. His current882\nresearch interests include digital image processing883\nand deep learning.884\nSIN-YE JHONGreceived the B.S. degree in elec-885\ntrical engineering from Chinese Culture Univer-886\nsity, Taipei, Taiwan, in 2017, and the M.E. degree887\nfrom the Graduate Institute of Automation Tech-888\nnology, National Taipei University of Technol-889\nogy, Taipei, in 2019. He is currently pursuing890\nthe Ph.D. degree with the Department of Engi-891\nneering Science, National Cheng Kung University,892\nTainan, Taiwan. His research interests include dig-893\nital image and video processing, computer vision,894\nand deep learning. He is also the Vice Chair of Young Professionals with the895\nIET Taipei Local Network.896\n897\nVOLUME 10, 202293759","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Distributed_Real-Time_Object_Detection_Based_on_Edge-Cloud_Collaboration_for_Smart_Video_Surveillance_Applications.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","Subject":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","Creator":"LaTeX with hyperref package","Producer":"pdfTeX-1.40.13; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)","CreationDate":"D:20220909211302+05'30'","ModDate":"D:20220912122735-04'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:publisher":"IEEE","dc:description":"IEEE Access;2022;10; ;10.1109/ACCESS.2022.3203053","dc:title":"Distributed Real-Time Object Detection Based on Edge-Cloud Collaboration for Smart Video Surveillance Applications","dc:subject":"Cloud computingedge computingedge-cloud collaborationobject detectionvideo surveillance","dc:creator":"Yung-Yao ChenYu-Hsiu LinYu-Chen HuChih-Hsien HsiaYi-An LianSin-Ye Jhong","prism:publicationname":"IEEE Access","prism:startingpage":"93745","prism:coverdisplaydate":"2022","prism:doi":"10.1109/ACCESS.2022.3203053","prism:volume":"10","prism:endingpage":"93759","jav:journal_article_version":"VoR"}},"totalPages":15},"loc":{"pageNumber":15,"lines":{"from":94,"to":112}}}}],["1ef32f26-7bfb-44ee-af04-22e18b93bbd8",{"pageContent":"Master of Science in Computer Science\nFebruary 2020\nReal Time Object Detection and\nRecognition\nUsing Deep Learning Methods\nSai Krishna Chadalawada\nFaculty of Computing, Blekinge Institute of Technology, 371 79 Karlskrona, Sweden","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":1,"lines":{"from":1,"to":7}}}}],["37ece6c0-bf62-4d38-ba9b-240a31672d91",{"pageContent":"This thesis is submitted to the Faculty of Computing at Blekinge Institute of Technology in\npartial fulfilment of the requirements for the degree of Master of Science in Computer Science.\nThe thesis is equivalent to 20 weeks of full time studies.\nThe authors declare that they are the sole authors of this thesis and that they have not used\nany sources other than those listed in the bibliography and identified as references. They further\ndeclare that they have not submitted this thesis at any other institution to obtain a degree.\nContact Information:\nAuthor(s):\nSai Krishna Chadalawada\nE-mail: sach17@student.bth.se\nUniversity advisor:\nDr. Hüseyin Kusetoğullari\nDepartment of Computer Science and Engineering\nBlekinge Institute of Technology, Karlskrona, Sweden\nFaculty of ComputingInternet   :   www.bth.se\nBlekinge Institute of TechnologyPhone:   +46 455 38 50 00\nSE–371 79 Karlskrona, SwedenFax:   +46 455 38 50 57","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":2,"lines":{"from":1,"to":17}}}}],["c7068c12-a29a-4d31-b07f-a1f4e26838c4",{"pageContent":"Abstract\nBackground.The driving conditions of construction vehicles and their surround-\ning environment is different from the traditional transportation vehicles. As a result,\nthey face unique challenges while operating in the construction/evacuation sites.\nTherefore, there needs to be research carried-out to address these challenges while\nimplementing autonomous driving, although the learning approach for construction\nvehicles is the same as for traditional transportation vehicles such as cars.\nObjectives.The following objectives have been identified to fulfil the aim of this\nthesis work. To identify suitable and highly efficient CNN models for real-time object\nrecognition and tracking of construction vehicles. Evaluate the classification perfor-\nmance of these CNN models.  Compare the results among one another and present\nthe results.\nMethods.To answer the research questions,  Literature review and Experiment","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":3,"lines":{"from":1,"to":13}}}}],["3a98ba32-ad41-4279-b7ce-113e265bf0b8",{"pageContent":"mance of these CNN models.  Compare the results among one another and present\nthe results.\nMethods.To answer the research questions,  Literature review and Experiment\nhave been identified as the appropriate research methodologies.  Literature review\nhas been performed to identify suitable object detection models for real-time object\nrecognition and tracking.  Following this, experiments have been conducted to eval-\nuate the performance of the selected object detection models.\nResults.Faster R-CNN model, YOLOv3 and Tiny-YOLOv3 have been identified\nfrom the literature review as the most suitable and efficient algorithms for detecting\nand tracking scaled construction vehicles in real-time. The classification performance\nof these algorithms has been calculated and compared with each other. The results\nhave been presented.\nConclusions.The F1 score and accuracy of YOLOv3 has been found to be better\namongst the algorithms, followed by Faster R-CNN. Therefore, it has been concluded","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":3,"lines":{"from":11,"to":24}}}}],["4b1a9cdd-8f28-4266-bce5-db865f504645",{"pageContent":"have been presented.\nConclusions.The F1 score and accuracy of YOLOv3 has been found to be better\namongst the algorithms, followed by Faster R-CNN. Therefore, it has been concluded\nthat YOLOv3 is the best algorithm in the real-time detection and tracking of scaled\nconstruction vehicles. The results are similar to the classification performance com-\nparison of these three algorithms provided in the literature.\nKeywords:Object detection and recognition, Deep Learning, Classification perfor-\nmance.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":3,"lines":{"from":22,"to":29}}}}],["727cd485-6c12-40b5-b369-334026b8cc1f",{"pageContent":"Acknowledgments\nI would first like to thank my thesis advisor Dr.  Hüseyin Kusetoğullari for provid-\ning me with continuous support and steering me in the right direction whenever I\nneeded it. This thesis would not have been possible without his expert guidance and\nmotivation.\nI would also like to thank Ryan Ruvald at the PDRL - BTH, for providing me\nthis thesis opportunity and valuable learning experience.\niii","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":5,"lines":{"from":1,"to":8}}}}],["4193dc9b-4b31-4fef-ac03-006d841bf3f6",{"pageContent":"Contents\nAbstracti\nAcknowledgmentsiii\n1  Introduction1\n1.1   Aim & Objectives.............................    3\n1.1.1   Problem Statement........................    3\n1.1.2   Document Structure.......................    4\n2  Background & Related Work7\n2.1   Machine Learning.............................    7\n2.1.1   Types...............................    7\n2.2   Artificial Neural Networks........................    8\n2.2.1   Backpropagation   .  .  .......................  11\n2.3   Computer Vision.............................  11\n2.4   Convolutional Neural Networks.....................  11\n2.4.1   Convolutional Layer.......................  12\n2.4.2   Pooling Layer...........................  12\n2.5   YOLOv3..................................  13\n2.5.1   Architecture............................  14\n2.6   Faster R-CNN...............................  16\n2.6.1   Architecture............................  17\n2.7   Tiny-YOLOv3...............................  19","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":7,"lines":{"from":1,"to":21}}}}],["ee758998-1218-48e3-bcce-6398d7904ca8",{"pageContent":"2.6   Faster R-CNN...............................  16\n2.6.1   Architecture............................  17\n2.7   Tiny-YOLOv3...............................  19\n2.7.1   Architecture............................  19\n2.8   Related Work...............................  20\n3  Experimental Results25\n3.1   Research Questions............................  25\n3.2   Literature Review.............................  26\n3.2.1   Search Process..........................  26\n3.2.2   Inclusion and Exclusion Criteria.................  26\n3.3   Experiment................................  26\n3.3.1   Experimental Setup........................  27\n3.3.2   Training  ..............................  29\n3.3.3   Metrics..............................  30\n3.4   Results...................................  31\nv","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":7,"lines":{"from":19,"to":34}}}}],["a0d3b88f-0afd-4a6a-a10a-2f79919b7d9b",{"pageContent":"4  Analysis and Discussion35\n4.1   Literature Review.............................  35\n4.2   Experiment................................  36\n4.2.1   Accuracy..............................  36\n4.2.2   F\n1\nScore..............................  36\n4.3   Validity Threats..............................  38\n4.3.1   Internal Validity.........................  38\n4.3.2   External Validity.........................  38\n4.3.3   Conclusion Validity .  .  ......................  39\n5  Conclusions and Future Work41\n5.1   Conclusion.................................  41\n5.2   Implications to Practice.........................  42\n5.3  FutureWork................................  42\nReferences43\n6  Appendix49\nvi","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":8,"lines":{"from":1,"to":18}}}}],["84a41406-32c6-4d52-b204-c909a4c96199",{"pageContent":"List of Figures\n1.1  Hauler ...................................    2\n1.2   Excavator.................................    2\n1.3   Wheeled Loader..............................    3\n1.4   PDRL site.................................    4\n1.5   Exemplar Excavation Site [1].......................    5\n1.6   Exemplar Construction Site [2].....................    5\n2.1   A Simple Artificial Neural Network [3].................    9\n2.2   Structure of a single perceptron or neuron [4]..............  10\n2.3   Multi-layer Artificial Neural Network [5]................  10\n2.4   Feature Filters of Front, Middle and Rear-End Layers in a CNN [6]   .    12\n2.5   Pooling Layer [7].............................  13\n2.6   Example of 2x2 Max-pooling [7].....................  13\n2.7   Image Splitting and Bounding-boxes Prediction [8]..........  14\n2.8   The Architecture of YOLOv3 [9].....................  15\n2.9   Structure of Faster R-CNN model [10].................  18","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":9,"lines":{"from":1,"to":16}}}}],["7cb79dbe-92e5-4ecc-8733-cf2ec4c7f66b",{"pageContent":"2.7   Image Splitting and Bounding-boxes Prediction [8]..........  14\n2.8   The Architecture of YOLOv3 [9].....................  15\n2.9   Structure of Faster R-CNN model [10].................  18\n2.10  The Architecture of Faster R-CNN [11].................  19\n2.11  Architecture of Tiny-YOLOv3 [12]...................  21\n3.1   Dataset Labelling.............................  29\n3.2   Training & Classification loss......................  30\n3.3   Predictions made by Faster R-CNN...................  32\n3.4   Predictions made by YOLOv3......................  33\n3.5   Predictions made by Tiny-YOLOv3...................  33\n4.1   Graphical Representation of Accuracy..................  37\n4.2   Graphical representation of F1 score..................  38\n6.1   Detection of real vehicles.........................  50\n6.2   Detection of real vehicles.........................  50\n6.3   Detection of real vehicles.........................  51","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":9,"lines":{"from":14,"to":28}}}}],["58f8617d-00e1-4a0c-a295-e3499dfa8c8d",{"pageContent":"6.1   Detection of real vehicles.........................  50\n6.2   Detection of real vehicles.........................  50\n6.3   Detection of real vehicles.........................  51\n6.4   Classification loss graph plotted using TensorBoard..........  51\n6.5   Detection of real vehicles.........................  52\n6.6   Detection of real vehicles.........................  52\nvii","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":9,"lines":{"from":26,"to":32}}}}],["ba333abc-7ed3-48f0-a163-ae407f923bc5",{"pageContent":"List of Tables\n3.1   Hardware Environment..........................  27\n4.1   Calculated Values of the Algorithms...................  36\n4.2   Calculated Values of the Algorithms...................  37\n4.3   Calculated Values of the Algorithms...................  37\nix","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":11,"lines":{"from":1,"to":6}}}}],["56ecd518-9bd5-4337-bf2a-bc8d701d313d",{"pageContent":"Chapter 1\nIntroduction\nThe process of recognizing objects in videos and images is known as Object recogni-\ntion. This computer vision technique enables the autonomous vehicles to classify and\ndetect objects in real-time [13].  An autonomous vehicle is an automobile that has\nthe ability to sense and react to its environment so as to navigate without the help or\ninvolvement of a human [14]. The object detection and recognition are considered to\nbe one of the most important tasks as this is what helps the vehicle detect obstacles\nand set the future courses of the vehicle [14]. Therefore, it is necessary for the object\ndetection algorithms to be highly accurate.\nThough there are many machine learning and deep learning algorithms for object\ndetection and recognition, such as Support vector machine (SVM), Convolutional\nNeural Networks (CNNs), Regional Convolutional Neural Networks (R-CNNs), You\nOnly Look Once (YOLO) model etc., it is important to choose the right algorithm for","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":13,"lines":{"from":1,"to":14}}}}],["8d073e1c-72d5-433c-8ebf-b4cb7f25f502",{"pageContent":"Neural Networks (CNNs), Regional Convolutional Neural Networks (R-CNNs), You\nOnly Look Once (YOLO) model etc., it is important to choose the right algorithm for\nautonomous driving as it requires real-time object detection and recognition.  Since\nmachines cannot detect the objects in an image instantly like humans, it is really\nnecessary for the algorithms to be fast and accurate and to detect the objects in\nreal-time [8], so that the vehicle controllers solve optimization problems at least at\na frequency of one per second [14].\nThis thesis is part of a collaborative project between Project Development Re-\nsearch Laboratory - BTH and Volvo CE. The main aim of the project is to train\na model to recognize three types of small scale vehicles – Hauler, Excavator and\nWheeled Loader shown in Figure 1.1, Figure 1.2 and Figure 1.3 respectively, as an\ninitial step towards implementing this on new ideas related to machine interaction,","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":13,"lines":{"from":13,"to":24}}}}],["6199307e-df77-43d8-9874-5526bc0f39a1",{"pageContent":"Wheeled Loader shown in Figure 1.1, Figure 1.2 and Figure 1.3 respectively, as an\ninitial step towards implementing this on new ideas related to machine interaction,\nintelligent machine navigation systems which includes autonomous driving, on a scale\nsite where it is cheaper and easier to radically innovate and later implement the same\nin the real world.\nThe scale site, which is shown in the Figure 1.4, is located at the PDRL lab\nin BTH. It is a small-scale representation of the real-world construction/excavation\nsites, which can be seen in the Figures 1.5 and 1.6.  The construction vehicles and\nconstruction site environment are quite different from that of city transportation\nenvironment, both of them serving unique purposes.  Autonomous construction ve-\nhicles are ground-breaking as they can address the labor shortage problem as well as\nperform tasks for lengthy periods of time with minimal errors.\n1","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":13,"lines":{"from":23,"to":35}}}}],["daef2bc2-9d33-42b9-b726-05f9f9019f4a",{"pageContent":"2Chapter 1.  Introduction\nFigure 1.1: Hauler\nFigure 1.2: Excavator","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":14,"lines":{"from":1,"to":3}}}}],["ab085c25-6a7d-4059-81ea-ece3001c7573",{"pageContent":"1.1.  Aim & Objectives3\nFigure 1.3: Wheeled Loader\n1.1  Aim & Objectives\nThe aim of this thesis is to evaluate the classification performance of the suitable deep\nlearning models for real-time object recognition and tracking of construction vehicles.\nThe following objectives have been identified to fulfil the aim of this thesis work:\n•  To  identify  suitable  and highly  efficient  deep learning  models  for  real-time\nobject recognition and tracking of construction vehicles.\n•  Evaluate the classification performance of the selected deep learning models.\n•  Compare the classification performance of the selected models among each\nother and present the results.\n1.1.1  Problem Statement\nThough the learning approach for construction vehicles is the same as for traditional\ntransportation vehicles such as cars, when it comes to autonomous driving, there\nexist unique challenges for the construction vehicles as their surroundings (construc-","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":15,"lines":{"from":1,"to":15}}}}],["0c3c8e39-2af3-4fcd-a7e7-0d6c0d68381f",{"pageContent":"transportation vehicles such as cars, when it comes to autonomous driving, there\nexist unique challenges for the construction vehicles as their surroundings (construc-\ntion and excavation sites) and driving conditions are different compared to cars or\nother transportation vehicles. Additionally, the characteristics and purpose of a con-\nstruction vehicle is different from traditional transportation vehicle. Therefore, there\nneeds to be research carried-out to evaluate the existing state-of-the-art deep learn-\ning models and identify the best deep learning model for the detection and tracking\nof objects in the construction/excavation environments, as only little research has\nbeen carried out in this area of study, to date.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":15,"lines":{"from":14,"to":22}}}}],["a21684be-28db-4efd-b120-479daa8dda36",{"pageContent":"4Chapter 1.  Introduction\nFigure 1.4: PDRL site\nThere are several deep learning models that are currently in practice. OverFeat,\nVGG16, Faster R-CNN, YOLO are some of the popular deep learning models. These\nmodels differ from each other majorly in their architecture and performance due to\nthe variables such as Layer depth and Prediction time. For example, OverFeat model\nis 8 layers deep while VGG16 model is 16-19 layers deep. Fast R-CNN model is known\nfor its hybrid ability to capture the accuracy of deep layer models as well as improving\ntheir speed at the same time.  YOLO, which is a 12-layer model, is known for its\namazing prediction speed as it can predict up to 45 frames per second. In this way,\nvarious deep learning models differ from each other and hence, these models need to\nbe evaluated as it is important to pick the right deep learning model for the desired\npurpose.\n1.1.2  Document Structure\nThis thesis report discusses about the state-of-the-art neural networks suitable for","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":16,"lines":{"from":1,"to":15}}}}],["9cb5af22-9faf-4fa6-a91e-e6eaefd6331c",{"pageContent":"purpose.\n1.1.2  Document Structure\nThis thesis report discusses about the state-of-the-art neural networks suitable for\nreal-time object recognition and evaluates their performance against the dataset of\nconstruction vehicles at the scale site. Further in this report, Chapter 2 presents the\nbackground and related work about the neural networks and related work performed\npreviously by other authors in the field of object detection using neural networks.\nChapter 3 discusses about the research questions formulated for this thesis and re-\nsearch methodologies selected to answer them.  Details about the literature review","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":16,"lines":{"from":13,"to":21}}}}],["5d4bffcc-4e36-4c7e-908c-c9f0a27dabc5",{"pageContent":"1.1.  Aim & Objectives5\nFigure 1.5: Exemplar Excavation Site [1]\nFigure 1.6: Exemplar Construction Site [2]","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":17,"lines":{"from":1,"to":3}}}}],["6ab3bffc-e585-4395-b423-0693003f7c68",{"pageContent":"6Chapter 1.  Introduction\nand experiment are also discussed.  The findings of literature review and results of\nthe experiment are presented in Chapter 4. Chapter 5 discusses and presents analy-\nsis regarding the results of literature review and experiment and finally conclusions\ndrawn from the analysis of results and future work in this area of study are discussed\nin the Chapter 6 of the document.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":18,"lines":{"from":1,"to":6}}}}],["6bea52c3-37a9-4109-a957-eb8c21487689",{"pageContent":"Chapter 2\nBackground & Related Work\nIn this chapter, theoretical knowledge required for understanding the methods dis-\ncussed in the chapter 3 has been provided.  Details about machine learning, neural\nnetworks, and computer vision have been discussed, followed by the explanation of\nthe Faster R-CNN, YOLOv3 and Tiny-YOLOv3. Related work performed by other\nresearchers related to this area of study has also been presented towards the end of\nthis chapter.\n2.1  Machine Learning\nMachine learning is one of the applications of Artificial Intelligence (AI) which en-\nables the computers to learn on their own and perform tasks without human inter-\nvention [15]. There are numerous applications of machine learning algorithms in the\nfield of computer vision. With the help of machine learning, formulation of some of\nthe most complex problems have been performed easily. Various computer programs\nwhich were previously programmed by humans, sometimes by-hand, are now be-","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":19,"lines":{"from":1,"to":15}}}}],["1264ccb8-d640-4206-98d0-7d45065ceadd",{"pageContent":"the most complex problems have been performed easily. Various computer programs\nwhich were previously programmed by humans, sometimes by-hand, are now be-\ning programmed without any human contribution with the help of machine learning\n[16].  In the recent years, due to remarkable increase in the availability of humon-\ngous sources of data and feasibility of computational resources, machine learning has\nbecome predominant with wide range of applications in our daily lives.\n2.1.1  Types\n•  Supervised Learning\n•  Unsupervised Learning\n•  Reinforcement Learning\nSupervised Learning\nSupervised learning is considered to be the most elementary class of machine learning\nalgorithms.  As the name suggests, these algorithms require direct supervision [17].\nIn this type of learning, the data labelled/annotated by humans is spoon-fed to the\nalgorithm.  This data contains the classes and locations of the objects of interest.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":19,"lines":{"from":14,"to":28}}}}],["5fea42a0-7639-4222-9962-06cbb588d486",{"pageContent":"In this type of learning, the data labelled/annotated by humans is spoon-fed to the\nalgorithm.  This data contains the classes and locations of the objects of interest.\nEventually, the algorithm learns from the annotated data and predicts the annota-\ntions of the new data previously not known to the algorithm, after the completion of\n7","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":19,"lines":{"from":27,"to":31}}}}],["c9e88bf5-a966-4b94-8a3f-61399a7c5a1e",{"pageContent":"8Chapter 2.  Background & Related Work\ntraining process [18]. Some of the popularly utilized supervised learning algorithms\nare:\n•  Neural Networks\n•  Decision Trees\n•  Random Forest\n•  K-Nearest Neighbors\n•  Linear Regression\n•  Logistic Regression\n•  Support Vector Machines\nUnsupervised Learning\nIn the unsupervised learning, the algorithm tries to learn and identify useful proper-\nties of the classes from the given annotated data, without the help or intervention of\na human [18]. Apriori algorithm, K-means clustering, etc. are some of the common\nunsupervised learning algorithms.\nReinforcement Learning\nIn this type of learning, the machine is allowed to train itself continually using trial\nand error.  As a result, the machine learns from past experience and attempts to\ncapture the best knowledge possible to predict accurately [18].   Markov Decision\nProcess, Q-learning, Temporal difference, etc. are some of the examples of reinforce-\nment learning [17].\n2.2  Artificial Neural Networks","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":20,"lines":{"from":1,"to":22}}}}],["9873260a-65fe-432f-b828-2cb4299ac11d",{"pageContent":"Process, Q-learning, Temporal difference, etc. are some of the examples of reinforce-\nment learning [17].\n2.2  Artificial Neural Networks\nArtificial neural networks are a popular type of supervised learning model.  A spe-\ncial case of a neural network called the convolutional neural network (CNN) is the\nprimary focus of this thesis.  The name ‘Artificial Neural Networks’ was given to\nthis model because they were developed to imitate the neural function of the human\nbrain.  An artificial neural network consists of a set of neurons connected to each\nother and are grouped into layers to replicate the neural function of our brain [19].\nSimilar to the neurons in a human brain, the neurons in an artificial neural net-\nwork function as units of calculation (see Figure 2.1).   The connections between\nneurons are known as ‘synapses’ which are nothing but weighted values [20]. There-\nfore, in a simple sense, when an input value is provided at a neuron(x\n1\n,x\n2\n, ...,\nx\nn","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":20,"lines":{"from":20,"to":38}}}}],["d4366774-73f0-440c-a2a9-ad0d6408334a",{"pageContent":"neurons are known as ‘synapses’ which are nothing but weighted values [20]. There-\nfore, in a simple sense, when an input value is provided at a neuron(x\n1\n,x\n2\n, ...,\nx\nn\n), it traverses the synapse, multiplying its value with the weighted value of the\nsynapse(w\n1\n,w\n2\n, ...,w\nn\n)as shown in the Figure 2.2.  Bias ‘b’ is then added to the\nsummation of these values.  This will be the output of the neuron.  Since a neuron\ndoes not know its boundary, a mapping mechanism is required to map the inputs to","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":20,"lines":{"from":31,"to":48}}}}],["fee6399b-be55-4b6b-b556-bf7c87051cf4",{"pageContent":"2.2.  Artificial Neural Networks9\nFigure 2.1: A Simple Artificial Neural Network [3]\nthe output, known as the ‘Activation function’ [21]. In a fully connected feed-forward\nmulti-layer network, all the outputs of a layer of neurons is fed as an input to every\nneuron of the next layer. As a result, some of layers get to process the original input\ndata, while some layers get to process the data that has been obtained from neurons\nfrom the previous layer (see Figure 2.3).  Therefore, the number of weights of any\nneuron in the network is equal to the number of neurons in the layer previous to the\nlayer of the neuron in question [19].\ny=\nn\n∑\ni=1\n(w\nn\n∗x\nn\n)+b(2.1)\nIn the above equation,  ‘x’ is the input value given at the neuron,  ‘w’isthe\nweighted value of the synapse, ‘n’ is the number of neurons, ‘b’ is the bias and ‘y’is\nthe output of the network.  Therefore, according to the equation (2.1), the value of\noutput ‘y’ is equal to the summation of the product of the values of ‘x’ with their","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":21,"lines":{"from":1,"to":22}}}}],["3afe51fc-a493-41d3-9fc9-edf52e0e8760",{"pageContent":"the output of the network.  Therefore, according to the equation (2.1), the value of\noutput ‘y’ is equal to the summation of the product of the values of ‘x’ with their\ncorresponding weights and bias ‘b’.\nA multi-layered artificial neural network, as shown in the Figure 2.3, typically\nincludes three types of layers: an input layer, one or more hidden layers and an output\nlayer [19].  The input layer usually merely passes data along without modifying it.\nMost of the computation happens in the hidden layers.  The output layer converts\nthe hidden layer activation to an output, such as a classification. The outputs of each\nhidden layer serve as the inputs for the next hidden layer.  The number of neurons\nin the output layer is equal to the number of classes trained for the neural network.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":21,"lines":{"from":21,"to":30}}}}],["1486bbca-f0d6-48a2-a004-3d57123032a4",{"pageContent":"10Chapter 2.  Background & Related Work\nFigure 2.2: Structure of a single perceptron or neuron [4]\nFigure 2.3: Multi-layer Artificial Neural Network [5]","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":22,"lines":{"from":1,"to":3}}}}],["4351a511-2968-45b0-a117-229a18bc0e7f",{"pageContent":"2.3.  Computer Vision11\n2.2.1  Backpropagation\nThough the artificial neural networks have shown predominant applications in var-\nious fields and aided in achieving groundbreaking innovations during recent times,\nthe concept of neural networks is quite old.  The neural networks were previously\nknown as ‘perceptrons’ and have been in action since the 1940s [22]. They were not\npopular as they are now due to the fact that they were single layered and required\nhigh computational power and data which was difficult to find during that time.\nThey have come to limelight mainly due to the inception of a technique known as\n‘Backpropagation’. The technique was first put forth by Rumelhart et al. in the year\n1986 [23]. Using this technique, networks can rearrange the weights of hidden layers\nin case the output is different from the expected output. The error is calculated and\nbackpropagated to all the layers of the network to adjust the weights according to\nthe requirement [22].","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":23,"lines":{"from":1,"to":14}}}}],["43ec84c9-4ad8-44f4-9478-97331b5bd4a2",{"pageContent":"in case the output is different from the expected output. The error is calculated and\nbackpropagated to all the layers of the network to adjust the weights according to\nthe requirement [22].\n2.3  Computer Vision\nComputer vision is the area of study in which computers are empowered to visualize,\nrecognize and process what they see in a similar way as that of humans [24].  The\nmain aim of computer vision is to generate relevant information from image and\nvideo data in order to deduce something about the world [25][26]. It can be classified\nas a sub-field of artificial intelligence and machine learning.  This is quite different\nfrom image processing, which involves manipulating or enhancing visual information\nand is not concerned about the contents of the image. Applications of computer vi-\nsion include image classification, visual detection, 3D scene reconstruction from 2D\nimages, image retrieval, augmented reality, machine vision and traffic automation\n[27][28][29][30][31][32]","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":23,"lines":{"from":12,"to":25}}}}],["b9f523c6-ef4c-462a-bfc9-385ae90a63b4",{"pageContent":"sion include image classification, visual detection, 3D scene reconstruction from 2D\nimages, image retrieval, augmented reality, machine vision and traffic automation\n[27][28][29][30][31][32]\nToday, machine learning is a necessary component of many computer vision al-\ngorithms [33].  These algorithms are typically a combination of image processing\nand machine learning techniques.  The major requirement of these algorithms is to\nhandle large amounts of image/video data and to be able to perform computation\nin real-time for wide range of applications.  For example, real-time detection and\ntracking.\n2.4  Convolutional Neural Networks\nThere are various types of artificial neural networks that are considered to be very im-\nportant such as Radial basis function neural network, Feed-forward neural network,\nConvolutional neural network, Recurrent neural network, Modular neural network\netc.   Among these types of networks,  the convolutional neural networks (CNNs)","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":23,"lines":{"from":23,"to":36}}}}],["ba638558-aab8-41fe-b6b4-a71b0dddd056",{"pageContent":"Convolutional neural network, Recurrent neural network, Modular neural network\netc.   Among these types of networks,  the convolutional neural networks (CNNs)\nare effective in applications such as image/video recognition [34], semantic parsing,\nnatural language processing and paraphrase detection [35].  A convolutional neural\nnetwork typically comprises of three layers – Convolutional layer, Pooling layer and\nFully-connected layer.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":23,"lines":{"from":35,"to":40}}}}],["e1ccbdb8-247d-45ac-b4b2-753983e63d5c",{"pageContent":"12Chapter 2.  Background & Related Work\nFigure 2.4: Feature Filters of Front, Middle and Rear-End Layers in a CNN [6]\n2.4.1  Convolutional Layer\nA convolutional neural network consists of one or more convolutional layers.  These\nlayers can either be pooled or fully connected [35].  A convolutional layer generally\nexecutes tasks that require heavy computation.  It comprises of a set of filters that\nhave the ability to learn. Though the filters are small in size, they reach to the entire\ndepth of the input. The dimensions of a filter are generally represented byl∗w∗d,\nwhere ‘l’ denotes the height of the length of the filter, ‘w’ denotes the width while ‘d’\ndenotes the depth of the feature filter which is equal to the number of color channels\npresent.\nIn general, the convolution process is executed by a feature filter upon sliding on\nthe input layer of the neural network, as a result of which a feature map is generated.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":24,"lines":{"from":1,"to":13}}}}],["cefa2bd1-62d9-4094-8de8-59fadb1d5c39",{"pageContent":"present.\nIn general, the convolution process is executed by a feature filter upon sliding on\nthe input layer of the neural network, as a result of which a feature map is generated.\nThe layer executing the convolution process is known as a convolutional layer. Hence,\nthe networks that consist of convolutional layers are called as convolutional neural\nnetworks. As shown in the Figure 2.4, in the initial stages, the input layer is searched\nfor any specific pattern by the filter. During the training of the algorithm, the filter\nsearches for the sake of learning to recognize a pattern which eventually becomes a\nsearch to validate the existence of a specific pattern, during the testing stages.  In\nreality, many feature filters exist, learning to recognize various patterns.\n2.4.2  Pooling Layer\nPooling layers are also an important component of a convolutional neural network.\nThe main function of a pooling layer is to decrease the number of parameters and","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":24,"lines":{"from":11,"to":23}}}}],["1d0022bc-7e11-48b9-a4b4-b85910749e1e",{"pageContent":"2.4.2  Pooling Layer\nPooling layers are also an important component of a convolutional neural network.\nThe main function of a pooling layer is to decrease the number of parameters and\ncomputation present in the network by decreasing the spatial size gradually and con-\ntinuously. This action is necessary to cut down the features that the filter has learnt\nand no longer requires the whereabouts of their location.  There are many benefits\nusing a pooling layer such as limiting of over-fitting, which is a state that occurs\nwhen the algorithm fits the data very closely by showing low bias and high variance.\nThough there are various types of pooling, max pooling is one of the most popular","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":24,"lines":{"from":21,"to":29}}}}],["222e0067-0e5d-43c8-9c74-b73c3e6874b8",{"pageContent":"2.5.  YOLOv313\nFigure 2.5: Pooling Layer [7]\nFigure 2.6: Example of 2x2 Max-pooling [7]\nones in practice.  This type of pooling conveniently down-samples the layer while\nkeeping the depth constant. Figure 2.5 shows the depiction of a pooling layer while\nFigure 2.6 provides an example of a 2x2 map pooling.\n2.5  YOLOv3\nThe state-of-the-art object detector YOLOv3 is designed to achieve high accuracy\nalong with real-time performance.  YOLOv3 is an improvement over the previous\nversion of YOLO. It uses a single neural network, which predicts the objects position\nand class score in a single iteration. This is achieved by considering object detection\nproblem as a regression problem, which in turn changes the input images to their\ncorresponding class probabilities and positions. YOLO generates ManySxSgrids\nfrom the input image and boundary boxes B are predicted, which consists of height,\nwidth, box centerxandy. Each of these boxes have their ownP(object probability)","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":25,"lines":{"from":1,"to":15}}}}],["15298386-120e-4be6-a80f-f1b399a4e960",{"pageContent":"from the input image and boundary boxes B are predicted, which consists of height,\nwidth, box centerxandy. Each of these boxes have their ownP(object probability)\nvalue and predicts the number of classes in it asCand has a conditional class\nprobabilityP\nc\nlassin theSxShaving an object in it. The overall prediction of the\nnetwork isSxSx(Bx5+C)in which the digit 5 represents each box coordinates\nas 4 and 1 as object probability.\nDuring the test, the network computes the number of classes present in each\ngrid by using the equation (2.2).P\nm\ninis defined at the start of the test and system\ndetects only the objects whoseP\nc\nlass > P\nm\nin. During the post-processing stage, the","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":25,"lines":{"from":14,"to":30}}}}],["5109e251-416a-48f8-a778-589ed016ef93",{"pageContent":"14Chapter 2.  Background & Related Work\nFigure 2.7: Image Splitting and Bounding-boxes Prediction [8]\nduplicated detection of the same object is omitted using Non-maximal suppression.\nP(class\ni\n)=P(class\ni\n|object)∗P(object)(2.2)\nHere,P(class\ni\n)is the probability ofi\nth\nclass.P(Object)is the probability of grid\ncontaining the object andP(class\ni\n|object)is the conditional class probability of the\ni\nth\nclass in which the object is present.\nIn YOLO, only the bounding boxes with the greatest value of confidence are\nselected since every grid-cell is predicting multiple bounding boxes. Therefore, YOLO\ngenerates a tensor as an output whose value is equal toSxSx(B∗5+C)[8].\nIn YOLOv3, the bounding boxes have been replaced by ‘Anchors’ which resolve the\nunstable gradient issue that used to occur while training of the algorithm. Therefore,\nYOLOv3 predicts outputs with confidence scores by generating a vector of bounding","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":26,"lines":{"from":1,"to":25}}}}],["13499d78-3cd1-45cc-8f11-ff6b9b56241e",{"pageContent":"unstable gradient issue that used to occur while training of the algorithm. Therefore,\nYOLOv3 predicts outputs with confidence scores by generating a vector of bounding\nboxes whenever an input is given to the algorithm in the form of an image or a video.\n2.5.1  Architecture\nYOLOv2 used a feature extractor known as the Darknet-19, which consisted of 19\nconvolutional layers.   The newer version of this algorithm,  YOLOv3 uses a new\nfeature extractor known as Darknet-53 which, as the name suggests, uses 53 convo-\nlutional layers while the overall algorithm consists of 75 convolutional layers and 31\nother layers making it a total of 106 layers [36].  Pooling layers have been removed\nfrom the architecture and replaced by another convolutional layer with stride ‘2’, for\nthe purpose of down-sampling. This key change has been made to prevent the loss of\nfeatures during the process of pooling. Figure 2.8 which is created by ‘CyberailAB’","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":26,"lines":{"from":24,"to":35}}}}],["632a080a-9bca-438e-8650-f6e2e28105e1",{"pageContent":"2.5.  YOLOv315\nFigure 2.8: The Architecture of YOLOv3 [9]\nclearly depicts the architecture of YOLOv3 algorithm.\nYOLOv3 performs detections at three different scales, as shown in the Figure\n2.8. 1 x 1 detection kernels are applied on the feature maps with three unique sizes\nlocated at three unique places in the network.  The shape of the detection kernel is\n1x1x(B∗(4+1+C)), where ‘B’ is the number of bounding boxes that can be\npredicted by a cell located on the feature map, ‘4’ represents the number of bounding\nbox attributes, ‘1’ represents the object confidence and ‘C’ represents the number\nof classes. Figure 2.7 depicts the splitting of an image and bounding-box prediction\nin YOLOv3 and Figure 2.8 depicts the architecture of YOLOv3 algorithm trained\non COCO dataset which has 80 classes and bounding boxes are considered to be 3.\nTherefore, the kernel size would be 1 x 1 x 255 [37]. In YOLOv3, the dimensions of","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":27,"lines":{"from":1,"to":13}}}}],["1b4d6c16-4fa7-4dcd-b15c-ebb0d4153064",{"pageContent":"on COCO dataset which has 80 classes and bounding boxes are considered to be 3.\nTherefore, the kernel size would be 1 x 1 x 255 [37]. In YOLOv3, the dimensions of\nthe input image are down sampled by 32, 16 and 8 to make predictions at scales 3,2\nand 1 respectively.\nIn the Figure 2.8, the size of the input image is 416 x 416.  As mentioned in\nthe earlier section, the total number of layers in YOLOv3 is 106.  As shown in the\nnetwork architecture diagram Figure 2.8, the input image is down sampled by the\nnetwork for the first 81 layers. Since the81\nst\nlayer has a stride of 32, the82\nnd\nlayer","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":27,"lines":{"from":12,"to":23}}}}],["595251be-16e6-49fd-84c5-c7ad42fc5157",{"pageContent":"16Chapter 2.  Background & Related Work\nperforms the first detection with a feature map of size 13 x 13. Since a 1 x 1 kernel\nis used to perform the detection, the size of the resulting detection feature map is 13\nx 13 x 255 which is responsible for the detection of objects at scale 3.\nFollowing this, the feature map from79\nth\nlayer is up sampled by 2x after sub-\njecting it to a few convolutional layers, resulting in the dimensions 26 x 26. This is\nthen concatenated with the feature map from 61st layer.  The features are fused by\nsubjecting the concatenated feature map to a few more 1 x 1 convolutional layers.\nAs a result, the94\nth\nlayer performs the second detection with a feature map of 26 x\n26 x 255, which is responsible for the detection of objects at scale 2.\nFollowing the second detection, the feature map from91\nst\nlayer is up sampled by\n2x after subjecting it to a few convolutional layers, resulting in the dimensions 52 x\n52. This is then concatenated with the feature map from36","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":28,"lines":{"from":1,"to":19}}}}],["31bc76eb-3c73-4341-9ef6-9fe6b0d89fe7",{"pageContent":"st\nlayer is up sampled by\n2x after subjecting it to a few convolutional layers, resulting in the dimensions 52 x\n52. This is then concatenated with the feature map from36\nth\nlayer. The features are\nfused by subjecting the concatenated feature map to a few more 1 x 1 convolutional\nlayers.  As a result, the106\nth\nlayer performs the third and final detection with a\nfeature map of 52 x 52 x 255, which is responsible for the detection of objects at\nscale 1. As a result, YOLOv3 is better at detecting smaller objects when compared\nto its predecessors YOLOv2 and YOLO.\n2.6  Faster R-CNN\nFaster R-CNN [38] by Ren et al.  is an integrated method.  The main idea is to use\nshared convolutional layers for region proposal generation and for detection.  The\nauthors discovered that feature maps generated by object detection networks can\nalso be used to generate the region proposals.  The fully convolutional part of the\nFaster R-CNN network that generates the feature proposals is called a Region Pro-","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":28,"lines":{"from":16,"to":34}}}}],["9ed1ba0f-cbc7-40e9-98f2-e90866ef7e8d",{"pageContent":"also be used to generate the region proposals.  The fully convolutional part of the\nFaster R-CNN network that generates the feature proposals is called a Region Pro-\nposal Network (RPN). The authors used Fast R-CNN architecture for the detection\nnetwork.\nA Faster R-CNN network is trained by alternating between training for Region\nof Interest (RoI) generation and detection. First, two separate networks are trained.\nThen, these networks are combined and fine-tuned. During fine-tuning, certain lay-\ners are kept fixed and certain layers are trained in turn.  Figure 2.9 represents a\nsimple depiction of the structure of Faster R-CNN model.  It typically comprises of\nthree neural networks namely a Feature Network, a Regional Proposal Network and\na Detection Network [38].\nThe Feature Network is responsible for generating good features from the input\nimages while maintaining the original attributes of the input image in the output,","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":28,"lines":{"from":33,"to":45}}}}],["d546f574-33b9-42bc-b526-6d7f1d7c1059",{"pageContent":"a Detection Network [38].\nThe Feature Network is responsible for generating good features from the input\nimages while maintaining the original attributes of the input image in the output,\nsuch as shape and structure. An image classification network generally takes the role\nas a Feature Network [39].\nThe Regional Proposal Network (RPN) comprises of three convolutional layers,\na layer each for classification and bounding box regression while the third one is\na common layer that feeds into these two layers.  The Regional Proposal Layer is","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":28,"lines":{"from":43,"to":50}}}}],["4f636727-e681-4b98-81bf-992b13cd7084",{"pageContent":"2.6.  Faster R-CNN17\nresponsible for generating numerous bounding boxes that have a high probability\nof including an object.  These bounding boxes are also called as Region of Interests\n(ROIs) [38]. A bounding box is identified using the co-ordinates of the pixels located\nat two diagonal corners of the box, followed by a value of 1, 0 or -1.  A value of 1\nindicates that there is an object present in that particular bounding box. Similarly,\na value of 0 indicates that there is no object present while a value of -1 indicates that\nthe particular bounding box can be ignored [39].\nThe Detection Network is responsible for generating the final class and its cor-\nresponding bounding box by taking the input from both the Feature Network and\nRegional Proposal Network [38].  The Detection Network generally comprises of a\nclassification layer and a bounding box regression layer.   Additionally,  a pair of\nstacked common layers are shared among the two layers. These four layers are fully","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":29,"lines":{"from":1,"to":13}}}}],["23281b86-4962-42bb-9c1f-e60f078e25f3",{"pageContent":"classification layer and a bounding box regression layer.   Additionally,  a pair of\nstacked common layers are shared among the two layers. These four layers are fully\nconnected layers.  The features are cropped as per the bounding boxes so that the\nnetwork classifies only the internal part of the bounding boxes [39].\n2.6.1  Architecture\nUsing shared convolutional layers, region proposals are computationally almost cost-\nfree.  Computing the region proposals on a CNN has the added benefit of being\nrealizable on a GPU. Traditional RoI generation methods, such as Selective Search,\nare implemented using a CPU. For dealing with different shapes and sizes of the\ndetection window, the method uses special anchor boxes instead of using a pyramid\nof scaled images or a pyramid of different filter sizes. The anchor boxes function as\nreference points to different region proposals centered on the same pixel.\nIn the architecture diagram of Faster R-CNN shown in Figure 2.10, the trained","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":29,"lines":{"from":12,"to":24}}}}],["03be3fbf-5a70-41ce-9129-bab2f8bfed8d",{"pageContent":"reference points to different region proposals centered on the same pixel.\nIn the architecture diagram of Faster R-CNN shown in Figure 2.10, the trained\nnetwork receives a single image as input. In this case, the Feature Network is VGG,\nwhich is an image classification network. The shared fully convolutional layers of this\nnetwork generate good feature maps from the input image while maintaining the size\nand structure of the original image in the output of this network.  The resulting\nfeature maps are fed into the Regional Proposal Network (RPN). Here, a number of\nbounding boxes are generated by a mechanism called as anchor boxes. Anchors are\nnothing but the pixels present on the feature image. In general, 9 boxes of different\nshapes and sizes, with the anchor as their center, are generated for each anchor. This\nlayer feeds into the classification (detection) layer and bounding box regression layer.\nNon-Maximum Suppression (NMS), which is an operation to reduce the number of","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":29,"lines":{"from":23,"to":34}}}}],["ac50a87f-55c7-472a-8c20-75d426393265",{"pageContent":"layer feeds into the classification (detection) layer and bounding box regression layer.\nNon-Maximum Suppression (NMS), which is an operation to reduce the number of\nboxes by removing the boxes that are overlapping with other boxes that have a high\nscore based on the probability of containing an object, is applied. These probability\nscores are later normalized using SoftMax function.  The resulting bounding boxes\n(ROIs) are fed into the Detection Network along with the output of the Feature\nNetwork. Since the resulting feature maps can be of various sizes, ROI pooling layer\nis introduced to crop and scale the features to 14 x 14.  These features are later\nmax-pooled to7x7andfedintotheDetection Network in the form of batches. The\npair of stacked common fully connected layers, along with the classification layer and\nbounding box regression layer can be seen in the Figure 2.10.  The output of this\nnetwork is the generation of final class and its corresponding bounding box.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":29,"lines":{"from":33,"to":44}}}}],["22d4c96c-1465-48d7-a067-8ff494bbff50",{"pageContent":"18Chapter 2.  Background & Related Work\nFigure 2.9: Structure of Faster R-CNN model [10]","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":30,"lines":{"from":1,"to":2}}}}],["f597cd6e-a426-4042-b465-30ad71602f77",{"pageContent":"2.7.  Tiny-YOLOv319\nFigure 2.10: The Architecture of Faster R-CNN [11]\n2.7  Tiny-YOLOv3\nTiny-YOLOv3 is the smaller and simplified version of YOLOv3.  Even though the\nnumber of layers in Tiny-YOLOv3 is quite less when compared to that of YOLOv3,\nthe accuracy of the model is almost the same as that of its bigger self when high\nframe rates are considered.  Tiny-YOLOv3 consists of only 13 convolutional layers\nand 8 max-pool layers and therefore, requires minimal memory to run which is way\nless than the layers in YOLOv3. The major difference between YOLOv3 and Tiny-\nYOLOv3 is that the former is designed to detect objects at three different scales\nwhile the later can only detect objects at two different scales.  Apart from these\ndifferences, the working of both these variants is similar.  Figure 2.11 shows the ar-\nchitecture details of Tiny-YOLOv3.\nCompared to YOLOv3, the number of convolutional layers is greatly reduced in","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":31,"lines":{"from":1,"to":14}}}}],["4debe2af-480a-4c36-89d9-8800f75ed9a1",{"pageContent":"differences, the working of both these variants is similar.  Figure 2.11 shows the ar-\nchitecture details of Tiny-YOLOv3.\nCompared to YOLOv3, the number of convolutional layers is greatly reduced in\nTiny-YOLOv3.  The primary structure of Tiny-YOLOv3 only has 13 convolutional\nlayers while the overall number of layers is 23. A limited number of 1 x 1 and 3 x 3\nkernels are utilized to extract the features in Tiny-YOLOv3 [12].  Unlike YOLOv3,\nwhich uses convolutional layers of stride 2 for the purpose of down sampling, the\nTiny-YOLOv3 uses the pooling layer.  The convolutional layer structure of Tiny-\nYOLOv3 is similar to that of YOLOv3.\n2.7.1  Architecture\nTiny-YOLOv3 performs detections at two different scales, as shown in the Figure\n2.11. 1 x 1 detection kernels are applied on the feature maps with two unique sizes\nlocated at two unique places in the network.  The shape of the detection kernel is\n1x1x(B∗(4+1+C)), where ‘B’ is the number of bounding boxes that can be","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":31,"lines":{"from":12,"to":25}}}}],["78a0d566-8842-4585-9821-ba44f6ec7ee0",{"pageContent":"located at two unique places in the network.  The shape of the detection kernel is\n1x1x(B∗(4+1+C)), where ‘B’ is the number of bounding boxes that can be\npredicted by a cell located on the feature map, ‘4’ represents the number of bounding\nbox attributes, ‘1’ represents the object confidence and ‘C’ represents the number\nof classes.  Figure 2.11 depicts the architecture of Tiny-YOLOv3 algorithm trained\non COCO dataset which has 80 classes and bounding boxes are considered to be 3.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":31,"lines":{"from":24,"to":29}}}}],["67908ee7-82fc-4d7d-aab6-00d3fbb9747f",{"pageContent":"20Chapter 2.  Background & Related Work\nTherefore, the kernel size would be 1 x 1 x 255 [37].\nIn the Figure 2.11, the size of the input image is 416 x 416.  As mentioned in\nthe earlier section, the total number of layers in Tiny-YOLOv3 is 23.  As shown in\nthe network architecture diagram Figure 2.11, the input image is max-pooled by the\nnetwork for the first 15 layers.  The15\nth\nlayer performs the first detection with a\nfeature map of size 13 x 13.  Since a 1 x 1 kernel is used to perform the detection,\nthe size of the resulting detection feature map is 13 x 13 x 255 which is responsible\nfor the detection of objects at scale 2.\nFollowing this, the feature map from14\nth\nlayer is up sampled by 2x after sub-\njecting it to a convolutional layer, resulting in the dimensions 26 x 26. This is then\nconcatenated with the feature map from9\nth\nlayer.  The features are fused by sub-\njecting the concatenated feature map toa1x1anda3x3convolutional layer. As\na result, the23\nrd","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":32,"lines":{"from":1,"to":21}}}}],["92fb6005-5e7d-447d-b7b9-3da9f3bf4117",{"pageContent":"concatenated with the feature map from9\nth\nlayer.  The features are fused by sub-\njecting the concatenated feature map toa1x1anda3x3convolutional layer. As\na result, the23\nrd\nlayer performs the second and final detection with a feature map\nof 26 x 26 x 255, which is responsible for the detection of objects at scale 1.\n2.8  Related Work\nYukui Luo et al [40], presented an OpenCL based implementation of the Deep Con-\nvolutional Neural Network, which is one of the most advanced deep learning frame-\nworks.   Their framework aimed at three major contributions- a real-time object\nrecognition system, framework with low power consumption, that can be applied\neven in portable devices, framework that can work on various compute devices [40].\nThe framework was evaluated by comparing its speed with the CUDA framework,\nbased on YOLO V2 benchmark.\nAlpaydin [41] proposed an adaptive fuzzy based network topology which is run","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":32,"lines":{"from":16,"to":32}}}}],["554769ec-a119-414a-8d03-9e9822412795",{"pageContent":"The framework was evaluated by comparing its speed with the CUDA framework,\nbased on YOLO V2 benchmark.\nAlpaydin [41] proposed an adaptive fuzzy based network topology which is run\nalongside Deep Convolutional Neural Networks,  to achieve highly efficient object\nrecognition for long range images that are low in contrast and having variable, noisy\nbackgrounds.\nDaniel et al [42], presented a 3D Convolutional Neural Network (CNN) architec-\nture ‘VoxNet’, to achieve accurate and efficient object detection, using LiDAR data\nand RGBD point clouds.  They evaluated their approach on state-of-the-art bench-\nmarks that are publicly available and found that their approach achieved accuracy\nbeyond these benchmarks while classifying the objects in real-time [42].\nLewis [14], in his paper, proposed a DIY network called as SimpleNet, that per-\nforms deep object recognition without pre-processing or deep evaluations that are","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":32,"lines":{"from":30,"to":42}}}}],["514cd5e5-9dfb-40a6-aa9b-8f89afcd01d3",{"pageContent":"Lewis [14], in his paper, proposed a DIY network called as SimpleNet, that per-\nforms deep object recognition without pre-processing or deep evaluations that are\notherwise very costly. Though the accuracy is quite less compared to the state-of-art,\nSimpleNet looks to draw power from appropriate loss functions with finite number of\nparameters while other networks draw power from the depth of the layers. The au-\nthor compared various CNN models such as OverFeat, VGG16, Fast R-CNN, YOLO\nwith SimpleNet to give the audience a profound insight into all these CNN models\nin terms of performance [14].","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":32,"lines":{"from":41,"to":48}}}}],["c0b83296-133b-4a30-af97-d162e7236b40",{"pageContent":"2.8.  Related Work21\nFigure 2.11: Architecture of Tiny-YOLOv3 [12]","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":33,"lines":{"from":1,"to":2}}}}],["dd78a995-9d9c-4fd0-bce5-b8726550b10a",{"pageContent":"22Chapter 2.  Background & Related Work\nGirshick et al [43] presented a Region based Convolutional Neural Network called\nas ‘Fast R-CNN. This network is capable of detecting objects at high accuracy while\ntrading-off with poor computational speed. Therefore, the network is considered to\nbe not suitable for real-time object detection and recognition though it exhibits a\ngood performance in terms of accuracy.\nRen et al [38], in their paper, presented an updated version of ‘Fast R-CNN’\nknown as ‘Faster R-CNN’. As the name suggests, the updated version of the Re-\ngion based Convolutional Neural Network, which showcased better computational\nspeed and accuracy when compared to its previous version and many of the other\nstate-of-the-art networks. A Region Proposal Network (RPN) has been added which\nenhances the computation speed of the network by generating features and sharing\nthem with the Detection Network which is responsible for performing the final de-","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":34,"lines":{"from":1,"to":13}}}}],["3644c099-3f62-452b-b9b7-9c5d51fd2bb5",{"pageContent":"enhances the computation speed of the network by generating features and sharing\nthem with the Detection Network which is responsible for performing the final de-\ntection.  Faster R-CNN models are capable of performing real-time detections but\nstruggles at detecting objects that are smaller in size.\nThough the Faster R-CNN is faster than the Fast R-CNN by an order of mag-\nnitude, the CNN feature extraction and an expensive per-region computation which\nare the first and second stages in the Faster R-CNN network, hinder the speed of the\nnetwork. Addressing this issue, Kim et al [44] made changes in the feature extraction\nstage by utilizing the cutting-edge technical innovations and presented a newer net-\nwork known as PVANET. This network is capable of detecting objects from multiple\ncategories with accuracy that is on par with its counterparts, while reducing the\ncomputational cost.\nDai et al [45], constructed a fully convolutional network called as R-FCN, while","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":34,"lines":{"from":12,"to":24}}}}],["b9ee2654-eecf-43f9-abdc-9d4684f04a40",{"pageContent":"categories with accuracy that is on par with its counterparts, while reducing the\ncomputational cost.\nDai et al [45], constructed a fully convolutional network called as R-FCN, while\nadopting the existing ResNet which are state-of-the-art when it comes to object de-\ntection. In an attempt to increase the object detection accuracy, the fully connected\nlayers in Fast R-CNN have been replaced by a set of score maps that are position-\nsensitive as well as capable of encoding spatial information. As a result, R-FCN dis-\nplayed similar accuracy as that of Faster R-CNN but at better computational speeds.\nKong et al [46], presented a network called as HyperNet, which is capable of de-\ntecting objects at multiple scales by performing detection at multiple output layers.\nThis network is similar to the MS-CNN, proposed by [34], which provides an efficient\nframework for detecting objects at multiple scales.\nLiu et al [47] presented a simple and straightforward network called as Single","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":34,"lines":{"from":22,"to":34}}}}],["11225ba7-f2f1-46f3-ae37-a68f3b26efa6",{"pageContent":"framework for detecting objects at multiple scales.\nLiu et al [47] presented a simple and straightforward network called as Single\nShot multi-box Detector (SSD) which is capable of delivering real-time performance\nat high accuracy.  This network does not utilize regional proposal method.  In this\nnetwork, the object localization and classification are performed in a single forward\npass of the network while using a technique known as ‘multi-box’ for performing\nthe bounding box regression.  The SSD is hence capable of performing end-to-end\ncomputations.\nRedmon et al [36], in their paper, presented YOLOv3 which is an updated version","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":34,"lines":{"from":33,"to":41}}}}],["ac6501e3-ae86-46c0-be50-4366fbcb8cbf",{"pageContent":"2.8.  Related Work23\nof their revolutionary network YOLO. This model surpassed all the other state-of-\nthe-art networks such as Faster R-CNN, VGG-16, ResNet, etc., in terms of computa-\ntional speed and accuracy, thus making it an ideal network for performing real-time\ndetections and tracking while maintaining high accuracy which the other networks\nhave failed to do. The YOLOv3 is also capable of detecting objects of small size as\nit can detect objects of three different scales effectively.\nFrom the above papers, one can say that CNNs are the best suited deep learn-\ning algorithm for real-time object detection and recognition.  From the knowledge\ngathered, it is evident that most of the research and development of autonomous\ndriving systems is being implemented on transportation vehicles such as cars while\nonly a little research is being carried-out to evaluate the existing state-of-the-art\ndeep learning models and identify the best deep learning model for the detection","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":35,"lines":{"from":1,"to":13}}}}],["47892fb3-47fd-4783-8911-821af66560f6",{"pageContent":"only a little research is being carried-out to evaluate the existing state-of-the-art\ndeep learning models and identify the best deep learning model for the detection\nand tracking of objects in the construction/excavation environments, as only little\nresearch has been carried out in this area of study, to date Therefore, this thesis will\nbe using CNN models to recognize small scale vehicles at real-time to evaluate the\nperformance of these algorithms and as a step towards future innovations.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":35,"lines":{"from":12,"to":17}}}}],["35549a36-887e-42ca-b5c3-709ae9cc37df",{"pageContent":"Chapter 3\nExperimental Results\nIn this chapter, we begin discussing the experimental part of the thesis. First, we will\ndiscuss selection criteria for methods and datasets. Then we will describe the selected\nmethods, their parameters and the selected datasets.  Finally, we will discuss post-\nprocessing and evaluation.  The implementation of the methods is mostly discussed\nin the following chapter.  However, some implementation details are also discussed\nin this chapter, since they influence method selection.\n3.1  Research Questions\nAs discussed in the earlier sections, the overall goal of this research is to identify\nsuitable and highly efficient deep learning models for real-time object recognition\nand tracking of construction vehicles, evaluate the classification performance of the\nselected deep learning models and finally, to compare the classification performance\nof the selected models among each other and present the results.  The following re-","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":37,"lines":{"from":1,"to":14}}}}],["cbb2b9e8-0428-470c-9cef-d65973782915",{"pageContent":"selected deep learning models and finally, to compare the classification performance\nof the selected models among each other and present the results.  The following re-\nsearch questions have been formulated to fulfill these research objectives.\n•RQ1:What are the most suitable and efficient Deep Learning models for real-\ntime object recognition and tracking of construction vehicles?\n–Since there are several deep-learning models that can perform object de-\ntection and recognition,  this research question has been formulated to\nidentify the best suited deep learning models for performing object recog-\nnition of construction vehicles in real-time, so that it would be useful in\nfuture projects in developing intelligent machine navigation systems, au-\ntonomous driving of heavy machinery as they require object recognition\nto be done in real-time with high accuracy.\n•RQ2:How is the classification performance of the Deep Learning models that\nare selected for object recognition?","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":37,"lines":{"from":13,"to":26}}}}],["cf739e7c-e6ca-438d-996c-4a7052049912",{"pageContent":"to be done in real-time with high accuracy.\n•RQ2:How is the classification performance of the Deep Learning models that\nare selected for object recognition?\n–This research question has been formulated to evaluate the classification\nperformance of the selected deep-learning models using relevant metrics\nso as to compare the results among each other.\n25","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":37,"lines":{"from":24,"to":30}}}}],["a68f35d3-959f-4449-9b60-7e1971b1e9c8",{"pageContent":"26Chapter 3.  Experimental Results\n3.2  Literature Review\nTo answer the RQ1,  literature review has been selected as the research method.\nThe literature review is selected in order to gain knowledge and deep understanding\nabout various deep learning models and their efficiency so that the most suitable and\nefficient method can be selected from the identified models.\n3.2.1  Search Process\nThe main focus of the search process was to find all the papers in which “Object\ndetection”  and “Deep Learning”  have been mentioned.   Therefore,  search strings\nsuch as “Object detection AND Recognition AND Deep Learning” have been created\nfor the search process.  The search process has been carried out on IEEEXplore,\nSpringer Link and ACM Digital Library databases.  The papers have been selected\nfollowing the inclusion and exclusion criteria discussed in the subsection 3.2.2. The\nselected research papers have been filtered by reading the title of the collected articles,","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":38,"lines":{"from":1,"to":14}}}}],["53cb0e83-3981-4b8b-8ef3-a1046dd496dd",{"pageContent":"following the inclusion and exclusion criteria discussed in the subsection 3.2.2. The\nselected research papers have been filtered by reading the title of the collected articles,\nfollowed by reading the abstract of the articles filtered from the previous stage and\nultimately,  by reading the entire text of the articles that were selected from the\nprevious stage.\n3.2.2  Inclusion and Exclusion Criteria\nThe following inclusion and exclusion criteria have been followed while collecting the\narticles for the literature review:\n•  Only those articles that discussed about object detection/recognition and deep\nlearning models have been included.\n•  Only the articles published between the years 2009 and 2019 have been in-\ncluded, as they reflect the most recent research conducted in this area.\n•  Only the journal articles, conference papers, magazines and reviews have been\nincluded [48].\n•  Only the articles written in English language have been included for understand-\nability purposes [48].","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":38,"lines":{"from":13,"to":28}}}}],["07662e27-b4ec-4aa6-bde1-5d771de977a0",{"pageContent":"included [48].\n•  Only the articles written in English language have been included for understand-\nability purposes [48].\n•  Abstracts and PowerPoint presentations have been excluded [48].\n3.3  Experiment\nAn experiment has been selected as the research method to answer the RQ2.  An\nexperiment has been chosen as the research method because when it comes to dealing\nwith quantitative data, experiment is considered to be the best method.  The main\ngoal of this experiment is to evaluate the deep learning models for object recognition\nand tracking of construction vehicles in real-time, the deep learning models being\nthe ones selected from the literature review.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":38,"lines":{"from":26,"to":36}}}}],["c39bf6f0-0694-4780-8386-262b8dccd578",{"pageContent":"3.3.  Experiment27\nSystemDell Precision 7710\nGPUNVIDIA Quadro M4000M\nCPUIntel Core i7-6820HQ\nInstalled Memory (RAM)65536 MB\nDisplay Memory (VRAM)4053 MB\nOperating System (OS)Windows 10\nTable 3.1: Hardware Environment\n3.3.1  Experimental Setup\nSoftware Environment:Python has been selected as the programming language\nas it is a high-level programming language, which is easy to learn and code, making\nit the widely used programming language for developing machine learning as well as\ndeep learning algorithms.\nCUDA and cuDNN have been installed,  as they allow training of the algorithm\non a GPU, making it way faster and efficient than training on a CPU.\nHardware Environment:Hardware specifications of the system on which the\nalgorithm has been trained and implemented are shown in Table 3.1.\nPrior to the commencement of the training process, the following steps have been\ncompleted that are essential for the training of the algorithm.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":39,"lines":{"from":1,"to":19}}}}],["14abc865-e040-4cc4-8634-65181c90f309",{"pageContent":"Prior to the commencement of the training process, the following steps have been\ncompleted that are essential for the training of the algorithm.\n•Dataset Collection:A dataset has been created by collecting the images of\nthe three types of vehicles- Hauler, Excavator and Wheeled Loader in various\nangles, brightness and contrasts. The collected images consisted of at least one\nof the three classes mentioned, alongside other objects in the PDRL lab.  A\ntotal of 1097 images have been collected, among which 250 each are the im-\nages of Hauler, Excavator and Wheeled Loader, and the remaining 347 images\ncomprise of all the three objects of interest. As the scaled construction site is a\nconstrained environment and has a limited scope, it resulted in the collection of\nlimited number of images. Since the rule of thumb for deep learning is to have\na minimum of 1000 images per class in the dataset, data augmentation meth-\nods have been applied. Among several data augmentation methods like image","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":39,"lines":{"from":18,"to":30}}}}],["884a22c4-aa08-4830-b016-a656fa1f1137",{"pageContent":"a minimum of 1000 images per class in the dataset, data augmentation meth-\nods have been applied. Among several data augmentation methods like image\npanning, zooming, flipping, rotating, etc., [12][49][50] the images in the dataset\nhave been augmented by rotating them at 90, 180- and 270-degree angles, and\nalso flipping them horizontally, thus multiplying the number of images in the\ndataset by a factor of 5, resulting in a dataset of 5,485 images.  Out of these\nimages, 1,250 each are the Hauler, Excavator and Wheeled Loader while the\nremaining 1,735 images comprise of all the three vehicles. Additionally, a test\nvideo has also been included in the test dataset, as False Negatives for each\nof the algorithms can only be yielded using a test-video as it contains certain\nframes where there are no objects of interest present while it has been ensured\nthat the images had at least one object class present in them.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":39,"lines":{"from":29,"to":40}}}}],["b6495294-b9ca-417e-b0ca-c01977ff5869",{"pageContent":"28Chapter 3.  Experimental Results\n•Data Pre-processing:For the Faster R-CNN, it has been ensured that all the\nimages are of dimensions 608 x 608 before feeding into the network. The size of\nthe input image depends mainly on the backbone convolutional neural network\nthat the image is being fed into.  It is suggested that the input image must\nbe resized in such a way that the shorter side of the image is around 600px\nwhile the other side is no greater than 1000px [51].  While for the YOLOv3\nand Tiny-YOLOv3, it has been ensured that all the images are resized into\n416 x 416 before feeding into the network.  Though the YOLO is unaffected\nby the size of the input image, it is suggested that a constant input size is\nmaintained throughout the dataset as problems might creep up later during\nthe implementation of the algorithm [52].  Additionally, since an input size of\n416 x 416 provides ideal results of accuracy and speed and is widely followed by","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":40,"lines":{"from":1,"to":13}}}}],["291c50ba-9ce4-41ff-8ca6-9819d6560f50",{"pageContent":"the implementation of the algorithm [52].  Additionally, since an input size of\n416 x 416 provides ideal results of accuracy and speed and is widely followed by\nvarious practitioners, such as [8][9], the images in the dataset have been resized\naccordingly.  This has been done using third-party software.  In all the three\ncases, the images have been fed into the network until the loss was saturated.\nAlso, it has been made sure that the batch size for all the three cases is less\nthan 1024, with a constant learning rate of 0.001, as it has been stated in the\nliterature that batch sizes higher that 1024 yield poorer performance for the\nCNNs [53].\n•Dataset Labelling:The images collected in the dataset have been labelled\nmanually using a tool known as ‘LabelImg’. Each image is labelled by drawing\nbounding boxes perfectly surrounding the desired objects in the image and\nselecting their respective classes, as shown in the Figure3.1.  As a result, an","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":40,"lines":{"from":12,"to":24}}}}],["6a3b83a9-afbb-4a44-9db2-786c8114a006",{"pageContent":"bounding boxes perfectly surrounding the desired objects in the image and\nselecting their respective classes, as shown in the Figure3.1.  As a result, an\nXML file, also known as ‘Annotation file’, is generated for each image and saved\ninto a specific folder. The annotation files contain details about the objects in\nthe image such as Image name and label name, Image path, class name of the\nobject(s), coordinates of the bounding boxes surrounding the objects present\nin the image. These files are further used to train and enable the algorithm to\ndetect the desired class objects.\n•Framework:TensorFlow’s Object Detection API is identified to be a powerful\ntool, as it enables us to build and deploy image recognition software quickly.\nHence it has been selected to train Faster R-CNN in this thesis.\n•Configuration:Various changes have been made to the default configuration\nfiles of the Faster R-CNN provided by the TensorFlow Object Detection API,","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":40,"lines":{"from":23,"to":35}}}}],["5f0ef8bc-7f41-4568-b1e2-c312d70f3263",{"pageContent":"•Configuration:Various changes have been made to the default configuration\nfiles of the Faster R-CNN provided by the TensorFlow Object Detection API,\nsuch as dataset, number of classes to be trained, batch size and label map. The\ndataset has been split into two parts – train dataset, test dataset.  The train\ndataset consisted of 80% images while test dataset consisted of 20% images\nfrom the original dataset, which is the general rule of thumb followed by various\nresearchers while splitting the dataset [54][55][56].\nNumber of classes to be trained is also changed to 3 classes, since the Faster R-CNN\nalgorithm being trained is expected to detect and recognize three classes – Wheel\nLoader, Hauler and Excavator.  A label map has been created comprising of class\nnames and their corresponding class ids.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":40,"lines":{"from":34,"to":44}}}}],["c0ef911f-53e8-4a68-919a-980efd1d8a39",{"pageContent":"3.3.  Experiment29\nFigure 3.1: Dataset Labelling\nBatch size, which represents the number of train images that are used by the algo-\nrithm in one iteration, is also changed as it affects the VRAM consumption. Higher\nthe batch size, greater is the VRAM consumption.  Therefore, a smaller batch size\nhas been selected to perform the training process.\n3.3.2  Training\nAfter finishing all the steps mentioned above and making necessary changes in the\nconfiguration file, the training process is initialized. The step count and classification\nloss in each step can be seen on screen, as shown in Figure3.2. It can be noted that\nthe classification loss starts at a really high value and gradually decreases as the\nalgorithm learns as the iterations progress.  This has been visualized in the form of\na graph, with the help of TensorFlow Board shown in Graph 6.4.\nIn Figure3.2,  the ‘Global_step’ represents the iteration or batch number that is","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":41,"lines":{"from":1,"to":14}}}}],["c7776c0b-4826-43ea-ac94-f5a2c82a3d57",{"pageContent":"a graph, with the help of TensorFlow Board shown in Graph 6.4.\nIn Figure3.2,  the ‘Global_step’ represents the iteration or batch number that is\nbeing processed. ‘Loss’ value given is the sum of Localization loss and Classification\nloss. These represent the price paid for inaccuracy of predictions. The optimization\nalgorithm keeps reducing the loss value until a point where the network is considered\nto be trained by the researcher. In general, lesser loss implies better training of the\nmodel. ‘Sec/step’ is the time taken to process that corresponding step.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":41,"lines":{"from":13,"to":19}}}}],["0da1b9bf-2d76-48a4-be40-cbe507a67edb",{"pageContent":"30Chapter 3.  Experimental Results\nFigure 3.2: Training & Classification loss\n3.3.3  Metrics\nThe following metrics are used to evaluate the classification performance of the al-\ngorithm:\nAccuracy\nIt is defined as the number of correct predictions made by the model over the total\nnumber of predictions.  This is a good measure, especially when the target variable\nclasses are balanced in the data. This can be represented as –\nNo.ofcorrectpredictions(CP)=TruePositives+TrueNegatives\nTotalno.ofpredictions(TP)=TruePositives+TrueNegatives+FalsePositives+FalseNegatives\nAccuracy=\nCP\nTP","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":42,"lines":{"from":1,"to":14}}}}],["fe0181b2-fe9e-4c9f-b2bc-b784e8ca7dac",{"pageContent":"3.4.  Results31\nWhere, a True Positive is defined as a correct detection of the object class trained.\nA True Negative is defined as a correct misdetection, meaning that nothing is being\ndetected when there is no object that must be detected. A False Positive is defined\nas a wrong detection, meaning that there is a detection even though there is no\nobject that must be detected.  A False Negative is defined as a ground truth being\nnot detected, meaning that the algorithm failed to detect an object that is required\nto be detected.\nF\n1\nScore\nThe balanced F-measure is used to measure a test’s accuracy.  The F1 score is con-\nsidered to be good if the overall number of false positives and false negatives is low.\nIt is defined as the harmonic mean of Precision and Recall.\nF\n1\nScore=\n2∗Precision∗Recall\n(Precision+Recall)\nWhere Precision and Recall are defined as follows:\ni. Precision:It is defined as the number of true positive results divided by total","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":43,"lines":{"from":1,"to":21}}}}],["7762a25c-b0b2-47ae-a29d-32f098318e5a",{"pageContent":"F\n1\nScore=\n2∗Precision∗Recall\n(Precision+Recall)\nWhere Precision and Recall are defined as follows:\ni. Precision:It is defined as the number of true positive results divided by total\nnumber of positive results predicted by the classifier.\nPrecision=\nTruePositives\n(TruePositives+FalsePositives)\nii. Recall::  It is defined as the number of true positive results divided by the\nsum of true positives and false negatives.\nRecall=\nTruepositives\n(TruePositives+FalseNegatives)\n3.4  Results\nFollowing the completion of the training process of the algorithm, a video consist-\ning of the three classes – Hauler, Wheel Loader and Excavator in the small-scale\nconstruction environment set up at the PDRL lab, has been used as the test data\nto evaluate the Faster R-CNN algorithm.  The results of the test conducted are as\nfollows:\n•  Each and every frame of the test video has been analyzed for the collection\nof true positives, true negatives, false positives and false negatives which are","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":43,"lines":{"from":15,"to":38}}}}],["8d8642e9-2c57-4fa0-8fed-4523a73eec09",{"pageContent":"32Chapter 3.  Experimental Results\nFigure 3.3: Predictions made by Faster R-CNN\nessential for the calculation of Accuracy, Precision and Recall which in turn\nare required to measure the F1 score.\n•  Figure 3.3 to 3.5 show the screenshots of the detections made by Faster R-\nCNN, YOLOv3 and tiny YOLOv3 along with the confidence intervals of the\ndetections that have been made.\n•  From the figures provided, it is worth noting that the models were successful\nin detecting and tracking the vehicles from various angles and distances, with\na maximum confidence level of 99%.\n•  The tests were also carried out multiple times, by providing live feed from the\nwebcam and using real construction vehicles as test data.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":44,"lines":{"from":1,"to":12}}}}],["f41eb4d6-c55e-4419-bcde-de79bf09bf52",{"pageContent":"3.4.  Results33\nFigure 3.4: Predictions made by YOLOv3\nFigure 3.5: Predictions made by Tiny-YOLOv3","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":45,"lines":{"from":1,"to":3}}}}],["b2443428-c6e2-4ce1-a127-3b6646e07dfa",{"pageContent":"Chapter 4\nAnalysis and Discussion\n4.1  Literature Review\nThe following conclusions have been drawn from the results obtained through the\nliterature review.\n•  From the results of performance of various object detection models on the MS\nCOCO dataset, it can be deduced that SSD and R-FCN models are faster when\ncompared to the Faster R-CNN.\n•  But if accuracy is given preference over speed, then Faster R-CNN performs\nbetter than SSD and R-FCN models.\n•  Faster R-CNN is the most accurate model while using Inception ResNet, run-\nning at a speed of 1 image per second which satisfies the minimum requirement\nto perform object detection and recognition in real-time.\n•  SSD is faster compared to other object detection models but has difficulty in\ndetecting small objects.\n•  Speed of the Faster R-CNN increases as the number of proposals decrease, also\ndecreasing the accuracy of the model.\n•  According to Redmond et al. [8], YOLOv3 is able to detect 10 times faster than","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":47,"lines":{"from":1,"to":18}}}}],["a13f2857-98ff-4335-ba0e-acc83c4519ac",{"pageContent":"decreasing the accuracy of the model.\n•  According to Redmond et al. [8], YOLOv3 is able to detect 10 times faster than\nthe state-of-the-art methods.  Hence YOLOv3 and its variant Tiny-YOLOv3\nhas been selected for the experimentation.\nIt has to be noted that since the construction vehicles move rather slow, speed need\nnot be a concern as long as the algorithm is able to perform object detection and\nrecognition in real-time. But considering the future scope of this research which is au-\ntonomous driving of these vehicles at the construction site and that the construction\nvehicles look similar at certain angles; accuracy has to be given importance. Consid-\nering all the above-mentioned points, it can be deduced that Faster R-CNN performs\nat the same speed as that of SSD and R-FCN models at an accuracy of 32 mAP,\nby reducing the number of proposals to 50. Therefore, Faster R-CNN, YOLOv3 and\nTiny-YOLOv3 have been considered to be suitable and efficient models in real-time","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":47,"lines":{"from":17,"to":29}}}}],["0f6ea917-01c7-4f21-87ff-27acf7f1a8d1",{"pageContent":"by reducing the number of proposals to 50. Therefore, Faster R-CNN, YOLOv3 and\nTiny-YOLOv3 have been considered to be suitable and efficient models in real-time\ndetection and tracking of the construction vehicles at the scaled site.\n35","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":47,"lines":{"from":28,"to":31}}}}],["04f8b5da-2f6d-4690-ae86-ade3e0bb277a",{"pageContent":"36Chapter 4.  Analysis and Discussion\n4.2  Experiment\nThe results obtained by Faster R-CNN, YOLOv3 and TinyYOLOv3 algorithms have\nbeen tabulated.  Tables 4.1 to 4.3 are the representation of the results obtained by\nthe algorithms after evaluating every frame in the test video.\n4.2.1  Accuracy\nThe number of true positives, true negatives, false positives and false negatives that\nhave been obtained by the models on the test video have been presented in the Table\n4.1.\nAlgorithmFaster R-CNNYOLOv3Tiny-YOLOv3\nTrue Positives11331214986\nTrue Negatives192195184\nFalse Positives443956\nFalse Negatives207126354\nTable 4.1: Calculated Values of the Algorithms\nFrom the values presented in the Table 4.1, the accuracy of the models has been\ncalculated to be 84.07%, 89.51%, 74.05% respectively for Faster R-CNN, YOLOv3\nand tiny YOLOv3 and it is visualized in Figure 4.1. The reason for the accuracy score\nof YOLOv3 being higher is because of its architecture where the object detections","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":48,"lines":{"from":1,"to":19}}}}],["54ddb86c-116e-4df3-bcbf-71b28ceefdac",{"pageContent":"and tiny YOLOv3 and it is visualized in Figure 4.1. The reason for the accuracy score\nof YOLOv3 being higher is because of its architecture where the object detections\nare performed at three different scales, making YOLOv3 more efficient in detecting\nsmaller objects or detecting objects in difficult scenarios such as objects appearing\npartly in a certain frame.  Since in certain scenarios, the objects are located on the\nfarther side of the scaled site, they appear smaller. As a result, Faster R-CNN strug-\ngled to predict the objects with higher accuracy during these scenarios.  Therefore,\nit can be said that the YOLOv3 model is highly accurate in the real-time detection\nand tracking of the construction vehicles – Hauler, Wheel Loader and Excavator at\nthe scaled construction site environment.\n4.2.2  F\n1\nScore\nSince the F\n1\nscore of a model is defined as the harmonic mean of precision and recall,\nit is essential that they are calculated prior to the calculation of the F\n1\nscore.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":48,"lines":{"from":18,"to":36}}}}],["803fd9cd-2266-462f-b440-c92dafbd8875",{"pageContent":"4.2.2  F\n1\nScore\nSince the F\n1\nscore of a model is defined as the harmonic mean of precision and recall,\nit is essential that they are calculated prior to the calculation of the F\n1\nscore.\nPrecision\nThe precision of a model is dependent on the number of true positives and number of\nfalse positives. The number of true positives and false positives have been obtained\nby models on the test video.\nFrom the values presented in the Table 4.2, the precision of the models has been\ncalculated as 0.9626, 0.9688, 0.9462 respectively for Faster R-CNN, YOLOv3 and\nTiny-YOLOv3.  The precision of YOLOv3 is higher that Faster R-CNN and Tiny-\nYOLOv3 is because, its predictions are very precise as it can detect at three different","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":48,"lines":{"from":28,"to":44}}}}],["5ad35af3-7f60-4f14-8085-e8b6eec25df0",{"pageContent":"4.2.  Experiment37\nFigure 4.1: Graphical Representation of Accuracy\nAlgorithmFaster R-CNNYOLOv3Tiny-YOLOv3\nTrue Positives11331214986\nFalse Positives443956\nTable 4.2: Calculated Values of the Algorithms\nscales, whereas Faster R-CNN and Tiny-YOLOv3 struggled to show correct predic-\ntion where the size of the object is considerably small. Therefore, it can be concluded\nthat the precision of YOLOv3 in real-time detection and tracking of the construction\nvehicles is really good.\nRecall\nThe recall of a model is dependent on the number of true positives and number of\nfalse negatives. The number of true positives and false negatives have been obtained\nby the models on the test video.\nAlgorithmFaster R-CNNYOLOv3Tiny-YOLOv3\nTrue Positives11331214986\nFalse Negatives207126354\nTable 4.3: Calculated Values of the Algorithms\nFrom the values presented in the Table 4.3, the recall of the models has been\ncalculated as 0.8455, 0.9059, 0.7358 respectively for Faster R-CNN, YOLOv3 and","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":49,"lines":{"from":1,"to":20}}}}],["1da482f7-f0df-438b-9ae6-7e0fe686b30a",{"pageContent":"From the values presented in the Table 4.3, the recall of the models has been\ncalculated as 0.8455, 0.9059, 0.7358 respectively for Faster R-CNN, YOLOv3 and\ntiny YOLOv3. The recall values for Faster R-CNN and Tiny-YOLOv3 is lower than\nYOLOv3 as they have shown incorrect detections in many frames where the object\nis farther away or the size of the object is smaller, while YOLOv3 provided better\nresults.  Therefore, it can be concluded that the recall of YOLOv3 in real-time de-\ntection and tracking of the construction vehicles is really good compared to other","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":49,"lines":{"from":19,"to":25}}}}],["2a4b7d56-804e-4bf5-aa9c-c82bedebfabd",{"pageContent":"38Chapter 4.  Analysis and Discussion\nmodels selected.\nFigure 4.2: Graphical representation of F1 score\nUsing the Precision and Recall values obtained from the previous steps, the F\n1\nscore of the Faster R-CNN, YOLOv3 and tiny YOLOv3 models has been calculated\nas 0.9, 0.9362 and 0.8278 respectively and it is visualized in figure 4.2 .  Since the\nperformance of the model is directly proportional to the F\n1\nscore and the upper\nlimit of the F\n1\nscore being 1, it can be said that the performance of YOLOv3 model\nin real-time detection and tracking of the construction vehicles at the scaled site is\nhigher than other models used in this experiment.\n4.3  Validity Threats\n4.3.1  Internal Validity\nSince the images for YOLOv3 have been resized into 608 x 608, while for the Faster\nR-CNN and Tiny-YOLOv3 the images have been resized into 416 x 416, this might\nbe taken into consideration as an internal validity threat. Errors might occur while","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":50,"lines":{"from":1,"to":20}}}}],["b7bd2fb2-549e-494e-879a-7a535d9c7d0c",{"pageContent":"R-CNN and Tiny-YOLOv3 the images have been resized into 416 x 416, this might\nbe taken into consideration as an internal validity threat. Errors might occur while\ncollection of the data and calculation of metrics,  which are a threat to validity.\nThis threat is mitigated by taking down the readings and confirming multiple times.\nImportant data used in the results such as classification loss, number of iterations,\netc. are taken from the Tensor Board and Darknet to avoid any errors.\n4.3.2  External Validity\nIt has been ensured that the train and test dataset have been properly separated in\nsuch a way that no clips from the train dataset is present in the test dataset. External\nvalidity is known as the validity of the experiment performed to be generalizable to\nthe external groups that are larger and wider. Even though the algorithms have been","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":50,"lines":{"from":19,"to":29}}}}],["2a27812f-ca59-4a7a-8415-130f3cf83902",{"pageContent":"4.3.  Validity Threats39\ntrained using the data obtained from scaled site provided at PDRL, the algorithms\nhave also been tested on real world images taken at the construction sites and the\nresults have also been presented in Figures 6.1, 6.2 and 6.3.\n4.3.3  Conclusion Validity\nThe threats relating to conclusion validity generally arise when no proper steps are\nfollowed for conducting a research and by selecting inappropriate metrics for evalu-\nation.  In this thesis, conclusion validity is avoided by selecting suitable metrics for\nevaluating the selected algorithms as well as following proper steps for conducting a\nresearch.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":51,"lines":{"from":1,"to":10}}}}],["c8619035-bb3a-4826-b59c-24b1e526dea5",{"pageContent":"Chapter 5\nConclusions and Future Work\n5.1  Conclusion\nThis thesis report discusses about the most suitable deep-learning models for real-\ntime object detection and recognition and evaluates the performance of these algo-\nrithms on the detection and recognition of three construction vehicles at a scaled site.\nThe results of this research are discussed in this chapter, along with answers to the re-\nsearch questions formulated in section 3.1, some concluding remarks and future work.\nRQ1. What are the most suitable and efficient Deep Learning models for\nreal-time object recognition and tracking of construction vehicles?\nA literature review has been performed to obtain knowledge about various deep-\nlearning models that are capable of performing real-time object detection and recog-\nnition.   By studying the performance of these algorithms on a standard dataset,\nYOLOv3, Tiny-YOLOv3 and Faster R-CNN have been identified as the most suitable","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":53,"lines":{"from":1,"to":14}}}}],["8f81ada9-12d1-4f97-b40f-079829530b37",{"pageContent":"nition.   By studying the performance of these algorithms on a standard dataset,\nYOLOv3, Tiny-YOLOv3 and Faster R-CNN have been identified as the most suitable\nand efficient deep-learning models to perform real-time object detection and recog-\nnition of scaled construction vehicles. It has been concluded that Faster R-CNN can\nperform equally in terms of speed with SSD and FCN models, while showing better\naccuracy compared to these models.  The architecture of these algorithms has been\nstudied and presented to gain deeper understanding of these algorithms.\nRQ2. How is the classification performance of the Deep Learning models\nthat are selected for object recognition?\nAn experiment has been carried out to evaluate the classification performance of\nthese deep-learning algorithms.   After the preparation of dataset,  the algorithms\nhave been trained on the train-dataset.  The trained models have been tested on","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":53,"lines":{"from":13,"to":24}}}}],["8220b64d-5585-478c-a7b3-87af0583c09f",{"pageContent":"these deep-learning algorithms.   After the preparation of dataset,  the algorithms\nhave been trained on the train-dataset.  The trained models have been tested on\nthe test video collected at the site, from which the number of true positives, true\nnegatives, false positives and false negatives have been identified for each frame of\nthe detections made by the three deep-learning models on the test video.  Using\nthese results the Accuracy, Precision, Recall and finally the F1 score of the models\nhave been calculated and the performance of the YOLOv3, Faster R-CNN and Tiny-\nYOLOv3 model have been evaluated and compared.  The results of the experiment\nhave been presented with detailed analysis of the results.  It has been concluded\nfrom the analysis that out of the three deep-learning models selected for this thesis,\nYOLOv3 showed best classification performance,  followed by Faster R-CNN and\nlastly by Tiny-YOLOv3.\n41","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":53,"lines":{"from":23,"to":35}}}}],["f41c32f5-4c53-49e5-b276-f5162b167cd7",{"pageContent":"42Chapter 5.  Conclusions and Future Work\n5.2  Implications to Practice\nFirstly, the readers of this report can find relevant information regarding Machine\nLearning, Deep Learning and the working of Neural Networks, which is required to\nunderstand and work with several deep-learning models.  The readers can also find\ninformation about the most suitable deep-learning algorithms for performing real-\ntime object detection and recognition and especially gain deeper understanding of\nthe deep-learning models – YOLOv3, Tiny-YOLOv3 and Faster R-CNN.\nMost importantly, they can obtain knowledge about the steps in dataset prepa-\nration for training the selected algorithm to perform desired detections.  The prac-\ntitioners can collect information regarding various evaluation metrics to evaluate\nthe classification performance of the deep-learning algorithms.   Using the results\nobtained through this research, the practitioners can apply these methods to their","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":54,"lines":{"from":1,"to":13}}}}],["e5bd4304-7d70-49ad-a802-3f3ef8754bc1",{"pageContent":"the classification performance of the deep-learning algorithms.   Using the results\nobtained through this research, the practitioners can apply these methods to their\nresearch depending on the similarity of the research area. They can also learn from\nthe limitations of this research and try to obtain better results for their research.\n5.3  Future Work\nThe autonomous driving of construction vehicles is a topic where very little research\nhas been performed. There is a considerable difference between autonomous driving\nof the construction vehicles at the construction site and the regular vehicles on the\nroads, as the environment is different, factors that affect autonomous driving will\nalso change and can be particularly challenging. Therefore, future work can be done\nat the scaled construction site at PDRL laboratory, BTH., to develop autonomous\nvehicles using the results obtained from this research.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":54,"lines":{"from":12,"to":23}}}}],["5825019b-be6d-4784-aa47-129669054ccf",{"pageContent":"References\n[1]  R.   Philippe,Volvo  Excavation  Site,   2020.   [Online].   Available:https:\n//www.korestudios.com/portfolio/volvo-construction-equipment/\n[2]  AHK,ExemplarConstructionSite,2018.[On-\nline].Available:https://urbantoronto.ca/news/2018/04/\ntorontos-largest-construction-site-well-spadina-front\n[3]  V. G. Maltarollo, K. M. Honório, and A. B. F. da Silva, “Applications of artificial\nneural networks in chemical problems,”Artificial neural networks-architectures\nand applications, pp. 203–223, 2013.\n[4]  TutorialsPoint,Supervised    Learning,2020.[Online].Avail-\nable:https://www.tutorialspoint.com/artificial_neural_network/artificial_\nneural_network_supervised_learning.htm\n[5]  B.  Frank,Deep Learning the Beautiful Mind,   2016.  [Online].  Available:\nwww.mindwise-groningen.nl/deep-learning-the-beautiful-mind/\n[6]  M. D. Zeiler and R. Fergus, “Visualizing and understanding convolutional net-\nworks,” inEuropean conference on computer vision.   Springer, 2014, pp. 818–\n833.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":55,"lines":{"from":1,"to":17}}}}],["76a86d0d-ff96-4a85-96bb-e5e769fe1ee6",{"pageContent":"[6]  M. D. Zeiler and R. Fergus, “Visualizing and understanding convolutional net-\nworks,” inEuropean conference on computer vision.   Springer, 2014, pp. 818–\n833.\n[7]  P.  Firelord,Pictorial example of max-pooling,   2018.  [Online].  Available:\nhttps://computersciencewiki.org/index.php/Max-pooling_/_Pooling\n[8]  J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once: Uni-\nfied, real-time object detection,” inProceedings of the IEEE conference on com-\nputer vision and patternrecognition, 2016, pp. 779–788.\n[9]  CyberAILab,A  Closer  Look  at  YOLOv3,   2018.   [Online].   Available:\nhttps://www.cyberailab.com/home/a-closer-look-at-yolov3\n[10]  C. C. Nguyen, G. S. Tran, T. P. Nghiem, N. Q. Doan, D. Gratadour, J. C.\nBurie, and C. M. Luong, “Towards real-time smile detection based on faster\nregion convolutional neural network,” in2018 1st International Conference on\nMultimedia Analysis and Pattern Recognition (MAPR).   IEEE, 2018, pp. 1–6.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":55,"lines":{"from":15,"to":28}}}}],["4e96dc1a-8cff-4797-9170-92496cc957ba",{"pageContent":"region convolutional neural network,” in2018 1st International Conference on\nMultimedia Analysis and Pattern Recognition (MAPR).   IEEE, 2018, pp. 1–6.\n[11]  Z. Deng, H. Sun, S. Zhou, J. Zhao, L. Lei, and H. Zou, “Multi-scale object\ndetection in remote sensing imagery with convolutional neural networks,”ISPRS\njournal of photogrammetry and remote sensing, vol. 145, pp. 3–22, 2018.\n43","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":55,"lines":{"from":27,"to":32}}}}],["b08bd689-56be-4673-9512-428efaf017fc",{"pageContent":"44References\n[12]  D. Xiao, F. Shan, Z. Li, B. T. Le, X. Liu, and X. Li, “A target detection model\nbased on improved tiny-yolov3 under the environment of mining truck,”IEEE\nAccess, vol. 7, pp. 123 757–123 764, 2019.\n[13]  S.    Smriti,Computer  Vision  Makes  Autonomous  Vehicles  Intelli-\ngent  and  Reliable,   2019.   [Online].   Available:www.analyticsinsight.net/\ncomputer-vision-makes-autonomous-vehicles-intelligent-and-reliable/\n[14]  G. Lewis, “Object detection for autonomous vehicles,” 2014.\n[15]  V. Marco,What Is Machine Learning? A Definition, 2017. [Online]. Available:\nwww.expertsystem.com/machine-learning-definition/\n[16]  I. Goodfellow, Y. Bengio, and A. Courville,Deep learning. Book in preparation\nfor MIT Press, 2016. [Online]. Available: http://www.deeplearningbook.org\n[17]  B.   Volodymyr,Machine  Learning  Algorithms:   4  Types  You  Should\nKnow, 2018. [Online]. Available: www.theappsolutions.com/blog/development/\nmachine-learning-algorithm-types/","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":56,"lines":{"from":1,"to":15}}}}],["8f6d4c45-349c-45e5-ae19-52b08a29f119",{"pageContent":"[17]  B.   Volodymyr,Machine  Learning  Algorithms:   4  Types  You  Should\nKnow, 2018. [Online]. Available: www.theappsolutions.com/blog/development/\nmachine-learning-algorithm-types/\n[18]  S.  Ray,Essentials  of  machine  learning  algorithms  (with  python  and  r\ncodes), 2017. [Online]. Available:  http://www.analyticsvidhya.com/blog/2015/\n08/common-machine-learning-algorithms\n[19]  C. M. Bishop,Patternrecognition and machine learning.   springer, 2006.\n[20]  J.  Aaron,Everything You Need to Know About Artificial Neural Networks,\n2015. [Online].  Available:  www.medium.com/technology-invention-and-more/\neverything-you-need-to-know-about-artificial-neural-networks-57fac18245a1\n[21]  N.   Avinash,Neural  Network  Models  in  R,   2019.   [Online].   Available:\nwww.datacamp.com/community/tutorials/neural-network-models-r\n[22]  D.  Luke,What Is an Artificial Neural Network?   Here’s Everything You\nNeed to Know,  2019.  [Online].  Available:   www.digitaltrends.com/cool-tech/","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":56,"lines":{"from":13,"to":26}}}}],["1d439033-1535-4d3a-8629-e30b16709e12",{"pageContent":"[22]  D.  Luke,What Is an Artificial Neural Network?   Here’s Everything You\nNeed to Know,  2019.  [Online].  Available:   www.digitaltrends.com/cool-tech/\nwhat-is-an-artificial-neural-network/\n[23]  D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning representations\nby back-propagating errors,”nature, vol. 323, no. 6088, pp. 533–536, 1986.\n[24]  B. Jason,A Gentle Introduction to Computer Vision, 2019. [Online]. Available:\nwww.machinelearningmastery.com/what-is-computer-vision/\n[25]  Technopedia,What Is Computer Vision? - Definition from Techopedia, 2019.\n[Online]. Available: www.techopedia.com/definition/32309/computer-vision\n[26]  S. J. Prince,Computer vision: models, learning, and inference.    Cambridge\nUniversity Press, 2012.\n[27]  T. Celik and H. Kusetogullari, “Solar-powered automated road surveillance sys-\ntem for speed violation detection,”IEEE Transactions on Industrial Electronics,\nvol. 57, no. 9, pp. 3216–3227, 2009.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":56,"lines":{"from":25,"to":38}}}}],["9245a262-6e87-4688-9e42-2a07f2f0a7ff",{"pageContent":"References45\n[28]  H. Kusetoˇgullari and T. Celik, “Real time vehicle detection and driver warning\nsystem using computer vision,” 2010.\n[29]  M. F. Demir, A. Cankirli, B. Karabatak, A. Yavariabdi, E. Mendi, and H. Kuse-\ntogullari, “Real-time resistor color code recognition using image processing in\nmobile devices,” in2018 International Conference on Intelligent Systems (IS).\nIEEE, 2018, pp. 26–30.\n[30]  A. Cheddad, H. Kusetogullari, and H. Grahn, “Object recognition using shape\ngrowth pattern,” inProceedings of the 10th International Symposium on Image\nand Signal Processing and Analysis.   IEEE, 2017, pp. 47–52.\n[31]  F. G. YaŞar and H. KusetoĞullari, “Underwater human body detection using\ncomputer vision algorithms,” in2018 26th Signal Processing and Communica-\ntions Applications Conference (SIU).   IEEE, 2018, pp. 1–4.\n[32]  H. Kusetogullari, H. Demirel, T. Celik, and S. Bayindir, “Real time detection\nand tracking of vehicles for speed measurement and licence plate detection,” in","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":57,"lines":{"from":1,"to":15}}}}],["a40d1b72-9873-461b-aadc-205fd0a611af",{"pageContent":"[32]  H. Kusetogullari, H. Demirel, T. Celik, and S. Bayindir, “Real time detection\nand tracking of vehicles for speed measurement and licence plate detection,” in\nThe Seventh IASTED International Conference on Visualization, Imaging and\nImage Processing.   ACTA Press, 2007, pp. 53–58.\n[33]  N. Sebe, I. Cohen, A. Garg, and T. S. Huang,Machine learning in computer\nvision.   Springer Science & Business Media, 2005, vol. 29.\n[34]  Z. Cai, Q. Fan, R. S. Feris, and N. Vasconcelos, “A unified multi-scale deep\nconvolutional neural network for fast object detection,” inEuropean conference\non computer vision.   Springer, 2016, pp. 354–370.\n[35]  M.  Anukrati,A Comprehensive Guide to Types of Neural Networks,  2019.\n[Online]. Available: www.digitalvidya.com/blog/types-of-neural-networks/\n[36]  J.  Redmon  and  A.  Farhadi,  “Yolov3:  An  incremental  improvement,”arXiv\npreprint arXiv:1804.02767, 2018.\n[37]  K.  Ayoosh,What’s New in YOLO v3?,  2018.  [Online].  Available:   www.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":57,"lines":{"from":14,"to":27}}}}],["6c6f927e-dd8a-4108-b3d8-f283e1f098e9",{"pageContent":"[36]  J.  Redmon  and  A.  Farhadi,  “Yolov3:  An  incremental  improvement,”arXiv\npreprint arXiv:1804.02767, 2018.\n[37]  K.  Ayoosh,What’s New in YOLO v3?,  2018.  [Online].  Available:   www.\ntowardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b\n[38]  S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time object\ndetection with region proposal networks,”  inAdvances in neural information\nprocessing systems, 2015, pp. 91–99.\n[39]  G.Subrata,ADeeperLookatHowFaster-RCNN\nWorks,2018.[Online].Available:www.medium.com/@whatdhack/\na-deeper-look-at-how-faster-rcnn-works-84081284e1cd\n[40]  Y. Luo,  S. Li,  K. Sun,  R. Renteria,  and K. Choi,  “Implementation of deep\nlearning neural network for real-time object recognition in opencl framework,”\nin2017 International SoC Design Conference (ISOCC).   IEEE, 2017, pp. 298–\n299.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":57,"lines":{"from":25,"to":38}}}}],["af4e082d-601c-4cd8-a3d8-0613292f8521",{"pageContent":"46References\n[41]  G. Alpaydin, “An adaptive deep neural network for detection, recognition of\nobjects with long range auto surveillance.” inICSC, 2018, pp. 316–317.\n[42]  D. Maturana and S. Scherer, “Voxnet:  A 3d convolutional neural network for\nreal-time object recognition,” in2015 IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS).   IEEE, 2015, pp. 922–928.\n[43]  R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies\nfor accurate object detection and semantic segmentation,” inProceedings of the\nIEEE conference on computer vision and patternrecognition, 2014, pp. 580–587.\n[44]  K.-H. Kim,  S. Hong,  B. Roh,  Y. Cheon,  and M. Park,  “Pvanet:  Deep but\nlightweight  neural  networks  for  real-time  object  detection,”arXiv preprint\narXiv:1608.08021, 2016.\n[45]  J. Dai, Y. Li, K. He, and J. Sun, “R-fcn: Object detection via region-based fully\nconvolutional networks,” inAdvances in neural information processing systems,\n2016, pp. 379–387.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":58,"lines":{"from":1,"to":15}}}}],["017ba726-d07a-4ebc-824c-6264d89140dc",{"pageContent":"[45]  J. Dai, Y. Li, K. He, and J. Sun, “R-fcn: Object detection via region-based fully\nconvolutional networks,” inAdvances in neural information processing systems,\n2016, pp. 379–387.\n[46]  T. Kong, A. Yao, Y. Chen, and F. Sun, “Hypernet:  Towards accurate region\nproposal generation and joint object detection,”  inProceedings of the IEEE\nconference on computer vision and patternrecognition, 2016, pp. 845–853.\n[47]  W. Liu,  D. Anguelov,  D. Erhan,  C. Szegedy,  S. Reed,  C.-Y. Fu,  and A. C.\nBerg, “Ssd: Single shot multibox detector,” inEuropean conference on computer\nvision.   Springer, 2016, pp. 21–37.\n[48]  B. A. Kitchenham, D. Budgen, and P. Brereton,Evidence-based software engi-\nneering and systematic reviews.   CRC press, 2015, vol. 4.\n[49]  L. Perez and J. Wang, “The effectiveness of data augmentation in image classi-\nfication using deep learning,”arXiv preprint arXiv:1712.04621, 2017.\n[50]  A. Hernández-García and P. König, “Data augmentation instead of explicit reg-","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":58,"lines":{"from":13,"to":26}}}}],["d2673388-e640-4ac4-a779-f747a1cb7271",{"pageContent":"fication using deep learning,”arXiv preprint arXiv:1712.04621, 2017.\n[50]  A. Hernández-García and P. König, “Data augmentation instead of explicit reg-\nularization,”arXiv preprint arXiv:1806.03852, 2018.\n[51]  A.Shilpa,FasterR-CNNforObjectDetec-\ntion,2019.[Online].Available:www.towardsdatascience.com/\nfaster-r-cnn-for-object-detection-a-technical-summary-474c5b857b46\n[52]  K.  Ayoosh,How to Implement a YOLO (v3) Object Detector from Scratch\nin PyTorch: Part 1, 2018. [Online]. Available:  www.kdnuggets.com/2018/05/\nimplement-yolo-v3-object-detector-pytorch-part-1.html\n[53]  N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang, “On\nlarge-batch training for deep learning:  Generalization gap and sharp minima,”\narXiv preprint arXiv:1609.04836, 2016.\n[54]  M. Elhoseiny, A. Bakry, and A. Elgammal, “Multiclass object classification in\nvideo surveillance systems-experimental study,” inProceedings of the IEEE Con-","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":58,"lines":{"from":25,"to":38}}}}],["53f788fe-8cc0-4971-a842-75cacaf75de9",{"pageContent":"[54]  M. Elhoseiny, A. Bakry, and A. Elgammal, “Multiclass object classification in\nvideo surveillance systems-experimental study,” inProceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition Workshops, 2013, pp. 788–\n793.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":58,"lines":{"from":37,"to":40}}}}],["c48f9cfb-6629-4517-9d22-bea8c9277787",{"pageContent":"References47\n[55]  D. Z. Wang and I. Posner, “Voting for voting in online point cloud object detec-\ntion.” inRobotics: Science and Systems, vol. 1, no. 3, 2015, pp. 10–15 607.\n[56]  Y.  Wei,  N.  Song,  L.  Ke,  M.-C.  Chang,  and  S.  Lyu,  “Street  object  detec-\ntion/tracking  for  ai  city  traffic  analysis,”  in2017 IEEE SmartWorld, Ubiq-\nuitous Intelligence & Computing, Advanced & Trusted Computed, Scalable\nComputing & Communications, Cloud & Big Data Computing, Internet of\nPeople and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBD-\nCom/IOP/SCI).   IEEE, 2017, pp. 1–5.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":59,"lines":{"from":1,"to":9}}}}],["ebe7ad6a-1b42-4b30-babf-9cd6227ba476",{"pageContent":"Chapter 6\nAppendix\n49","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":61,"lines":{"from":1,"to":3}}}}],["67055809-550f-4ce2-bcc2-e2f97d1c1800",{"pageContent":"50Chapter 6.  Appendix\nFigure 6.1: Detection of real vehicles\nFigure 6.2: Detection of real vehicles","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":62,"lines":{"from":1,"to":3}}}}],["1799b445-830f-49be-ae27-5dbe97cdac4a",{"pageContent":"51\nFigure 6.3: Detection of real vehicles\nFigure 6.4: Classification loss graph plotted using TensorBoard","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":63,"lines":{"from":1,"to":3}}}}],["706454a0-11c7-40e7-bc84-f8ead5f4e470",{"pageContent":"52Chapter 6.  Appendix\nFigure 6.5: Detection of real vehicles\nFigure 6.6: Detection of real vehicles","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":64,"lines":{"from":1,"to":3}}}}],["e713fa35-f1cc-4737-97e2-7d7277389e96",{"pageContent":"Faculty of Computing, Blekinge Institute of Technology, 371 79 Karlskrona, Sweden","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\FULLTEXT02.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","Author":"mbk","Creator":"LaTeX with hyperref","Producer":"Acrobat Distiller 20.0 (Windows)","CreationDate":"D:20200317124623+01'00'","ModDate":"D:20200317124623+01'00'"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:creator":"mbk","dc:description":"","dc:title":"chadalawadasaikrishna_4360_243835_SKC_Thesis_final.pdf","xmp:createdate":"2020-03-17T12:46:23+01:00","xmp:creatortool":"LaTeX with hyperref","xmp:modifydate":"2020-03-17T12:46:23+01:00","xmp:metadatadate":"2020-02-10T21:22:01+01:00","pdf:keywords":"","pdf:producer":"Acrobat Distiller 20.0 (Windows)","pdf:trapped":"False","pdfx:ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1","xmpmm:documentid":"uuid:8c73496b-7d8e-425a-9470-5673997c6dd2","xmpmm:instanceid":"uuid:db257831-b2c8-4293-bbe1-deac39f9c9fe","pdfaid:part":"1","pdfaid:conformance":"B"}},"totalPages":66},"loc":{"pageNumber":66,"lines":{"from":1,"to":1}}}}],["7e3a6c03-195e-4361-b0fb-cf651e9ff576",{"pageContent":"Real-Time, Cloud-based Object Detection for\nUnmanned Aerial Vehicles\nJangwon Lee, Jingya Wang, David Crandall, Selma\nˇ\nSabanovi\n ́\nc, and Geoffrey Fox\nSchool of Informatics and Computing\nIndiana University\nBloomington, Indiana 47408\nEmail:{leejang, wang203, djcran, selmas, gcf}@indiana.edu\nAbstract—Real-time object detection is crucial for many ap-\nplications of Unmanned Aerial Vehicles (UAVs) such as recon-\nnaissance and surveillance, search-and-rescue, and infrastructure\ninspection. In the last few years, Convolutional Neural Networks\n(CNNs) have emerged as a powerful class of models for recog-\nnizing image content, and are widely considered in the computer\nvision community to be the de facto standard approach for\nmost problems. However, object detection based on CNNs is\nextremely computationally demanding, typically requiring high-\nend Graphics Processing Units (GPUs) that require too much\npower and weight, especially for a lightweight and low-cost drone.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":1,"lines":{"from":1,"to":22}}}}],["d5b9508d-f4b9-430b-a7c0-9cad6efa4527",{"pageContent":"extremely computationally demanding, typically requiring high-\nend Graphics Processing Units (GPUs) that require too much\npower and weight, especially for a lightweight and low-cost drone.\nIn this paper, we propose moving the computation to an off-board\ncomputing cloud, while keeping low-level object detection and\nshort-term navigation onboard. We apply Faster Regions with\nCNNs (R-CNNs), a state-of-the-art algorithm, to detect not one\nor two but hundreds of object types in near real-time.\nI.  INTRODUCTION\nRecent years have brought increasing interest in autonomous\nUAVs  and  their  applications,  including  reconnaissance  and\nsurveillance,  search-and-rescue,  and  infrastructure  inspection\n[1]–[5].  Visual  object  detection  is  an  important  component\nof  such  UAV  applications,  and  is  critical  to  develop  fully\nautonomous  systems.  However,  the  task  of  object  detection\nis  very  challenging,  and  is  made  even  more  difficult  by  the","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":1,"lines":{"from":20,"to":35}}}}],["95a02cb1-d572-4ddf-876b-977f2eb9a78b",{"pageContent":"autonomous  systems.  However,  the  task  of  object  detection\nis  very  challenging,  and  is  made  even  more  difficult  by  the\nimaging conditions aboard low-cost consumer UAVs: images\nare  often  noisy  and  blurred  due  to  UAV  motion,  onboard\ncameras  often  have  relatively  low  resolution,  and  targets  are\nusually  quite  small.  The  task  is  even  more  difficult  because\nof  the  need  for  near  real-time  performance  in  many  UAV\napplications.\nMany  UAV  studies  have  tried  to  detect  and  track  certain\ntypes  of  objects  such  as  vehicles  [6],  [7],  people  including\nmoving  pedestrians  [8],  [9],  and  landmarks  for  autonomous\nnavigation and landing [10], [11] in real-time. However, there\nare only a few that consider detecting multiple objects [12], de-\nspite the fact that detecting multiple target objects is obviously\nimportant for many applications of UAVs. In our view, this gap","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":1,"lines":{"from":34,"to":48}}}}],["9623c0d2-db7a-40c8-a4ab-f59dca22630b",{"pageContent":"are only a few that consider detecting multiple objects [12], de-\nspite the fact that detecting multiple target objects is obviously\nimportant for many applications of UAVs. In our view, this gap\nbetween  application  needs  and  technical  capabilities  are  due\nto three practical but critical limitations: (1) object recognition\nalgorithms often need to be hand-tuned to particular object and\ncontext types; (2) it is difficult to build and store a variety of\ntarget object models, especially when the objects are diverse in\nappearance,  and  (3)  real-time  object  detection  demands  high\ncomputing  power  even  to  detect  a  single  object,  much  less\nt\nInput frames Objectnessestimation\nCloud-based object detection with R-CNNs\nFig. 1.A drone is able to detect hundreds of object categories in near real-\ntime  with  our  hybrid  approach.  Convolutional  Neural  Network-based  object\ndetection  runs  on  a  remote  cloud,  while  a  local  machine  plays  a  role  in","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":1,"lines":{"from":46,"to":61}}}}],["d828b395-f6db-49ed-8fd4-8548d37dfced",{"pageContent":"time  with  our  hybrid  approach.  Convolutional  Neural  Network-based  object\ndetection  runs  on  a  remote  cloud,  while  a  local  machine  plays  a  role  in\nobjectness estimation, short-term navigation and stability control.\nwhen many target objects are involved.\nHowever, object recognition performance is rapidly improv-\ning, thanks to breakthrough techniques in computer vision that\nwork  well  on  a  wide  variety  of  objects.  Most  of  these  tech-\nniques are based on “deep learning” with Convolutional Neural\nNetworks,  and  have  delivered  striking  performance  increases\non a range of recognition problems [13]–[15]. The key idea is\nto learn the object models from raw pixel data, instead of using\nhand-tuned  features  as  in  tradition  recognition  approaches.\nTraining  these  deep  models  typically  requires  large  training\ndatasets,  but  this  problem  has  also  been  overcome  by  new\nlarge-scale labeled datasets like ImageNet [16]. Unfortunately,","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":1,"lines":{"from":60,"to":74}}}}],["398aa72f-c181-4777-9b1d-01532f3438cb",{"pageContent":"Training  these  deep  models  typically  requires  large  training\ndatasets,  but  this  problem  has  also  been  overcome  by  new\nlarge-scale labeled datasets like ImageNet [16]. Unfortunately,\nthese  new  techniques  also  require  unprecedented  amounts  of\ncomputation;  the  number  of  parameters  in  an  object  model\nis typically in the millions or billions, requiring gigabytes of\nmemory, and training and recognition using the object models\nrequires  high-end  Graphics  Processing  Units  (GPUs).  Using\nthese new techniques on low-cost, light-weight drones is thus\ninfeasible because of the size, weight, and power requirements\nof these devices.\nIn  this  paper,  we  propose  moving  the  computationally-\ndemanding  object  recognition  to  a  remote  compute  cloud,\ninstead  of  trying  to  implement  it  on  the  drone  itself,  let-\nting  us  take  advantage  of  these  breakthroughs  in  computer\n2017 First IEEE International Conference on Robotic Computing","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":1,"lines":{"from":72,"to":87}}}}],["f843ee6e-a31a-4761-a369-820afd91265a",{"pageContent":"instead  of  trying  to  implement  it  on  the  drone  itself,  let-\nting  us  take  advantage  of  these  breakthroughs  in  computer\n2017 First IEEE International Conference on Robotic Computing\n978-1-5090-6724-4/17 $31.00 © 2017 IEEE\nDOI 10.1109/IRC.2017.77\n36","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":1,"lines":{"from":85,"to":90}}}}],["847b705c-2820-49c6-9674-ad2c6770796f",{"pageContent":"vision technology without paying the weight and power costs.\nCommercial compute clouds like Amazon Web Services also\nhave  the  advantage  of  allowing  on-demand  access  to  nearly\nunlimited  compute  resources.  This  is  especially  useful  for\ndrone applications where most of the processing for navigation\nand control can be handled onboard, but short bursts of intense\ncomputation are required when an unknown object is detected\nor  during  active  object  search  and  tracking.  Using  the  cloud\nsystem,  we  are  able  to  apply  Faster  R-CNNs  [17],  a  state-\nof-the-art recognition algorithm, to detect not one or two but\nhundredsof object types in near real-time. Of course, moving\nrecognition  to  the  cloud  introduces  unpredictable  lag  from\ncommunication  latencies.  Thus,  we  retain  some  visual  pro-\ncessing  locally,  including  a  triage  step  that  quickly  identifies\nregion(s) of an image that are likely to correspond with objects","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":2,"lines":{"from":1,"to":15}}}}],["10523043-5d35-4e84-a84e-71fee6590c06",{"pageContent":"cessing  locally,  including  a  triage  step  that  quickly  identifies\nregion(s) of an image that are likely to correspond with objects\nof  interest,  as  well  as  low-level  feature  matching  needed  for\nreal-time  navigation  and  stability.  Fig.  1  shows  the  image\nprocessing dataflow of this hybrid approach that allows a low-\ncost drone to detect hundreds of objects in near real-time. We\nreport  on  experiments  measuring  accuracy,  recognition  time,\nand  latencies  using  the  low-cost  Parrot  AR  Drone  2.0  as  a\nhardware platform, in the scenario of the drone searching for\ntarget objects in an indoor environment.\nII.  R\nELATED WORK\nA. Deep Learning Approaches in Robotics\nWe  apply  object  detection  based  on  Convolutional  Neural\nNetworks (CNNs) [13], [18] for detecting a variety of objects\nin  images  captured  from  a  drone.  These  networks  are  a  type\nof deep learning approach that are much like traditional multi-","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":2,"lines":{"from":14,"to":30}}}}],["04d6d4f9-a548-47f8-9c47-91cd5b93ae65",{"pageContent":"Networks (CNNs) [13], [18] for detecting a variety of objects\nin  images  captured  from  a  drone.  These  networks  are  a  type\nof deep learning approach that are much like traditional multi-\nlayer,  feed-forward  perceptron  networks,  with  two  key  struc-\ntural  differences:  (1)  they  have  a  special  structure  that  takes\nadvantage  of  the  unique  properties  of  image  data,  including\nlocal  receptive  fields,  since  image  data  within  local  spatial\nregions is likely to be related, and (2) weights are shared across\nreceptive fields, since the absolute position within an image is\ntypically not important to an object’s identity. Moreover, these\nnetworks are typically much deeper than traditional networks,\noften  with  a  dozen  or  more  layers  [18].  CNNs  have  been\ndemonstrated  as  a  powerful  class  of  models  in  the  computer\nvision field, beating state-of-the-art results on many tasks such\nas object detection, image segmentation and object recognition","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":2,"lines":{"from":28,"to":42}}}}],["d49fb9fc-d954-46dd-afe4-91f42189cf20",{"pageContent":"demonstrated  as  a  powerful  class  of  models  in  the  computer\nvision field, beating state-of-the-art results on many tasks such\nas object detection, image segmentation and object recognition\n[13]–[15].\nRecent  work  in  robotics  has  applied  these  deep  learning\ntechniques to object manipulation [19], hand gesture recogni-\ntion for Human-Robot Interaction [20], and detecting robotic\ngrasps [21]. These studies show the potential promise of apply-\ning deep learning to robotics. However, it is often difficult to\napply recent computer vision technologies directly to robotics\nbecause  most  work  with  recognition  in  the  computer  vision\ncommunity  does  not  consider  hardware  limitation  or  power\nrequirements as an important factors (since most applications\nare  focused  on  batch-mode  processing  of  large  image  and\nvideo  collections  like  social  media).  In  our  work  we  explore\nusing  cloud  computing  to  bring  near  real-time  performance","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":2,"lines":{"from":40,"to":55}}}}],["eee32626-882f-4be1-8e7a-58edea6c35fc",{"pageContent":"are  focused  on  batch-mode  processing  of  large  image  and\nvideo  collections  like  social  media).  In  our  work  we  explore\nusing  cloud  computing  to  bring  near  real-time  performance\nFig.  2.We  use  the  Parrot  AR.Drone2.0  as  our  hardware  platform  (top),\nadding  a  mirror  to  the  front-facing  camera  in  order  to  detect  objects  on  the\nground (bottom).\nto  robotics  applications,  without  having  to  compromise  on\naccuracy or the number of object classes that can be detected.\nB. Cloud Robotics\nSince James Kuffner introduced the term “Cloud Robotics”\nin  2010,  numerous  studies  have  explored  the  benefits  of\nthis approach [22], [23]. Cloud computing allows on-demand\naccess  to  nearly  unlimited  computational  resources,  which\nis  especially  useful  for  bursty  computational  workloads  that\nperiodically  require  huge  amounts  of  computation.  Although\nthe idea of taking advantage of remote computers in robotics","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":2,"lines":{"from":53,"to":68}}}}],["1859eb3c-68c6-4486-bfaf-f515e0a59aae",{"pageContent":"is  especially  useful  for  bursty  computational  workloads  that\nperiodically  require  huge  amounts  of  computation.  Although\nthe idea of taking advantage of remote computers in robotics\nis not new, the unparalleled scale and accessibility of modern\nclouds has opened up many otherwise unrealistic applications\nfor mobile robot systems. For example, automated self-driving\ncars  can  access  large-scale  image  and  map  data  through  the\ncloud without having to store or process this data locally [22].\nCloud-based infrastructures can also allow robots to commu-\nnicate and collaborate with one another, as in the RoboEarth\nproject [24].\nHowever, a key challenge in using remote cloud resources,\nand  especially  commodity  cloud  facilities  like  Amazon  Web\nServices, is that they introduce a number of variables that are\nbeyond the control of the robot system. Communicating with a\nremote cloud typically introduces unpredictable network delay,","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":2,"lines":{"from":66,"to":81}}}}],["30b44125-39d1-4f0c-a98d-764f13385bab",{"pageContent":"Services, is that they introduce a number of variables that are\nbeyond the control of the robot system. Communicating with a\nremote cloud typically introduces unpredictable network delay,\nand  the  cloud  computation  time  itself  may  depend  on  which\ncompute  resources  are  available  and  how  many  other  jobs\nare  running  on  the  system  at  any  given  moment  in  time.\nThis  means  that  although  the  cloud  may  deliver  near  real-\ntime performance in the average case, latencies may be quite\nhigh  at  times,  such  that  onboard  processing  is  still  needed\nfor  critical  tasks  like  stability  control.  Here  we  move  target\nrecognition  to  the  cloud,  while  keeping  low-level  detection,\n37","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":2,"lines":{"from":79,"to":90}}}}],["aa6a6e2f-94f0-47f0-a7a5-fa803494dd83",{"pageContent":"short-term  navigation  and  stability  control  local.  This  hybrid\napproach allows a low-cost quadcopter to recognize hundreds\nof objects in near real-time on average, with limited negative\nconsequences when the real-time target cannot be met.\nC. Objectness Estimation\nWhile  modern  object  recognition  may  be  too  resource-\nintensive to run on a lightweight drone, it is also unrealistic to\ntransfer all imagery to a remote cloud due to bandwidth limita-\ntions. Instead, we propose locally running a single, lightweight\n“triage”  object  detector  identifies  images  and  image  regions\nthat are likely to contain  some object of interest, which then\ncan be identified by a more computationally-intensive, cloud-\nbased  algorithm.  To  do  this,  we  evaluate‘objectness’[25],\nwhich is measure of how likely a certain window of an image\ncontains  an  object  of  any  class.  Most  recent  object  detectors\nin  the  computer  vision  field  use  one  of  the  objectness  esti-","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":3,"lines":{"from":1,"to":16}}}}],["8246dff4-ee4b-473a-8f32-47b3d35a83ad",{"pageContent":"contains  an  object  of  any  class.  Most  recent  object  detectors\nin  the  computer  vision  field  use  one  of  the  objectness  esti-\nmation  techniques  (or  object  proposal  methods)  for  reducing\ncomputation instead of using brute-force sliding windows that\nrun detectors at every possible image location [13], [26].\nSeveral  object  proposal  methods  have  been  recently  pro-\nposed,  each  with  strengths  and  weaknesses  [27].  We  apply\nthe Binarized Normed Gradients (BING) algorithm to measure\nobjectness on input frames as a first step process in our hybrid\nobject detection system [28]. While it is not the most accurate\ntechnique  available  [27],  it  is  one  of  the  simplest  and  fastest\nproposal methods (1 ms / image on a CPU), and thus can run\nin real-time on local machine.\nIII.  H\nARDWARE PLATFORM\nWe  use  a  Parrot  AR.Drone  2.0  as  a  low-cost  hardware\nplatform  [29]  to  test  our  cloud-based  recognition  approach.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":3,"lines":{"from":15,"to":31}}}}],["1c532b86-752b-4788-8d13-5c12015861e1",{"pageContent":"in real-time on local machine.\nIII.  H\nARDWARE PLATFORM\nWe  use  a  Parrot  AR.Drone  2.0  as  a  low-cost  hardware\nplatform  [29]  to  test  our  cloud-based  recognition  approach.\nThe AR.Drone costs about US$300, is small and lightweight\n(about 50cm×50cm and 420g including the battery), and can\nbe operated both indoors and outdoors.\nA. Hardware Specifications\nThe AR.Drone 2.0 is equipped with two cameras, an Inertial\nMeasurement Unit (IMU) including a 3-axis gyroscope, 3-axis\naccelerometer,  and  3-axis  magnetometer,  and  pressure-  and\nultrasound-based altitude sensors. The front-facing camera has\na resolution of 1280×720 at 30fps with a diagonal field of\nview of 92\n◦\n, and the lower-resolution downward-facing camera\nhas a resolution of 320×240 at 60fps with a diagonal field\nof  view  of  64\n◦\n.  We  use  both  cameras,  although  we  can  only\ncapture images from one of the two cameras at the same time\ndue to firmware limitations.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":3,"lines":{"from":27,"to":49}}}}],["a2dcd84c-dece-4ead-bee7-ae4f6155e7b5",{"pageContent":"of  view  of  64\n◦\n.  We  use  both  cameras,  although  we  can  only\ncapture images from one of the two cameras at the same time\ndue to firmware limitations.\nBecause the front-facing camera has a higher resolution and\nwider field of view than the downward-facing one, we use the\nfront-facing  camera  for  object  detection.  To  allow  the  drone\nto see objects on the ground, which is needed for most UAV\napplications like search and rescue, we mounted a mirror at a\n45\n◦\nangle to the front camera (see Fig. 2).\nCloud Server\nSend detected \nimages by BING\nLocation of target object\nCloud Computing\nObject Detection with R-CNNs\nLocal Machine\nPosition Estimation\nObjectness Estimation with BING\nPID Control\nWireless / LAN\nObject?\nYe s\nInput Video\n(320 x 240 @ 60 fps)\nObjectness Estimation\nwith BING\nTa k e   a   \nhigh resolution image\n(1280 x 720 with position)\nObject Detection\nwith R-CNNs\nFig. 3.System Overview: Our approach consists of four main components:","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":3,"lines":{"from":45,"to":80}}}}],["55e8f435-b015-4d6f-ad06-75ebfee42817",{"pageContent":"Objectness Estimation\nwith BING\nTa k e   a   \nhigh resolution image\n(1280 x 720 with position)\nObject Detection\nwith R-CNNs\nFig. 3.System Overview: Our approach consists of four main components:\nBING based objectness estimation, a position estimation for localization, PID\ncontrol for navigation, and R-CNNs based object detection. All components\nare  implemented  under  the  ROS  framework,  so  each  component  can  com-\nmunicate  with  every  other  via  the  ROS  network  protocol  (top).  Given  input\nvideo,  the  local  machine  detects  generic  objects  in  every  frame  with  BING,\nthen  takes  a  high  resolution  image  and  sends  it  to  the  cloud  server  if  the\nframe  contains  generic  objects.  The  cloud  server  then  runs  R-CNNs  based\nobject detection to find a target object (bottom).\nB. Embedded Software\nThe  AR.Drone  2.0  comes  equipped  with  a  1  GHz  ARM\nCortex-A8  as  the  CPU  and  an  embedded  version  of  Linux","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":3,"lines":{"from":73,"to":91}}}}],["f6f8da93-6ae1-4405-8bf9-5ed837f85373",{"pageContent":"object detection to find a target object (bottom).\nB. Embedded Software\nThe  AR.Drone  2.0  comes  equipped  with  a  1  GHz  ARM\nCortex-A8  as  the  CPU  and  an  embedded  version  of  Linux\nas its operating system. The embedded software on the board\nmeasures horizontal velocity of the drone using its downward-\nfacing camera and estimates the state of the drone such as roll,\npitch, yaw and altitude using available sensor information. The\nhorizontal velocity is measured based on two complementary\ncomputer  vision  features,  one  based  on  optical  flow  and  the\nother based on tracking image features (like corners), with the\nquality of the speed estimates highly dependent on the texture\nin the input video streams [29]. All sensor measurements are\nupdated at 200Hz. The AR.Drone 2.0 can communicate with\nother devices like smartphones or laptops over a standard WiFi\nnetwork.\nIV.  A\nPPROACH\nA. System Overview\nOur  approach  consists  of  four  main  components  shown  at","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":3,"lines":{"from":88,"to":107}}}}],["4c7823d6-bed9-4c9f-ba96-0e466d00c75d",{"pageContent":"other devices like smartphones or laptops over a standard WiFi\nnetwork.\nIV.  A\nPPROACH\nA. System Overview\nOur  approach  consists  of  four  main  components  shown  at\ntop in Fig. 3. Each component is implemented as a node in the\nRobot  Operating  System  (ROS),  allowing  it  to  communicate\nwith  others  using  the  ROS  transport  protocol  [30].  Three\ncomponents,  the  objectness  estimator,  the  position  estimator\nand PID controller, are run on a laptop (with an Intel Core i7\nProcessor running at 2.4 GHz), connected to the drone through\nthe AR.Drone device driver package of ROS, over a WiFi link.\nThe  drone  is  controlled  by  the  control  commands  with  four\nparameters, the rollΦ, the pitchΘ, the vertical speedz, and\nthe yawΨ. The most computationally demanding component,\nthe  R-CNN-based  object  detection  node,  runs  on  a  remote\ncloud  computing  server  that  the  laptop  connects  to  via  the\nopen Internet.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":3,"lines":{"from":102,"to":120}}}}],["c672b0d5-ce2a-48e1-bc54-b882d3e0e68b",{"pageContent":"the  R-CNN-based  object  detection  node,  runs  on  a  remote\ncloud  computing  server  that  the  laptop  connects  to  via  the\nopen Internet.\nThe bottom of Fig. 3 shows the pipeline of image processing\nin  our  hybrid  approach.  The  drone  takes  off  and  starts  to\n38","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":3,"lines":{"from":118,"to":123}}}}],["d27c12ac-7bb6-4c9c-8be9-f0c96ae51be3",{"pageContent":"search  for  target  objects  with  the  downward-facing  camera.\nGiven  input  video  taken  from  this  downward-facing  camera,\nthe  objectness  estimator  node  runs  the  BING  algorithm  to\ndetect  generic  objects  on  every  frame,  and  then  takes  a  high\nresolution  image  with  the  front-facing  camera  if  it  detects\ncandidate  objects  in  the  frame  [28].  Consequently,  only  the\n“interesting”  images  that  have  a  high  likelihood  to  contain\nobjects are sent to the cloud server, where the R-CNN-based\nobject detection node is run to recognize the target objects in\nthe environment.\nB. Position Estimation and PID Controller for Navigation\nWe  employ  an  Extended  Kalman  Filter  (EKF)  to  estimate\nthe  current  position  of  the  drone  from  all  available  sensing\ndata. We use a visual marker detection library, ArtoolkitPlus,\nin  our  update  step  in  order  to  get  accurate  and  robust  abso-","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":4,"lines":{"from":1,"to":15}}}}],["d9f17668-d67c-4edf-b601-0eb304bf9a8e",{"pageContent":"data. We use a visual marker detection library, ArtoolkitPlus,\nin  our  update  step  in  order  to  get  accurate  and  robust  abso-\nlution  position  estimation  results  within  the  test  environment\n[31].  (It  would  be  more  realistic  if  the  drone  estimated  its\ncurrent  position  without  these  artificial  markers,  but  position\nestimation  is  not  the  focus  of  this  paper  so  we  made  this\nsimplification here.)\nFurthermore, since our test environment is free of obstruc-\ntions,  we  assume  that  the  drone  can  move  without  changing\naltitude while it is exploring the environment to look for target\nobjects. This is a strong assumption but again is reasonable for\nthe purposes of this paper, and it makes the position estimation\nproblem much easier because this assumption reduces the state\nspace from 3D to 2D. Note that this assumption does not mean\nthat the drone never changes its altitude — in fact, it can and","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":4,"lines":{"from":14,"to":28}}}}],["7b27a7a7-f44f-4352-a544-3b6b4b2a00a2",{"pageContent":"problem much easier because this assumption reduces the state\nspace from 3D to 2D. Note that this assumption does not mean\nthat the drone never changes its altitude — in fact, it can and\ndoes  change  altitude  to  get  a  closer  view  of  objects,  when\nneeded, but it does so in hovering mode and returns back to the\ncanonical altitude before flying elsewhere in the environment.\nIn  order  to  generate  the  control  commands  that  drive  the\ndrone towards its desired goal locations, we employ a standard\nPID controller. Thus, the PID controller generates the control\ncommands  to  drive  the  drone  according  the  computed  error\nvalues,  and  finally,  the  drone  changes  operation  mode  to\nhovering mode when the drone reaches within a small distance\nof the desired goal position.\nC. Objectness Estimation with BING\nThe  quadrocopter  starts  its  object  detection  mission  with\nthe downward-facing camera, which takes video at 60fps with","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":4,"lines":{"from":26,"to":41}}}}],["955ac5e0-4a04-48f8-96a9-ad816d6f4bb8",{"pageContent":"of the desired goal position.\nC. Objectness Estimation with BING\nThe  quadrocopter  starts  its  object  detection  mission  with\nthe downward-facing camera, which takes video at 60fps with\n320×240 image resolution. Given this video input, the local\nobjectness estimation node decides whether the current input\nframe  contains  a  potential  object  of  interest.  We  apply  the\nBinarized  Normed  Gradients  (BING)  algorithm  to  measure\nthis objectness on every input frame [28].\nWe trained the BING parameters on the Pascal VOC 2012\ndataset [16], and used the average score of the top 10 bounding\nboxes  for  making  a  decision.  In  order  to  set  a  decision\nthreshold  for  our  approach,  we  collected  background  images\nhaving no object using our quadrocopter. Using the threshold,\nthe  object  estimator  node  measures  the  objectness  of  each\nframe,  then  takes  a  high  resolution  image  with  the  front-\nfacing camera if the score is above the threshold. Finally, the","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":4,"lines":{"from":38,"to":54}}}}],["379f6877-9be7-4cb7-8b86-062e4b0ac259",{"pageContent":"the  object  estimator  node  measures  the  objectness  of  each\nframe,  then  takes  a  high  resolution  image  with  the  front-\nfacing camera if the score is above the threshold. Finally, the\nFig. 4.    An example of R-CNNs-based object detection with an image taken\nby our drone.\nnode  sends  the  images  to  the  cloud  server  with  its  position\ninformation.\nD. Cloud-based R-CNNs for Object Detection\nAfter receiving an image of a candidate object, we apply the\nFaster R-CNN algorithm for object detection [17]. R-CNNs are\na  leading  approach  for  object  detection  that  combines  a  fast\nobject  proposal  mechanism  with  CNN-based  classifiers  [13],\nand Faster R-CNN is a follow-up approach by the same authors\nthat  increases  accuracy  while  reducing  the  running  time  of\nthe  algorithm.  Very  briefly,  the  technique  runs  a  lightweight,\nunsupervised  hierarchical  segmentation  algorithm  on  an  im-\nage,  breaking  the  image  into  many  (hundreds  or  thousands","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":4,"lines":{"from":52,"to":68}}}}],["18e65fdd-d57f-411c-be97-692094f66e23",{"pageContent":"unsupervised  hierarchical  segmentation  algorithm  on  an  im-\nage,  breaking  the  image  into  many  (hundreds  or  thousands\nof)  overlapping  windows  that  seem  to  contain  “interesting”\nimage content that may correspond to an object, and then each\nof  these  windows  is  classified  separately  using  a  CNN.  R-\nCNNs have shown leading performance in datasets for object\ndetection  challenges,  but  these  images  are  usually  collected\nfrom social media (e.g. Flickr), and to our knowledge, have not\nbeen applied to robotic applications. The main reason for this\nis probably that CNNs demand very high computational power,\ntypically  in  the  form  of  high-end  GPUs,  even  though  their\nrecent  approach  only  requires  around  200  ms  for  processing\nper image with GPUs. We therefore move the R-CNNs based\nobject detection part to a cloud system.\nBesides  the  computational  cost,  another  major  challenge","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":4,"lines":{"from":67,"to":81}}}}],["16afe8b4-cf9f-4ec4-a85c-257565925219",{"pageContent":"per image with GPUs. We therefore move the R-CNNs based\nobject detection part to a cloud system.\nBesides  the  computational  cost,  another  major  challenge\nwith  using  CNNs  is  their  need  for  very  large-scale  training\ndatasets,  typically  in  the  hundreds  of  thousands  or  millions\nof  images.  Because  it  is  unrealistic  for  us  to  capture  this\nscale  dataset  for  our  application,  we  used  R-CNN  models\ntrained for the 200 object types of the ImageNet Large Scale\nVisual  Recognition  Challenge  (ILSVRC13)  dataset  [16].  A\ndisadvantage of this approach is that the training images were\nmostly collected from sources like Google Images and Flickr,\nand thus are largely consumer images and not the aerial-type\nimages seen by our drone. We could likely achieve much better\nrecognition  accuracies  by  training  on  a  more  representative\ndataset; one option for future work is to take a hybrid approach","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":4,"lines":{"from":79,"to":93}}}}],["7886c202-33f2-439d-8c12-d14c311cb7e2",{"pageContent":"images seen by our drone. We could likely achieve much better\nrecognition  accuracies  by  training  on  a  more  representative\ndataset; one option for future work is to take a hybrid approach\nthat  uses  the  ILSVRC13  data  to  bootstrap  a  classifier  fine-\ntuned  for  our  aerial  images.  Nevertheless,  our  approach  has\n39","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":4,"lines":{"from":91,"to":96}}}}],["48215d96-6784-4100-b763-fcf026e1aa30",{"pageContent":"TABLE I\nO\nBJECT DETECTION RESULTS ON OUR AERIAL IMAGES COLLECTED BY THE DRONE.\nMethodaero\nbikebirdboatbottlebuscarcatchaircowtabledoghorsembikepersonplantsheepsofatraintv\nmAP\nF\nast YOLO\n87.584.6\n0.050.065.5100.087.980.092.347.160.075.088.9100.076.6100.054.5100.066.784.6\n78.3\nY\nOLO\n60.9\n88.280.080.092.3100.087.2100.070.450.040.081.077.893.488.7100.045.2100.081.879.2\n79.4\nSSD300\n60.094.120.0100.090.0100.0\n100.0   100.075.047.850.066.781.392.992.9    100.066.7100.085.782.6\n81.6\nSSD500\n66.7\n88.250.088.9100.092.993.2100.072.165    85.769.688.993.881.7100.066.789.566.7100.0\n82.6\nF\naster R-CNN\n70.6\n93.883.385.791.992.989.7100.087.262.5-77.3100.093.881.766.772.7    100.0   100.062.5\n83.9\nthe advantage of giving our robot the ability to detect several\nhundred  types  of  objects  “for  free,”  without  much  additional\ninvestment  in  dataset  collection.  We  use  the  Faster  R-CNN\nimplementation in Caffe [32], a C++ deep learning framework\nlibrary.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":5,"lines":{"from":1,"to":34}}}}],["46ef62b3-2b2c-4043-872f-63ce2b1835ad",{"pageContent":"investment  in  dataset  collection.  We  use  the  Faster  R-CNN\nimplementation in Caffe [32], a C++ deep learning framework\nlibrary.\nAn  example  of  our  detection  results  with  an  image  taken\nby  the  drone  is  shown  in  Fig.  4.  Here,  the  numbers  above\nthe  box  are  the  confidence  scores  of  detected  object,  with\ngreater score meaning greater confidence. The drone detected\nfour  different  types  of  objects  correctly,  even  though  one\nobject,  a  computer  mouse,  has  a  relatively  low  confidence.\nHowever,  an  advantage  of  robotic  applications  is  that  when\nsuch uncertainty is detected, the drone can choose to approach\nthe  computer  mouse  and  take  more  pictures  from  different\nangles  and  distances,  in  order  to  confirm  the  detection.  For\nexample, if a detection score decreases while approaching the\nobject  and  falls  under  some  threshold,  the  drone  can  decide\nthat the object is not the target.\nV.   E\nXPERIMENTALRESULTS","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":5,"lines":{"from":32,"to":49}}}}],["326dc970-2d94-4a10-b650-cc46fb74c2af",{"pageContent":"example, if a detection score decreases while approaching the\nobject  and  falls  under  some  threshold,  the  drone  can  decide\nthat the object is not the target.\nV.   E\nXPERIMENTALRESULTS\nWe  conducted  three  sets  of  experiments  to  demonstrate\nthat  our  approach  performs  successfully  in  a  realistic  but\ncontrolled  environment.  In  the  first  set  of  experiments,  we\nfocus  on  testing  the  accuracy  of  recent  deep  network  based\nobject  detectors  with  aerial  images  taken  by  the  drone,  and\nspecifically the viability of our idea of applying object models\ntrained  on  consumer  images  (from  ImageNet)  to  a  robot\napplication. In the second set of experiments, we evaluate the\nspeed of our cloud based object detection approach, comparing\nwith  running  time  of  the  fastest  deep  learning  based  object\ndetector  on  a  local  laptop.  Finally,  we  verify  our  approach\nwith  the  scenario  of  a  drone  searching  for  a  target  object  in","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":5,"lines":{"from":45,"to":61}}}}],["c39d007c-3889-484a-9d0e-00dc16abef0f",{"pageContent":"detector  on  a  local  laptop.  Finally,  we  verify  our  approach\nwith  the  scenario  of  a  drone  searching  for  a  target  object  in\nan  indoor  environment,  as  a  simple  simulation  of  a  search-\nand-rescue or surveillance application.\nThe  first  two  sets  of  experiments  were  conducted  on  our\naerial  image  dataset  and  the  last  experiment  was  conducted\nin  an  indoor  room  of  about3m×3m.  We  did  not  make\nany attempt to control for illumination or background clutter,\nalthough  the  illumination  was  fixed  (overhead  fluorescent\nlighting)  and  the  background  was  largely  composed  of  the\nnavigation markers mentioned above.\nA. Object Detection Accuracy\nWe  first  compared  the  ability  of  Faster  R-CNNs  with  two\nrecent state-of-the-are object detectors (YOLO [33] and SSD\n[34]) to recognize aerial images taken by the drone. YOLO and\nSSD  are  approaches  that  are  designed  to  speed  up  classifier-","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":5,"lines":{"from":60,"to":75}}}}],["2af0ade8-90f7-4ccd-b7a9-7667c953db84",{"pageContent":"recent state-of-the-are object detectors (YOLO [33] and SSD\n[34]) to recognize aerial images taken by the drone. YOLO and\nSSD  are  approaches  that  are  designed  to  speed  up  classifier-\nbased  object  detection  systems  through  eliminating  the  most\ncomputationally demanding part (generating region proposals\nand computing CNN features for each region). Both methods\nshowed  accurate  mean  average  precision  (mAP)  on  Pascal\nVOC 2007 dataset (YOLO: 69.0% vs. SSD300: 74.3%) with\nreal-time performance (faster than 30 FPS) on GPU.\nTo  make  a  fair  comparison,  we  used  models  that  were  all\npre-trained on the same dataset (Pascal VOC 2007 and Pascal\nVOC  2012).  We  collected  294  aerial  images  of  20  object\nclasses and annotated 578 objects in the images. The images\nhad  the  same  object  classes  as  the  Pascal  VOC  2007  dataset\nand  were  collected  from  two  sources  (some  of  them  taken\nby  ourselves  and  the  others  were  collected  from  31  publicly","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":5,"lines":{"from":73,"to":88}}}}],["2e7c27a8-67d2-4a83-96cd-4329e4a2a97a",{"pageContent":"and  were  collected  from  two  sources  (some  of  them  taken\nby  ourselves  and  the  others  were  collected  from  31  publicly\navailable  Youtube  videos  taken  by  the  same  drone  as  ours).\nTable    I  shows  average  precision  of  each  algorithm  on  this\ndataset. Here, the SSD300 model and SSD500 model have the\nsame  architecture  and  the  only  difference  is  the  input  image\nsize  (300×300  pixels  vs.  500×500  pixels).  YOLO  and\nFast YOLO also use similar architectures except Fast YOLO\nuses fewer convolutional layers (24 convolutional layers vs. 9\nconvolutional layers for Fast YOLO).\nOn this dataset, Faster R-CNN acheived 83.9% mAP com-\npared  to  YOLO  models  (78.3%  and  79.4%)  and  two  SSD\nmodels (81.6% and 82.6%). All models achieved higher mAP\non  our  aerial  image  dataset  than  their  detection  results  on\nPascal  VOC2007  since  images  of  some  object  classes  such\nas cats and plants are very distinctive with clean backgrounds.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":5,"lines":{"from":87,"to":102}}}}],["140eb4d4-8ba9-4d42-b02f-b5f8147332d5",{"pageContent":"on  our  aerial  image  dataset  than  their  detection  results  on\nPascal  VOC2007  since  images  of  some  object  classes  such\nas cats and plants are very distinctive with clean backgrounds.\nThe  first  row  of  Fig.  8  shows  these  “easy”  images  on  this\ndataset,  and  the  second  row  presents  some  “hard”  examples\nwhich were taken at high altitude.\nAs  discussed  above,  we  applied  Faster  R-CNN  trained  on\nImageNet  consumer  images  and  fine-tuned  with  Pascal  VOC\ndataset  to  our  drone  scenario.  This  time,  we  did  not  limit\nthe  objects  to  those  20  object  categories  of  VOC  2007,  but\ninstead  we  looked  at  the  results  among  the  200  categories\nFaster R-CNN provided. We did this though the aerial drone\nimages look nothing like most consumer images, because we\ndid not have the large-scale dataset needed to train a CNN from\nscratch.  This  can  be  thought  of  as  a  simple  case  of  transfer","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":5,"lines":{"from":100,"to":114}}}}],["b100ff06-febc-4a90-8481-69b421b8f169",{"pageContent":"images look nothing like most consumer images, because we\ndid not have the large-scale dataset needed to train a CNN from\nscratch.  This  can  be  thought  of  as  a  simple  case  of  transfer\nlearning, and likely suffers from the usual mismatch problem\nwhen training sets and testing sets are sampled from different\ndistributions. We took other 74 images like bottom two rows\nof Fig. 8, and achieved 63.5% of accuracy.\nB. Recognition Speed on Cloud System\nOur  second  set  of  experiments  evaluated  the  running  time\nperformance of the CNN-based object recognition, testing the\nextent  to  which  cloud  computing  could  improve  recognition\ntimes, and the variability of cloud-based recognition times due\nto unpredictable communication times. For these experiments\nwe  used  the  same  set  of  images  and  objects  collected  in\n40","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":5,"lines":{"from":112,"to":126}}}}],["420ab7c5-eb0f-4ea5-8478-6cbab6ddd93e",{"pageContent":"78798081828384\n0\n10\n20\n30\n40\n50\n60\nObject Detection on Aerial Images\nmean average precision (mAP) in %\nframes per second (FPS)\nFast YOLO\nYOLO\nSSD300\nSSD500\nFaster R−CNN\nFig. 5.    A running time comparison of recent state-of-the-art object detectors\non our aerial images.\nthe  previous  section,  and  compared  the  speed  of  each  algo-\nrithm  using  Graphics  Processing  Unit  (GPU)  on  a  simulated\ncloud machine at first. We measured the running time includ-\ning  image  loading,  pre-processing,  and  output  parsing  (post-\nprocessing) time, since those times are important in real-time\napplications.\nFig.  5  shows  the  running  time  of  each  algorithm  as  a\nfunction  of  its  accuracy.  Even  though  all  recent  state-of-the-\nart methods showed reasonable speed with high-accuracy, for\ninstance, SSD 300 models showed 6.55 FPS with mAP 81.6,\nthe  result  shows  detection  speed  and  accuracy  are  still  in\ninverse  related.  Fast  YOLO  showed  the  highest  speed  (57.4","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":6,"lines":{"from":1,"to":30}}}}],["a4cafcab-3b0a-4034-bc57-d46e26771d74",{"pageContent":"instance, SSD 300 models showed 6.55 FPS with mAP 81.6,\nthe  result  shows  detection  speed  and  accuracy  are  still  in\ninverse  related.  Fast  YOLO  showed  the  highest  speed  (57.4\nFPS)  with  the  lowest  accuracy  (mAP  78.3),  while  Faster\nR-CNN  had  the  lowest  speed  (3.48  FPS)  with  the  highest\naccuracy (mAP 83.9).\nIn  the  second  experiment,  we  thus  compared  Fast  YOLO\non a local laptop versus Faster R-CNN on a remote server as\na simulated cloud. A comparison of these computing facilities\nare shown in Table II. Fig. 6 shows the running time of Fast\nYOLO and Faster R-CNN on the two different machines.\nThe  average  running  time  of  Fast  YOLO  on  the  local\nmachine was 7.31 seconds per image, while the average time\non the cloud machine based Faster R-CNN was 1.29 seconds,\nincluding   latencies   for   sending   each   image   to   the   cloud\ncomputer (which averaged about 600ms), and for exchanging","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":6,"lines":{"from":28,"to":43}}}}],["f02d7503-f6cf-45c5-9d79-6111875ab91b",{"pageContent":"on the cloud machine based Faster R-CNN was 1.29 seconds,\nincluding   latencies   for   sending   each   image   to   the   cloud\ncomputer (which averaged about 600ms), and for exchanging\ndetected results and other command messages (which averaged\n0.41ms).  Thus  the  cloud-based  recognition  performed  about\n5.7  times  faster  than  the  local  Fast  YOLO  on  average.  The\naverage  running  time  on  our  single-server  simulated  cloud\nis  not  fast  enough  to  be  considered  real  time,  but  is  still\nfast  enough  to  be  useful  in  many  applications.  Moreover,\nrecognition could be easily made faster by parallelizing object\nmodel evaluations across different machines.\nFast YOLO on LocalFaster R−CNN on Cloud\n0\n1\n2\n3\n4\n5\n6\n7\n8\nobject detecion time (seconds)\nRunning time of object detection on each machine\nsending an image to cloud\ncommunication latency\nrunning time of algorithm\nFig. 6.Running time of object detection on each machine.\nC. Target Search with a Drone","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":6,"lines":{"from":41,"to":68}}}}],["dc72db94-1199-4771-9202-a7ae044c6b9f",{"pageContent":"sending an image to cloud\ncommunication latency\nrunning time of algorithm\nFig. 6.Running time of object detection on each machine.\nC. Target Search with a Drone\nIn this section, we demonstrate our approach with a simple\nscenario of the drone searching for a target object in an indoor\nenvironment.  We  assume  that  a  drone  is  supposed  to  find  a\nsingle target object in a room in a building. There are several\ndifferent types of objects in the room, but fortunately there are\nno obstacles.\nIn  the  test  scenario,  we  used  a  screwdriver  as  a  target\nobject and scattered various distractor objects on the floor in\nthe  indoor  test  room.  The  drone  started  this  object  searching\nmission  with  lower-resolution  downward-facing  camera,  and\nran the BING algorithm for finding generic objects given the\ninput  video.  At  the  same  time,  the  position  estimator  node\nestimated  the  drone’s  position  continuously.  When  the  drone","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":6,"lines":{"from":64,"to":81}}}}],["92ed1150-1567-437c-b17d-63af3b6452f7",{"pageContent":"ran the BING algorithm for finding generic objects given the\ninput  video.  At  the  same  time,  the  position  estimator  node\nestimated  the  drone’s  position  continuously.  When  the  drone\nfound any “interesting” objects on the floor, it switched to the\nfront-facing  camera  to  capture  a  photo  at  a  higher  resolution\nand with a wider angle, then took picture of the candidate area\nand sends it to the cloud system (t=3sandt= 8 s). Then,\nthe  drone  switched  the  camera  back  to  the  downward-facing\ncamera  for  localization  and  stability  control,  and  proceeded\nto  the  other  candidate  positions.  In  the  meantime,  the  cloud\nsystem  performed  recognition  then  sent  results  to  the  drone.\nThe  drone  performed  the  same  steps  until  it  found  a  target\nobject, at which point the mission was completed (t=17s).\nTABLE II\nH\nARDWARECOMPARISON BETWEENLOCAL ANDCLOUDMACHINE\nlocal computercloud computer\nCPUs\none Intel Core\ni7-4700HQ @ 2.4GHz\ntwo Intel Xeon","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":6,"lines":{"from":79,"to":99}}}}],["585a26aa-57eb-4730-9677-6a97b430b9a8",{"pageContent":"object, at which point the mission was completed (t=17s).\nTABLE II\nH\nARDWARECOMPARISON BETWEENLOCAL ANDCLOUDMACHINE\nlocal computercloud computer\nCPUs\none Intel Core\ni7-4700HQ @ 2.4GHz\ntwo Intel Xeon\nE5-2680 v3 @ 2.5GHz\nGPUs\none Nvidia\nGeForce GTX 770M\ntwo Nvidia Tesla K40\nRAM\n16 GB128 GB\n41","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":6,"lines":{"from":91,"to":107}}}}],["9a3f5354-fdeb-407e-8b7c-cc7c126b524b",{"pageContent":"t=0st=3st=8st=13st=17s\nFig. 7.Target Search with a Drone: First rows show movements of the drone during the experiment, and second and third rows indicate detection results\nfrom BING and R-CNNs respectively. Att= 0 s the drone started to search for a target object and did not find generic objects with BING. Att=3s,t=\n8 s, the drone found generic objects with BING, thus took high resolution pictures and sent them to cloud server. However, R-CNNs did not detect a target\nobject in those images. Att= 17 s, the drone found generic objects again, thus it took the high resolution picture and sent it to cloud server. Then, finally\nR-CNNs based object detector found a target object.\nFig. 7 shows a sequence of images taken during the drone’s\nsearch  for  a  target  object  in  our  test  scenario.  It  shows  that\nthe  drone  only  took  pictures  and  sent  them  when  there  were\n“interesting” objects on the floor, and finally found the target","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":7,"lines":{"from":1,"to":10}}}}],["e5516ac5-e2b7-49ec-ab83-dc9ad7a6e585",{"pageContent":"the  drone  only  took  pictures  and  sent  them  when  there  were\n“interesting” objects on the floor, and finally found the target\nobject,  a  screwdriver,  with  the  cloud-based  R-CNNs  object\ndetector.\nVI.  C\nONCLUSION\nIn this paper, we proposed to use Convolutional Neural Net-\nworks to allow UAVs to detect hundreds of object categories.\nCNNs are computationally expensive, however, so we explore\nthe  hybrid  approach  that  moving  the  recognition  to  a  remote\ncomputing  cloud  while  keeping  low-level  object  detection\nand short-term navigation onboard. Our approach enables the\nUAVs, especially lightweight, low-cost consumer UAVs, to use\nstate-of-the-art object detection algorithms, despite their very\nlarge  computational  demands.  The  (nearly)  unlimited  cloud-\nbased  computation  resources,  however,  come  at  the  cost  of\npotentially  high  and  unpredictable  communication  lag  and\nhighly  variable  system  load.  We  tested  our  approach  with  a","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":7,"lines":{"from":9,"to":26}}}}],["447c54d0-887c-4bf3-9ed7-0e4e7bf2ca11",{"pageContent":"based  computation  resources,  however,  come  at  the  cost  of\npotentially  high  and  unpredictable  communication  lag  and\nhighly  variable  system  load.  We  tested  our  approach  with  a\nParrot  AR.Drone  2.0  as  a  low-cost  hardware  platform  in  a\nreal  indoor  environment.  The  results  suggest  that  the  cloud-\nbased  approach  could  allow  speed-ups  of  nearly  an  order  of\nmagnitude, approaching real-time performance even when de-\ntecting hundreds of object categories, despite these additional\ncommunication lags. We demonstrated our approach in terms\nof  recognition  accuracy  and  speed,  and  in  a  simple  target\nsearching scenario.\nA\nCKNOWLEDGMENT\nThe  authors  wish  to  thank  Matt  Francisco  for  helping  to\ndesign and fabricate the forward-facing camera mirror, Supun\nKamburugamuve for helping with the software interface to the\ncloud infrastructure, and Bruce Shei for configuring the cloud\nservers.\nR\nEFERENCES","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":7,"lines":{"from":24,"to":43}}}}],["23e4a8bf-19ed-44f8-80bf-fe2062a0ef45",{"pageContent":"Kamburugamuve for helping with the software interface to the\ncloud infrastructure, and Bruce Shei for configuring the cloud\nservers.\nR\nEFERENCES\n[1]   M.  Bhaskaranand  and  J.  D.  Gibson,  “Low-complexity  video  encoding\nfor  uav  reconnaissance  and  surveillance,”  inMilitary Communications\nConference (MILCOM).    IEEE, 2011, pp. 1633–1638.\n[2]   P. Doherty and P. Rudol, “A uav search and rescue scenario with human\nbody  detection  and  geolocalization,”  inAustralasian Joint Conference\non Artificial Intelligence.    Springer, 2007, pp. 1–13.\n[3]   T.  Tomic,  K.  Schmid,  P.  Lutz,  A.  Domel,  M.  Kassecker,  E.  Mair,\nI.  L.  Grixa,  F.  Ruess,  M.  Suppa,  and  D.  Burschka,  “Toward  a  fully\nautonomous uav: Research platform for indoor and outdoor urban search\nand rescue,”IEEE robotics & automation magazine, vol. 19, no. 3, pp.\n46–56, 2012.\n[4]   L. Merino, F. Caballero, J. R. Mart\n ́\nınez-de Dios, J. Ferruz, and A. Ollero,","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":7,"lines":{"from":39,"to":57}}}}],["53592b40-e417-4654-a1d2-920ab72e8238",{"pageContent":"and rescue,”IEEE robotics & automation magazine, vol. 19, no. 3, pp.\n46–56, 2012.\n[4]   L. Merino, F. Caballero, J. R. Mart\n ́\nınez-de Dios, J. Ferruz, and A. Ollero,\n“A  cooperative  perception  system  for  multiple  uavs:  Application  to\nautomatic detection of forest fires,”Journal of Field Robotics, vol. 23,\nno. 3-4, pp. 165–184, 2006.\n[5]   I. Sa, S. Hrabar, and P. Corke, “Outdoor flight testing of a pole inspection\nuav  incorporating  high-speed  vision,”  inField and Service Robotics.\nSpringer, 2015, pp. 107–121.\n[6]   T. P. Breckon, S. E. Barnes, M. L. Eichner, and K. Wahren, “Autonomous\nreal-time  vehicle  detection  from  a  medium-level  uav,”  inProc. 24th\nInternational Conference on Unmanned Air Vehicle Systems, 2009, pp.\n29–1.\n[7]   J.  Gleason,  A.  V.  Nefian,  X.  Bouyssounousse,  T.  Fong,  and  G.  Bebis,\n“Vehicle  detection  from  aerial  imagery,”  inRobotics and Automation\n(ICRA), 2011 IEEE International Conference on.IEEE,  2011,  pp.\n2065–2070.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":7,"lines":{"from":53,"to":71}}}}],["b74e15ba-693b-463c-844e-063cd2ecd026",{"pageContent":"“Vehicle  detection  from  aerial  imagery,”  inRobotics and Automation\n(ICRA), 2011 IEEE International Conference on.IEEE,  2011,  pp.\n2065–2070.\n[8]   A. Gaszczak, T. P. Breckon, and J. Han, “Real-time people and vehicle\ndetection from uav imagery,” inIS&T/SPIE Electronic Imaging.    Inter-\nnational Society for Optics and Photonics, 2011, pp. 78 780B–78 780B.\n[9]   H.  Lim  and  S.  N.  Sinha,  “Monocular  localization  of  a  moving  person\nonboard  a  quadrotor  mav,”  in2015 IEEE International Conference on\nRobotics and Automation (ICRA).    IEEE, 2015, pp. 2182–2189.\n[10]   J. Engel, J. Sturm, and D. Cremers, “Scale-aware navigation of a low-\ncost quadrocopter with a monocular camera,”Robotics and Autonomous\nSystems, vol. 62, no. 11, pp. 1646–1656, 2014.\n[11]   C. Forster, M. Faessler, F. Fontana, M. Werlberger, and D. Scaramuzza,\n“Continuous  on-board  monocular-vision-based  elevation  mapping  ap-\nplied  to  autonomous  landing  of  micro  aerial  vehicles,”  in2015 IEEE","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":7,"lines":{"from":69,"to":83}}}}],["441ddb58-8d49-4653-b248-f942537faf01",{"pageContent":"“Continuous  on-board  monocular-vision-based  elevation  mapping  ap-\nplied  to  autonomous  landing  of  micro  aerial  vehicles,”  in2015 IEEE\nInternational Conference on Robotics and Automation (ICRA).IEEE,\n2015, pp. 111–118.\n42","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":7,"lines":{"from":82,"to":86}}}}],["a76cab46-3ff7-4b7a-9fce-317a5dd78cb4",{"pageContent":"Fig. 8.Sample images collected by our drone. R-CNNs based object recognition are able to detect a wide variety of different types of objects.\n[12]   F.  S.  Leira,  T.  A.  Johansen,  and  T.  I.  Fossen,  “Automatic  detection,\nclassification  and  tracking  of  objects  in  the  ocean  surface  from  uavs\nusing a thermal camera,” in2015 IEEE Aerospace Conference.    IEEE,\n2015, pp. 1–10.\n[13]   R.   Girshick,   J.   Donahue,   T.   Darrell,   and   J.   Malik,   “Rich   feature\nhierarchies  for  accurate  object  detection  and  semantic  segmentation,”\ninProceedings of the IEEE conference on computer vision and pattern\nrecognition, 2014, pp. 580–587.\n[14]   J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Venu-\ngopalan, K. Saenko, and T. Darrell, “Long-term recurrent convolutional\nnetworks for visual recognition and description,” inProceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition,  2015,\npp. 2625–2634.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":8,"lines":{"from":1,"to":14}}}}],["40149cab-c506-42f7-b862-eb16efc7df0b",{"pageContent":"networks for visual recognition and description,” inProceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition,  2015,\npp. 2625–2634.\n[15]   D. Ciresan, A. Giusti, L. M. Gambardella, and J. Schmidhuber, “Deep\nneural  networks  segment  neuronal  membranes  in  electron  microscopy\nimages,”  inAdvances in neural information processing systems,  2012,\npp. 2843–2851.\n[16]   O.  Russakovsky,  J.  Deng,  H.  Su,  J.  Krause,  S.  Satheesh,  S.  Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernsteinet al., “Imagenet large\nscale  visual  recognition  challenge,”International Journal of Computer\nVision, vol. 115, no. 3, pp. 211–252, 2015.\n[17]   S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards real-\ntime  object  detection  with  region  proposal  networks,”  inAdvances in\nNeural Information Processing Systems (NIPS), 2015.\n[18]   Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":8,"lines":{"from":12,"to":26}}}}],["b4467e1e-6b7f-4da5-905a-af78617096ba",{"pageContent":"Neural Information Processing Systems (NIPS), 2015.\n[18]   Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning\napplied  to  document  recognition,”Proceedings of the IEEE,  vol.  86,\nno. 11, pp. 2278–2324, 1998.\n[19]   S.  Levine,  C.  Finn,  T.  Darrell,  and  P.  Abbeel,  “End-to-end  training\nof  deep  visuomotor  policies,”Journal of Machine Learning Research,\nvol. 17, no. 39, pp. 1–40, 2016.\n[20]   J.  Nagi,  A.  Giusti,  F.  Nagi,  L.  M.  Gambardella,  and  G.  A.  Di  Caro,\n“Online  feature  extraction  for  the  incremental  learning  of  gestures  in\nhuman-swarm  interaction,”  in2014 IEEE International Conference on\nRobotics and Automation (ICRA).    IEEE, 2014, pp. 3331–3338.\n[21]   I.  Lenz,  H.  Lee,  and  A.  Saxena,  “Deep  learning  for  detecting  robotic\ngrasps,”The International Journal of Robotics Research,  vol.  34,  no.\n4-5, pp. 705–724, 2015.\n[22]   K. Goldberg and B. Kehoe, “Cloud robotics and automation: A survey","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":8,"lines":{"from":25,"to":39}}}}],["ee8bf838-f7ed-456d-b92b-dfc9d1f9a42f",{"pageContent":"grasps,”The International Journal of Robotics Research,  vol.  34,  no.\n4-5, pp. 705–724, 2015.\n[22]   K. Goldberg and B. Kehoe, “Cloud robotics and automation: A survey\nof related work,”EECS Department, University of California, Berkeley,\nTech. Rep. UCB/EECS-2013-5, 2013.\n[23]   B. Kehoe, S. Patil, P. Abbeel, and K. Goldberg, “A survey of research\non  cloud  robotics  and  automation,”IEEE Transactions on Automation\nScience and Engineering, vol. 12, no. 2, pp. 398–409, 2015.\n[24]   D.  Hunziker,  M.  Gajamohan,  M.  Waibel,  and  R.  D’Andrea,  “Rapyuta:\nThe roboearth cloud engine,” inRobotics and Automation (ICRA), 2013\nIEEE International Conference on.    IEEE, 2013, pp. 438–444.\n[25]   B.  Alexe,  T.  Deselaers,  and  V.  Ferrari,  “Measuring  the  objectness  of\nimage windows,”IEEE Transactions on Pattern Analysis and Machine\nIntelligence, vol. 34, no. 11, pp. 2189–2202, 2012.\n[26]   X. Wang, M. Yang, S. Zhu, and Y. Lin, “Regionlets for generic object","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":8,"lines":{"from":37,"to":51}}}}],["45c47256-4137-475f-9b60-952a81bce917",{"pageContent":"image windows,”IEEE Transactions on Pattern Analysis and Machine\nIntelligence, vol. 34, no. 11, pp. 2189–2202, 2012.\n[26]   X. Wang, M. Yang, S. Zhu, and Y. Lin, “Regionlets for generic object\ndetection,”  inProceedings of the IEEE International Conference on\nComputer Vision, 2013, pp. 17–24.\n[27]   J.  Hosang,  R.  Benenson,  P.  Doll\n ́\nar,  and  B.  Schiele,  “What  makes  for\neffective  detection  proposals?”IEEE transactions on pattern analysis\nand machine intelligence, vol. 38, no. 4, pp. 814–830, 2016.\n[28]   M.-M.  Cheng,  Z.  Zhang,  W.-Y.  Lin,  and  P.  Torr,  “Bing:  Binarized\nnormed gradients for objectness estimation at 300fps,” inProceedings of\nthe IEEE conference on computer vision and pattern recognition, 2014,\npp. 3286–3293.\n[29]   P.-J. Bristeau, F. Callou, D. Vissiere, and N. Petit, “The navigation and\ncontrol  technology  inside  the  ar.  drone  micro  uav,”IFAC Proceedings\nVolumes\n, vol. 44, no. 1, pp. 1477–1484, 2011.\n[30]","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":8,"lines":{"from":49,"to":67}}}}],["9c682c9b-b227-4eba-b2f4-7a8a2fb6074d",{"pageContent":"control  technology  inside  the  ar.  drone  micro  uav,”IFAC Proceedings\nVolumes\n, vol. 44, no. 1, pp. 1477–1484, 2011.\n[30]\nM.   Quigley,   K.   Conley,   B.   Gerkey,   J.   Faust,   T.   Foote,   J.   Leibs,\nR.  Wheeler,  and  A.  Y.  Ng,  “ROS:  an  open-source  robot  operating\nsystem,” inICRA workshop on open source software, 2009.\n[31]   D.  Wagner  and  D.  Schmalstieg,  “Artoolkitplus  for  pose  tracking  on\nmobile devices,” inComputer Vision Winter Workshop (CVWW), 2007.\n[32]   Y.  Jia,  E.  Shelhamer,  J.  Donahue,  S.  Karayev,  J.  Long,  R.  Girshick,\nS.  Guadarrama,  and  T.  Darrell,  “Caffe:  Convolutional  architecture  for\nfast feature embedding,”arXiv preprint arXiv:1408.5093, 2014.\n[33]   J.  Redmon,  S.  Divvala,  R.  Girshick,  and  A.  Farhadi,  “You  only  look\nonce:  Unified,  real-time  object  detection,”  inThe IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2016.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":8,"lines":{"from":64,"to":78}}}}],["2cc60553-b0d6-4c33-98df-4c93b2285677",{"pageContent":"once:  Unified,  real-time  object  detection,”  inThe IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2016.\n[34]   W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C.\nBerg,  “Ssd:  Single  shot  multibox  detector,”  inroceedings of European\nConference on Computer Vision (ECCV), 2016.\n43","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\leejang_irc2017.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles","Author":"Jangwon Lee, Jingya Wang, David Crandall, Selma Šabanović, Geoffrey Fox","Subject":"2017 First IEEE International Conference on Robotic Computing","Keywords":"Robot Vision, Object Detection, Unmanned Aerial Systems, Convolutional Neural Networks","Producer":"iTextSharp 4.0.7 (based on iText 2.0.7)","CreationDate":"D:20170315094958-07'00'","ModDate":"D:20170511083112-04'00'"},"metadata":null,"totalPages":8},"loc":{"pageNumber":8,"lines":{"from":77,"to":82}}}}],["96616d37-603d-4f8a-b15a-46e946487a5d",{"pageContent":"JOURNAL OF AMRAN UNIVERSITY \nJ. Amr. Uni.  03 (2023) p.267 \n_________________________________________________ \n* \nEmail: Sharabi28@hotmail.com \n  ©Amran University Publishing \n \nReal-Time Object Detection Overview: Advancements, Challenges, and \nApplications \n \n*\nNaif Alsharabi\n1,2\n \n1\nCollege of Engineering and IT, Amran University, Amran, Yemen. \n2\nCollege of computer science and engineering, University of Ha’il, 81481, Hail, Saudi Arabia \n \nAbstract \nReal-time object detection is a crucial aspect of computer vision with applications spanning autonomous \nvehicles, surveillance, robotics, and augmented reality. This study examines real-time object detection \ntechniques, highlighting their significance in artificial intelligence. The primary goal is swift and accurate \nobject identification in images or video streams. Traditional methods like sliding windows and region-\nbased approaches had limitations in computational efficiency. Deep learning, particularly Convolutional","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":1,"lines":{"from":1,"to":25}}}}],["82e34ce3-5ccd-4982-8d4b-35fd78ccd7f6",{"pageContent":"based approaches had limitations in computational efficiency. Deep learning, particularly Convolutional \nNeural Networks (CNNs), revolutionized object detection. Models like SSD, YOLO, and Faster R-CNN \nexcel in accuracy and speed. They employ anchor boxes, feature pyramid networks, and non-maximum \nsuppression to balance precision and processing speed. Hardware accelerators like GPUs, TPUs, and \nFPGAs facilitate real-time inference. \nChallenges in real-time object detection include occlusion, scale variations, and cluttered environments. \nResearchers  must  navigate  the  trade-offs  between  accuracy  and  speed.  Real-time  object  detection  is \npivotal  in  computer  vision,  enabling  intelligent  systems  across  diverse  applications.  The  continuous \nevolution  of  deep  learning  algorithms  and  hardware  capabilities pushes  the  boundaries of  this  field, \nmaking it a dynamic research domain in artificial intelligence.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":1,"lines":{"from":25,"to":34}}}}],["595ac521-4175-4e9c-a89c-090a629a5504",{"pageContent":"evolution  of  deep  learning  algorithms  and  hardware  capabilities pushes  the  boundaries of  this  field, \nmaking it a dynamic research domain in artificial intelligence. \n \nKeyword: Object detection, Video Detection, Real time Detection, Algorithm detection. \nالملخص :يعد الكشف عن الكائنات في الزمن الحقيقي مهمة أساسية في ميدان الرؤية الحاسوبية، ويستخدم على نطاق واسع \nفي مجالات مثل السيارات ذاتية القيادة ومراقبة الأمان والروبوتات والواقع المعزز. تقدم هذه الدراسة الشاملة نظرة مفصلة على \nتقنيات الكشف في الزمن الحقيقي، مع التركيز على دورها الحاسم في مجال الذكاء الاصطناعي. يهدف الكشف في الزمن \nالحقيقي إلى التعرف على الكائنات بسرعة ودقة في الصور أو مقاطع الفيديو. بينما وضعت الأساليب التقليدية مثل \n slide window  region-based approaches andالأسس لهذا الميدان، كانت تعاني من قيود في الكفاءة الحسابية \nللتطبيقات الفعلية. ظهور التعلم العميق قاد إلى تغيير جذري في كشف الكائنات، حيث ظهرت الشبكات العصبية التحويلية","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":1,"lines":{"from":33,"to":42}}}}],["6a6983ee-69fe-477b-a095-e3fb94c43801",{"pageContent":"للتطبيقات الفعلية. ظهور التعلم العميق قاد إلى تغيير جذري في كشف الكائنات، حيث ظهرت الشبكات العصبية التحويلية    \n (CNN) كعامل مهم في أنظمة الكشف الحديثة. النماذج المعروفة مثل SSD وYOLO وFaster R-CNN  تميزت \nبدقتها وسرعتها. تستفيد هذه النهج من تقنيات مثل صناديق الربط وشبكات هرم السمات والقمع الأقصى لتحقيق توازن بين \nالدقة وسرعة المعالجة. تلعب تسريعات الأجهزة مثل GPUs وTPUs و FPGAs  ،يقيقحلا نمزلا يف فشكلا يف ا\nً\nريبك ا\nً\nرود\nحيث تمكن من تنفيذ نماذج التعلم العميق بسرعة. يتناول هذا البحث التحديات التي تواجه كشف الكائنات في الزمن الحقيقي، \nمثل التغطية وتباين الحجم والبيئات المزدحمة، بالإضافة إلى التوازن بين الدقة والسرعة الذي يجب معالجته. يظل كشف \nست .ةعونتم تاقيبطت يف ةيكذلا ةمظنلأا نيكمت هنكمي ثيح ،ةيبوساحلا ةيؤرلا لاجم يف ا\nً\nمهم يقيقحلا نمزلا يف تانئاكلاتمر \nتطورات خوارزميات التعلم العميق وقدرات الأجهزة في دفع حدود هذا المجال، مؤكدة على وضعه كمجال بحثي ديناميكي \nومتقدم في ميدان الذكاء الاصطناعي. \n \n1. Introduction","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":1,"lines":{"from":42,"to":58}}}}],["77b10869-6e18-4cc0-be47-038c96818fc3",{"pageContent":"ً\nمهم يقيقحلا نمزلا يف تانئاكلاتمر \nتطورات خوارزميات التعلم العميق وقدرات الأجهزة في دفع حدود هذا المجال، مؤكدة على وضعه كمجال بحثي ديناميكي \nومتقدم في ميدان الذكاء الاصطناعي. \n \n1. Introduction \nThe realm of real-time object detection has emerged as an indispensable facet of computer vision, \nencompassing the rapid and precise identification of objects within images and video streams [1]. \nThis  capability  finds  extensive  utility  across  diverse  applications,  including  but  not  limited  to \nautonomous vehicles, robotics, surveillance, and augmented reality [2]. The ability to promptly \ndiscern and accurately locate objects within visual data streams holds the potential to revolutionize \ndecision-making processes and enhance the efficiency of state-of-the-art technologies. Real-time \nobject detection has witnessed a transformative shift with the advent of deep learning, particularly","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":1,"lines":{"from":53,"to":65}}}}],["ea8879af-486a-4ba1-927f-65ed11837b18",{"pageContent":"decision-making processes and enhance the efficiency of state-of-the-art technologies. Real-time \nobject detection has witnessed a transformative shift with the advent of deep learning, particularly \nthe  emergence  of  Convolutional  Neural  Networks  (CNNs) [3].  These  neural  networks  have \nrevolutionized computer vision tasks by autonomously extracting intricate feature representations","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":1,"lines":{"from":64,"to":67}}}}],["075bacd0-8c73-4661-9e55-50fd2efb7d03",{"pageContent":"J. Amr. Uni. 03 (2023) p.267                                                                                                                    Naif Alsharabi  \n \n \n268 \n \nfrom raw pixel data, enabling the discernment of complex patterns and features essential for robust \nobject recognition. \nAn array of deep learning-based object detection models have come to the fore, each distinguished \nby its unique architecture and strengths [2][3][5]. Eminent examples include YOLO (You Only \nLook  Once),  SSD  (Single  Shot  Multibox Detector),  and  Faster  R-CNN  (Region-based \nConvolutional  Neural  Networks).  These  models  employ  diverse  strategies  to  strike  a  balance \nbetween  accuracy  and  speed,  catering  to  the  dynamic  requisites  of  real-time  applications. The \nprocess of real-time object detection entails a series of stages, including data collection, annotation, \nmodel training, and inference. Deep learning models are trained on extensive datasets containing","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":2,"lines":{"from":1,"to":14}}}}],["4126eedd-f977-4e00-b9a8-ac1dbbfed1a5",{"pageContent":"model training, and inference. Deep learning models are trained on extensive datasets containing \nlabeled instances of objects, enabling them to acquire the ability to accurately detect objects. During \ninference, these models expeditiously analyze live video streams or sequences of images, generating \nbounding boxes around identified objects along with corresponding class labels. The pursuit of real-\ntime capabilities necessitates the implementation of optimization techniques [5]. Strategies such as \nmodel quantization, which reduces model size while preserving performance, combined with the \nutilization of hardware acceleration through GPUs or TPUs (Tensor Processing Units), collectively \ncontribute to enhancing the processing speed and overall efficiency of real-time object detection \nsystems. \nWhile remarkable progress has been achieved, the domain of real-time object detection remains a","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":2,"lines":{"from":14,"to":23}}}}],["f6124ba6-6b2c-4f03-880d-a5890fb32e2e",{"pageContent":"systems. \nWhile remarkable progress has been achieved, the domain of real-time object detection remains a \nvibrant area of research [2, 4]. Researchers persistently explore novel algorithms, architectures, and \nenhancements  in  hardware  to  amplify  model  accuracy,  efficiency,  and  adaptability,  thereby \nestablishing more robust and practical real-time object detection systems capable of thriving in real-\nworld scenarios. \nReal-time object detection is rooted in the fundamental task of identifying and localizing objects \nwithin  images  or  video  streams.  The  objective  is  to  categorize  objects  while  delineating  their \nprecise positions through bounding boxes within visual data. The trajectory of object detection has \nwitnessed substantial evolution, driven by the rise of deep learning techniques and the availability \nof  meticulously  annotated  datasets.  Central  to  these  advancements  are  Convolutional  Neural","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":2,"lines":{"from":22,"to":32}}}}],["b4add3f7-7cd8-4cd1-94e9-e118045a2d22",{"pageContent":"of  meticulously  annotated  datasets.  Central  to  these  advancements  are  Convolutional  Neural \nNetworks (CNNs), which autonomously discern hierarchical features from data. Models such as \nYOLO (You Only Look Once) and Faster R-CNN serve as exemplars of the remarkable accuracy \nand real-time performance that have come to define object detection across diverse applications. \nThe motivation behind real-time object detection stems from the growing need for efficient and \naccurate  visual  understanding  systems  in  various  real-world  applications.  Traditional  object \ndetection  methods,  although  effective,  often  fell  short  in  handling the  challenges  of  real-time \nprocessing, which is crucial in dynamic environments where timely responses are essential. The \nemergence of deep learning-based approaches, such as YOLO (You Only Look Once) and SSD \n(Single  Shot  Multibox  Detector),  provided  a  solution  to  this  problem,  sparking  a  significant","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":2,"lines":{"from":32,"to":41}}}}],["c6d991fb-b5fd-477d-9be4-b2bba28c35a4",{"pageContent":"emergence of deep learning-based approaches, such as YOLO (You Only Look Once) and SSD \n(Single  Shot  Multibox  Detector),  provided  a  solution  to  this  problem,  sparking  a  significant \nadvancement in real-time object detection capabilities [2] \n \n2. Literature  Overview \nObject detection is a fundamental task in computer vision that involves identifying and localizing \nobjects of interest within images or video streams. The objective is to not only classify objects into \npredefined categories but also draw bounding boxes around them, pinpointing their exact locations \nin the visual data. Over the years, significant advancements have been made in object detection, \ndriven by the emergence of deep learning techniques and the availability of large annotated datasets. \nConvolutional Neural  Networks  (CNNs)  have  played  a  pivotal  role  in  revolutionizing  object \ndetection by automatically learning hierarchical features from data. Several state-of-the-art models,","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":2,"lines":{"from":40,"to":51}}}}],["16a56379-02fc-4a02-82fc-52a457298712",{"pageContent":"detection by automatically learning hierarchical features from data. Several state-of-the-art models, \nsuch as YOLO (You Only Look Once) [2] and Faster R-CNN (Region-based Convolutional Neural \nNetworks) [4], have demonstrated impressive accuracy and real-time performance. These models \nhave found applications in various fields, including autonomous vehicles, surveillance systems, \nmedical imaging, and more, showcasing the significance of object detection in enabling a wide \nrange of practical and innovative solutions.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":2,"lines":{"from":51,"to":56}}}}],["548dcdee-d904-4285-a0cc-96359ab7a983",{"pageContent":"J. Amr. Uni. 03 (2023) p.267                                                                                                                    Naif Alsharabi  \n \n \n269 \n \n2.1 Definition and Importance of Object Detection \nObject detection is a fundamental computer vision task that involves identifying and localizing \nspecific objects of interest within images or video frames. The main objective is to detect the \npresence of objects and draw bounding boxes around them, indicating their precise locations and \nextents. Additionally, object detection often includes classifying the detected objects into predefined \ncategories or classes, enabling a comprehensive understanding of the scene. \n \nImportance of Object Detection: \nObject  detection  plays  a  crucial  role  in  various  real-world  applications  and  has  become  a \nfundamental component of modern computer vision systems. Its importance lies in the following \naspects:","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":3,"lines":{"from":1,"to":16}}}}],["0a48cfe9-d607-41f6-b110-7de4ce4c1398",{"pageContent":"fundamental component of modern computer vision systems. Its importance lies in the following \naspects: \n1. Scene  Understanding:  Object  detection  enables  machines  to  perceive  and  understand  the \ncontent of images or video streams. By identifying and localizing objects, systems gain a deeper \nunderstanding of the visual data, facilitating more advanced analysis and decision-making. \n2. Autonomous  Systems:  In  fields  like  autonomous  vehicles  and  robotics,  object  detection  is \nessential  for  detecting  pedestrians,  vehicles,  obstacles,  and  other  relevant  objects  in  the \nenvironment. This  information  is  critical  for  ensuring  safe  navigation  and  interaction  with  the \nsurroundings. \n3. Surveillance and Security: Object detection is vital in surveillance systems to detect potential \nthreats, intruders, or suspicious activities. Real-time object detection allows for immediate response \nto security breaches, enhancing safety and security measures.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":3,"lines":{"from":15,"to":26}}}}],["006a6231-f97a-4964-80c1-02777749a5df",{"pageContent":"threats, intruders, or suspicious activities. Real-time object detection allows for immediate response \nto security breaches, enhancing safety and security measures. \n4. Medical Imaging: In medical imaging, object detection is used to identify anatomical structures, \nlesions, and abnormalities. It aids in the diagnosis and treatment planning, contributing to improved \nhealthcare outcomes. \n5. Augmented Reality: Object detection is employed in augmented reality applications to interact \nwith and overlay virtual objects onto the real world. It enables seamless integration of virtual and \nphysical environments, creating immersive user experiences. \n6. Human-Computer Interaction: Object detection is utilized in gesture recognition and tracking \nhuman poses, enabling more intuitive and natural interactions with computers and devices. \n7. Retail  and  E-commerce:  Object  detection  facilitates  product  recognition  and  localization,","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":3,"lines":{"from":25,"to":35}}}}],["17c4746b-cf1a-4722-b681-96517d8a2166",{"pageContent":"human poses, enabling more intuitive and natural interactions with computers and devices. \n7. Retail  and  E-commerce:  Object  detection  facilitates  product  recognition  and  localization, \nmaking it valuable in applications like automated checkout systems and inventory management. \n8. Environmental Monitoring: Object detection can be employed for wildlife monitoring, plant \nspecies identification, and tracking changes in the natural environment, aiding conservation efforts \nand ecological studies. \nOverall, object detection is of utmost importance in enabling machines to understand visual data \nand interact effectively with the real world. Its versatility and wide-ranging applications make it a \nfundamental  tool  in  various  industries  and  research  domains,  contributing  to  advancements  in \ntechnology and enhancing our daily lives \n \n2.2 Challenges Encountered in Real-Time Object Detection:","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":3,"lines":{"from":34,"to":45}}}}],["395a4494-b94b-4236-a209-1e85f3b85d9f",{"pageContent":"technology and enhancing our daily lives \n \n2.2 Challenges Encountered in Real-Time Object Detection: \n The landscape of real-time object detection presents an array of challenges rooted in the imperative \nof rapid and precise processing of visual data. These challenges emanate from the complexities of \nreal-world scenes, the exigencies of real-time performance, and the delicate equilibrium between \nspeed and accuracy within object detection algorithms. Key challenges encompass: \nComputational Complexity: Deep learning-based object detection models, exemplified by YOLO \nand  SSD,  impose  significant  computational  demands  due  to  their  intricate  architectures  and \nparameter  configurations.  Achieving  real-time  performance  on  platforms  with  constrained \ncomputational  resources  necessitates  meticulous  model  optimization  and harnessing hardware \nacceleration techniques. \n \nTrade-off Between Speed and Accuracy: Real-time object detection systems often grapple with a","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":3,"lines":{"from":43,"to":56}}}}],["b6a688fd-8dfb-486d-a6a1-3fd0746f48dc",{"pageContent":"acceleration techniques. \n \nTrade-off Between Speed and Accuracy: Real-time object detection systems often grapple with a \ntrade-off between speed and accuracy. Accelerated processing may entail model simplifications or","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":3,"lines":{"from":54,"to":57}}}}],["3f566fe3-8512-41be-8f18-d3c330ac7f77",{"pageContent":"J. Amr. Uni. 03 (2023) p.267                                                                                                                    Naif Alsharabi  \n \n \n270 \n \nreductions in spatial resolution, potentially impacting detection accuracy [2]. Striking an optimal \nequilibrium between speed and accuracy is imperative to meet real-time requisites while upholding \nacceptable detection performance. \nMulti-Scale  Object  Detection:  Real-world  scenes  feature  objects  of  varying  scales,  demanding \nsimultaneous detection of objects with diverse sizes [3]. Effectively addressing multi-scale objects \nis essential for comprehensive scene comprehension. \nOcclusion and Clutter: Effective object detection is hindered by occluded objects and cluttered \nbackgrounds.  Robust  algorithms  are  necessary  to  detect  partially  visible  objects  and  manage \ninstances with overlapping characteristics.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":4,"lines":{"from":1,"to":14}}}}],["fffa8d62-6d2a-47bb-8467-c27cfc359ea8",{"pageContent":"backgrounds.  Robust  algorithms  are  necessary  to  detect  partially  visible  objects  and  manage \ninstances with overlapping characteristics. \nAdaptation to Dynamic Environments: Real-world scenarios are inherently dynamic, requiring real-\ntime  object  detection  systems  to  promptly  adapt  to  environmental  shifts,  changes  in  lighting \nconditions, and moving objects to sustain precision and reliability. \nSmall Object Detection: Detecting diminutive objects, especially those situated at a distance or \npossessing low resolution, presents a challenge. Real-time object detection models must exhibit \nsensitivity to small objects without compromising overall performance. \nAnnotated Data and Labeling: The curation of extensive annotated datasets for real-time object \ndetection can be labor-intensive. The availability of accurate annotations spanning diverse object \nclasses is pivotal for effective model training [6].","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":4,"lines":{"from":13,"to":23}}}}],["132ddbd8-7906-43e3-a5c6-7423a1413410",{"pageContent":"detection can be labor-intensive. The availability of accurate annotations spanning diverse object \nclasses is pivotal for effective model training [6]. \nResolving  these  challenges  necessitates  ongoing  research  and  innovation,  encompassing  the \ndevelopment  of  sophisticated algorithms,  optimization  strategies,  and  support  from  hardware \ncomponents. Real-time object detection systems, adept at swift and precise analysis of visual data, \npossess  the  potential  to  revolutionize  applications  spanning  industries,  fostering  intelligent  and \nsecure interactions between machines and the physical world. Applications of Real-Time Object \nDetection:  Real-time  object  detection  has  permeated  a  plethora  of  applications,  leveraging  its \ncapacity  for  rapid  and  accurate  detection  and  localization  of objects  within  dynamic  scenes. \nProminent applications include:","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":4,"lines":{"from":22,"to":31}}}}],["ff6a8378-f01a-4dc3-8c39-5d801baab081",{"pageContent":"capacity  for  rapid  and  accurate  detection  and  localization  of objects  within  dynamic  scenes. \nProminent applications include: \nAutonomous Vehicles: Real-time object detection constitutes a foundational element of autonomous \ndriving systems, enabling vehicles to perceive and respond to pedestrians, vehicles, and obstacles \n[7]. The technology plays a pivotal role in collision avoidance, lane tracking, and overall situational \nawareness. \nSurveillance and Security: Real-time object detection facilitates real-time monitoring and threat \nassessment  in  surveillance  systems [8]. It  enables  the  identification  of  suspicious  behaviors, \nunattended baggage, or unauthorized access, contributing to heightened security measures. \nRobotics: Robots  endowed  with  real-time  object  detection  capabilities  navigate  environments, \nmanipulate  objects,  and  engage  with  their  surroundings [9]. This  enhances  human-robot","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":4,"lines":{"from":30,"to":40}}}}],["1ce43903-ae7a-428f-8881-569ddd920476",{"pageContent":"Robotics: Robots  endowed  with  real-time  object  detection  capabilities  navigate  environments, \nmanipulate  objects,  and  engage  with  their  surroundings [9]. This  enhances  human-robot \ncollaborations and extends the autonomy of robotic systems. \nAugmented  Reality: Real-time  object  detection  enhances  augmented  reality  experiences  by \noverlaying  digital  content  onto  real-world  objects [10].  It  enables  applications  such  as  object \nrecognition, interactive gaming, and immersive visualization. \nMedical Imaging: Real-time object detection finds utility in medical imaging, assisting radiologists \nin identifying and localizing anatomical structures and anomalies [11]. It expedites diagnosis and \ntreatment planning, contributing to elevated patient care. These applications serve as compelling \nexemplars  of  the  extensive  impact  of  real-time  object  detection  on  modern  technological","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":4,"lines":{"from":39,"to":48}}}}],["5f8f8a15-2439-408f-83d8-b177441498bd",{"pageContent":"treatment planning, contributing to elevated patient care. These applications serve as compelling \nexemplars  of  the  extensive  impact  of  real-time  object  detection  on  modern  technological \nlandscapes. As advancements in artificial intelligence and computer vision continue to unfold, real-\ntime object detection systems are poised to reshape industries and domains, facilitating safer, more \nefficient, and intelligent interactions between humans and machines.  \nConclusion: Real-time object detection constitutes a cornerstone of contemporary computer vision, \nenabling swift and accurate identification of objects within visual data streams. The fusion of deep \nlearning  algorithms,  hardware  acceleration,  and  optimization  strategies  has  catalyzed  the \ndevelopment of real-time object detection systems with applications spanning autonomous vehicles, \nrobotics,  security,  and  beyond.  Despite  remarkable  strides,  challenges  endure,  encompassing","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":4,"lines":{"from":47,"to":56}}}}],["3f037c6f-313d-4345-a604-28d62a234fd5",{"pageContent":"robotics,  security,  and  beyond.  Despite  remarkable  strides,  challenges  endure,  encompassing \ncomputational complexity, multi-scale object handling, and the delicate trade-offs between accuracy","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":4,"lines":{"from":56,"to":57}}}}],["c8e0a182-8edd-4896-b842-57f96f2b2f4b",{"pageContent":"J. Amr. Uni. 03 (2023) p.267                                                                                                                    Naif Alsharabi  \n \n \n271 \n \nand  speed.  Researchers  and  practitioners  must  collaborate  in addressing  these  challenges, \ncultivating innovative solutions and robust real-time object detection systems capable of flourishing \nin complex and dynamic real-world scenarios. The trajectory of real-time object detection continues \nto  unfold,  with  future  research  poised  to  yield  novel  algorithms,  architectures,  and  hardware \nenhancements. This  dynamic  research  domain  remains  pivotal  to  the  advancement  of  artificial \nintelligence,  propelling  intelligent  systems  toward  heightened  autonomy,  efficiency,  and \nadaptability.  \n \n2.3 Applications of Real-Time Object Detection \nReal-time object detection has found diverse applications across various domains, owing to its","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":5,"lines":{"from":1,"to":15}}}}],["3a334818-3b15-40ea-8327-da41eb95946f",{"pageContent":"adaptability.  \n \n2.3 Applications of Real-Time Object Detection \nReal-time object detection has found diverse applications across various domains, owing to its \nability to swiftly and accurately identify and localize objects in dynamic environments. Here are \nsome notable applications: \nAutonomous Vehicles: Real-time object detection is crucial in autonomous vehicles for identifying \npedestrians, other vehicles, traffic signs, and obstacles.  It plays  a pivotal role in enabling safe \nnavigation and decision-making for self-driving cars. [8]. \nSurveillance and Security: In surveillance systems, real-time object detection is used for detecting \nintruders, tracking suspicious activities, and identifying potential threats in live video streams. It \nenhances security measures and enables immediate response to security breaches. [12] \nRobotics: Object detection is essential in robotics for tasks such as object manipulation, object","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":5,"lines":{"from":12,"to":24}}}}],["6cd2aacf-ed74-4eef-9f14-16359039b811",{"pageContent":"enhances security measures and enables immediate response to security breaches. [12] \nRobotics: Object detection is essential in robotics for tasks such as object manipulation, object \nrecognition, and scene understanding. Robots equipped with real-time object detection capabilities \ncan interact safely and efficiently with their surroundings. [13]. \nAugmented Reality: Real-time object detection is utilized in augmented reality (AR) applications to \noverlay virtual objects onto the real world. It enables AR systems to recognize and interact with real \nobjects and enhance user experiences. [14]. \nMedical Imaging: In medical imaging, real-time object detection is applied to identify anatomical \nstructures, lesions, tumors, and abnormalities. It aids in faster diagnosis, treatment planning, and \nmedical interventions. [15]. \nGesture Recognition: Real-time object detection can be utilized for recognizing and tracking human","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":5,"lines":{"from":23,"to":33}}}}],["401882bb-ff8d-44ec-b708-17dc5b82e55c",{"pageContent":"medical interventions. [15]. \nGesture Recognition: Real-time object detection can be utilized for recognizing and tracking human \ngestures in human-computer interaction systems. It enables natural and intuitive interactions with \ncomputers and devices. [16]. \nRetail  and  E-commerce:  Real-time  object  detection  is  valuable  in  retail  and  e-commerce \napplications for automated checkout systems, inventory management, and product recognition. It \nstreamlines retail operations and enhances the shopping experience. [17]. \nEnvironmental Monitoring: Real-time object detection can be applied to wildlife monitoring, plant \nspecies identification, and tracking changes in the natural environment. It aids in ecological studies \nand conservation efforts. [18]. \nThese applications highlight the significance of real-time object detection in enabling advanced and \nefficient solutions across various domains. \n \n2.4 Real-World Applications of Real-Time Object Detection","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":5,"lines":{"from":32,"to":45}}}}],["768ec1e9-cd43-40fe-834e-7072be402eea",{"pageContent":"efficient solutions across various domains. \n \n2.4 Real-World Applications of Real-Time Object Detection \nReal-time object detection finds applications in: \n1. Autonomous Vehicles and ADAS: Ensuring safe navigation, collision prevention, and adaptive \ncruise control [19]. \n2. Surveillance  and  Security: Identifying  and  tracking  intruders,  enhancing  security  protocols \n[20]. \n3. Smart Retail and Marketing: Customer tracking, footfall analysis, and targeted marketing [21]. \n4. Industrial Automation and Robotics: Object manipulation, quality inspection, and automation \n[22]. \n5. Healthcare: Medical image analysis, surgical support, and patient monitoring.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":5,"lines":{"from":43,"to":54}}}}],["7a47eb0e-e0e2-4af7-b8cb-c73e4ed21d36",{"pageContent":"J. Amr. Uni. 03 (2023) p.267                                                                                                                    Naif Alsharabi  \n \n \n272 \n \n3. Traditional Approaches to Object Detection \nBefore the emergence of deep learning-based approaches, traditional methods for object detection \nrelied  on  handcrafted  features  and  specialized  algorithms.  Some  of  the  notable  traditional \napproaches to object detection are: \nHistogram of Oriented Gradients (HOG): HOG is a feature descriptor used to represent the local \ntexture  and  shape  information  of  an  image.  It  captures  gradient  orientation  information  and \ncomputes histograms of gradient directions to detect object edges and boundaries. HOG has been \nwidely used in pedestrian detection and other object detection tasks. [23]. \nHaar-like  Features:  Haar-like  features  are  simple  rectangular  filters  used  in  the  Viola-Jones","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":6,"lines":{"from":1,"to":14}}}}],["bf5bbbfa-fdc6-4f18-a900-fa9e762bd76b",{"pageContent":"widely used in pedestrian detection and other object detection tasks. [23]. \nHaar-like  Features:  Haar-like  features  are  simple  rectangular  filters  used  in  the  Viola-Jones \nalgorithm for object detection. These features capture intensity variations in specific regions of the \nimage and are computationally efficient for real-time applications. The Viola-Jones algorithm is \nknown for its fast face detection capabilities. [24]. \nFeature Matching: Feature matching methods, such as Scale-Invariant Feature Transform (SIFT) \nand Speeded-Up Robust Features (SURF), detect distinctive local features in an image and match \nthem across frames for object recognition and tracking. These methods have been used for object \ndetection and image alignment tasks. [25]. \nDeformable Part Models (DPM): DPM is a classic framework for object detection that represents \nobjects as a collection of deformable parts. It models the spatial relationship between parts and","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":6,"lines":{"from":13,"to":23}}}}],["847d749e-868d-4c33-8a28-7002af6038f2",{"pageContent":"Deformable Part Models (DPM): DPM is a classic framework for object detection that represents \nobjects as a collection of deformable parts. It models the spatial relationship between parts and \ncaptures  object  appearance  variations  to  improve  detection  accuracy.  DPM  has  been  used  for \ndetecting objects with articulated structures. [26]. \nSelective Search: Selective Search is a proposal generation method used in object detection to \ngenerate candidate regions likely to contain objects. It segments the image based on color, texture, \nand size to obtain potential object regions for further processing. [27]. \nWhile  traditional  approaches  to  object  detection  have  been  effective  in  certain  scenarios,  deep \nlearning-based methods, such as YOLO and SSD, have surpassed them in terms of accuracy and \nefficiency, especially in real-time object detection tasks. \n \n3.1 Sliding Window-based Methods","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":6,"lines":{"from":22,"to":33}}}}],["ad1829c8-a150-4469-86ef-347ca20fb476",{"pageContent":"learning-based methods, such as YOLO and SSD, have surpassed them in terms of accuracy and \nefficiency, especially in real-time object detection tasks. \n \n3.1 Sliding Window-based Methods \nSliding window-based methods were among the early traditional approaches to object detection. \nThese methods involve moving a fixed-size window across the image at different scales to detect \nobjects  at  various  locations  and  sizes. Although  sliding  window  approaches  have  been  largely \nsuperseded by deep learning-based methods, they provide valuable insights into the evolution of \nobject detection techniques. Here are some references on sliding window-based methods: \n1. Histograms of Oriented Gradients for Human Detection. This seminal paper introduced the \nHistogram of Oriented Gradients (HOG) feature descriptor, which became a key component of \nmany  sliding  window-based  object  detectors.  The  HOG  descriptor  captures  local  gradients'","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":6,"lines":{"from":30,"to":41}}}}],["61c84de8-e471-4c3d-9411-ef7f59ec3465",{"pageContent":"Histogram of Oriented Gradients (HOG) feature descriptor, which became a key component of \nmany  sliding  window-based  object  detectors.  The  HOG  descriptor  captures  local  gradients' \norientation information to represent object edges and has been widely used in pedestrian detection \n[23]. \n2. Object Detection with Discriminatively Trained Part-based Models. This work introduced \nthe Deformable Part Models (DPM) framework for object detection. DPM uses a sliding window \napproach to search for object parts, modeling the spatial relationships between parts for better \ndetection accuracy [26]. \n3. Distinctive Image Features from Scale-invariant Key Points. The Scale-Invariant Feature \nTransform  (SIFT)  introduced  in  this  paper  is  widely  used  for  feature  matching  and  object \nrecognition tasks. Sliding windows are often employed in SIFT-based methods to detect keypoints \nand perform feature matching across image scales [29].","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":6,"lines":{"from":40,"to":51}}}}],["8da92b36-5005-41f0-8040-2f913d700abd",{"pageContent":"recognition tasks. Sliding windows are often employed in SIFT-based methods to detect keypoints \nand perform feature matching across image scales [29].  \n4. Rapid Object Detection Using a Boosted Cascade of Simple Features. This paper presented \nthe Viola-Jones algorithm, which is one of the earliest and successful real-time object detection \nmethods based on Haar-like features. The sliding window technique is used in the Viola-Jones \nalgorithm to scan the entire image for potential object locations [24].  \nSliding window-based methods were limited by their computational complexity, as they involved \nexhaustive evaluation of the sliding windows at multiple scales, leading to high computation time.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":6,"lines":{"from":50,"to":57}}}}],["c7307fcb-e1fe-4b1c-9e1b-3fdb7a96f03e",{"pageContent":"J. Amr. Uni. 03 (2023) p.267                                                                                                                    Naif Alsharabi  \n \n \n273 \n \nThe  development  of  deep  learning-based  approaches,  such  as  YOLO  and  SSD,  significantly \nimproved the speed and accuracy of object detection by introducing end-to-end learning and novel \narchitectures. These modern methods have largely replaced sliding window-based approaches in \npractical applications, as they achieve real-time performance without the need for explicit window \nscanning. \n \n3.2 Feature-based Approaches \nFeature-based  approaches  indeed  played  a  significant  role  in  the  early  development  of  object \ndetection methods and were fundamental in the history of computer vision. These methods relied on \nhandcrafted feature extraction and specialized algorithms to identify objects in images. While they","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":7,"lines":{"from":1,"to":15}}}}],["0fec9f88-7879-4be8-bd32-19e84cb29efa",{"pageContent":"detection methods and were fundamental in the history of computer vision. These methods relied on \nhandcrafted feature extraction and specialized algorithms to identify objects in images. While they \nhave been largely surpassed by deep learning-based methods, feature-based approaches have paved \nthe way for more advanced techniques. The references provided highlight some of the key feature-\nbased methods used in object detection: \nLowe, D. G. [28]. Distinctive image features from scale-invariant key points. The Scale-Invariant \nFeature Transform (SIFT) introduced in this paper has become one of the most widely used feature \ndescriptors. It is valuable for object recognition, image matching, and object detection tasks due to \nits ability to extract scale-invariant key points and descriptors.  \nBay [29]. SURF: Speeded up robust features. SURF is another influential feature-based method \nknown for providing robust and efficient local feature descriptors. It uses approximations of the","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":7,"lines":{"from":14,"to":24}}}}],["4a5dd2b6-7f55-465f-822c-520eeb0f3f7e",{"pageContent":"Bay [29]. SURF: Speeded up robust features. SURF is another influential feature-based method \nknown for providing robust and efficient local feature descriptors. It uses approximations of the \nHessian  matrix  to  extract key  points and  is  commonly  used  in  object  recognition  and  image \nmatching tasks.  \nDalal, N [23]. Histograms of oriented gradients for human detection. The Histogram of Oriented \nGradients (HOG) feature descriptor presented in this paper has been instrumental in pedestrian \ndetection and object detection tasks. HOG captures local gradient orientation information, making it \nsuitable for identifying object edges and boundaries.  \nFelzenszwalb,  P. F [26]. Object detection with discriminatively trained part-based models. The \nDeformable Part Models (DPM) framework introduced in this work is a feature-based approach that \nrepresents objects as a collection of deformable parts. It captures the spatial relationships between","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":7,"lines":{"from":23,"to":33}}}}],["b5b9ef68-ef7c-4c86-b7ff-0df4f055f2d5",{"pageContent":"Deformable Part Models (DPM) framework introduced in this work is a feature-based approach that \nrepresents objects as a collection of deformable parts. It captures the spatial relationships between \nparts to improve object detection accuracy.  \nViola, P.,[24]. Rapid object detection using a boosted cascade of simple features. The Viola-Jones \nalgorithm, presented in this classic paper, is one of the earliest and successful real-time object \ndetection methods based on Haar-like features. It efficiently detects objects by selecting a subset of \nHaar-like features using AdaBoost.  These feature-based methods provided valuable insights and \nlaid the foundation for object detection research. However, they had limitations, such as the need \nfor handcrafted features and extensive computational resources. The rise of deep learning and end-\nto-end  learning  approaches  has  brought  about  substantial  improvements  in  object  detection","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":7,"lines":{"from":32,"to":41}}}}],["72d185ca-7783-4a57-9265-e3e790008671",{"pageContent":"for handcrafted features and extensive computational resources. The rise of deep learning and end-\nto-end  learning  approaches  has  brought  about  substantial  improvements  in  object  detection \naccuracy  and  efficiency,  making  feature-based  methods  less commonly  used  in  modern \napplications. The shift to deep learning-based methods has allowed for automatic feature learning, \nreducing the dependence on handcrafted features and enabling more sophisticated and accurate \nobject detection systems. \n \n3.3 Cascade Classifiers \nCascade classifiers are indeed a significant type of feature-based approach used in object detection, \nparticularly in real-time scenarios. They are designed to efficiently identify objects by using a series \nof stages or layers, each consisting of a weak classifier. The cascade structure enables the rapid \nrejection of non-object regions, which leads to faster processing times and makes cascade classifiers","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":7,"lines":{"from":40,"to":51}}}}],["90dbbef9-77ff-4bc4-bb75-2b93c2909096",{"pageContent":"rejection of non-object regions, which leads to faster processing times and makes cascade classifiers \nsuitable for real-time applications. The references provided highlight some of the key works related \nto cascade classifiers. Rapid object detection using a boosted cascade of simple features[24]. This \nseminal paper introduced the Viola-Jones algorithm, which utilizes a cascade of Haar-like features \nand AdaBoost to efficiently detect faces in real-time. The cascade structure ensures that easy-to-\nclassify regions are rejected early, speeding up the detection process.  Lienhart, R.[30]. An extended \nset of Haar-like features for efficient object detection. This research further extended the set of","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":7,"lines":{"from":51,"to":57}}}}],["dc23f229-a28c-49c9-9665-b8aad1112fd2",{"pageContent":"J. Amr. Uni. 03 (2023) p.267                                                                                                                    Naif Alsharabi  \n \n \n274 \n \nHaar-like features to enhance the detection of various objects and improve the efficiency of the \ncascade classifier.  Cascade classifiers have been historically successful in real-time face detection \nand have also been adapted to detect other objects. They were groundbreaking at the time of their \nintroduction and have inspired subsequent research in the field of object detection. However, their \nperformance is limited compared to modern deep learning-based object detection methods, such as \nYOLO  and  SSD,  which  have  achieved  higher  accuracy  and  versatility.  Nevertheless,  cascade \nclassifiers remain a significant milestone in the history of object detection, showcasing the potential \nof using a series of weak classifiers to efficiently filter out non-object regions and focus on potential \nobject regions.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":8,"lines":{"from":1,"to":14}}}}],["66c675c7-2367-47ec-bc19-3c7ff629b736",{"pageContent":"of using a series of weak classifiers to efficiently filter out non-object regions and focus on potential \nobject regions. \n \n4. Real-Time Object Detection Challenges Solutions \nSpeed  and  Efficiency: Achieving  real-time  functionality  necessitates  swift  frame  or  image \nprocessing, posing a challenge due to the computational intensity of deep learning methodologies \nlike  Faster  R-CNN  and  YOLO.  To  address  this,  researchers  have  developed  lightweight \narchitectures  like  SSD  and  YOLOv3-tiny,  compromising  some  accuracy  for  faster  processing. \nComputation speed augmentation is possible through hardware acceleration mechanisms such as \nGPUs or TPUs [31]. \nAccuracy: Maintaining detection accuracy is crucial, but expedited processing may lead to reduced \naccuracy compared to slower but more precise methods. To mitigate accuracy decline, advanced \narchitectures, intricate backbones like ResNet, and hyperparameter optimization during training can \nbe employed.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":8,"lines":{"from":13,"to":26}}}}],["670bd788-b5ee-4297-b904-cac813b8653d",{"pageContent":"architectures, intricate backbones like ResNet, and hyperparameter optimization during training can \nbe employed. \nVariability in Object Scales and Aspect Ratios: Addressing object size and aspect ratio diversity \nin real-world scenes requires techniques like feature pyramid networks (FPN) and anchor boxes. \nFPN captures multi-scale features, while anchor boxes facilitate predictions for differently sized \nobjects [3]. \nOcclusion and Clutter: Partial occlusion and clutter in real-world scenes complicate detection. \nResilient  object  detection  models  handling  occlusion  and  clutter  can  be  designed,  utilizing \ncontextual information or temporal consistency across frames for improved accuracy. \nLimited Computational Resources: Resource constraints in edge devices or embedded systems \ncan  be  tackled  through  lightweight  architectures  and  model  quantization  techniques,  reducing \nweight precision to optimize models.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":8,"lines":{"from":25,"to":36}}}}],["6be9c802-2e9e-4e92-80c7-a28108125bfd",{"pageContent":"can  be  tackled  through  lightweight  architectures  and  model  quantization  techniques,  reducing \nweight precision to optimize models. \nData Annotation: Training real-time object detection models demands substantial annotated data. \nEfficacy can be enhanced through transfer learning and data augmentation, utilizing pre-trained \nmodels and synthetic data to reduce annotation requirements. \nGeneralization to Different Environments: Adapting models from one environment to different \nones  with  varying  conditions  requires  assimilating  diverse  training  data  and  applying  domain \nadaptation techniques for adaptability. \nThe  convergence  of  algorithmic  advancements,  hardware  optimization,  and  curated  datasets  is \npivotal in surmounting these multifaceted challenges. Scholars and practitioners continually explore \ninnovative  techniques  to  advance  real-time  object  detection  for  applications  spanning  robotics, \nsurveillance, autonomous vehicles, and more.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":8,"lines":{"from":35,"to":46}}}}],["a1070db1-5c7d-4ce5-8e67-3099c6dd7a79",{"pageContent":"innovative  techniques  to  advance  real-time  object  detection  for  applications  spanning  robotics, \nsurveillance, autonomous vehicles, and more. \nHardware Acceleration : \nHardware  acceleration  enables  real-time  detection  on  resource-constrained  devices.  Techniques \nencompass GPUs, TPUs, FPGAs, NPUs, ASICs, and quantization. Edge computing, combined with \nhardware acceleration, mitigates latency, conserves bandwidth, enhances security, and enables real-\ntime decision-making, optimally tailored for specific platforms and requirements [32]. \n \n5. Evaluation Metrics for Real-Time Object Detection \nIn  evaluating  real-time  object  detection  algorithms,  a  range  of  metrics  quantitatively  assess \naccuracy, efficiency, and robustness. Key evaluation metrics include:","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":8,"lines":{"from":45,"to":55}}}}],["dcd23b2b-06bb-4d03-a075-b03bd4de569c",{"pageContent":"J. Amr. Uni. 03 (2023) p.267                                                                                                                    Naif Alsharabi  \n \n \n275 \n \nPrecision  and  Recall: Precision  measures  accurate  positive  predictions  among  all  predicted \npositives,  while  recall  gauges  accurate  positive  predictions  among  actual  positives,  evaluating \ndetection precision and the system's ability to identify target objects. \nAverage Precision (AP): AP averages precision values at different recall levels, synthesizing the \nprecision-recall curve for an overall performance assessment. \nIntersection  over  Union  (IoU): IoU  quantifies  overlap  between  predicted  and  ground  truth \nbounding boxes, determining true positive or false positive classifications. \nFrames per Second (FPS): FPS indicates processing speed, crucial for real-time applications. \nInference Time: Inference time measures model processing duration per frame, reflecting real-time \nefficiency.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":9,"lines":{"from":1,"to":15}}}}],["0bcbc417-e47a-4666-8aef-caaa30be74f3",{"pageContent":"Inference Time: Inference time measures model processing duration per frame, reflecting real-time \nefficiency. \nMean  Average  Precision  (mAP): mAP  calculates  mean  AP  values  across  object  classes  for \ncomprehensive performance assessment. \nAccuracy vs. Speed Trade-off: Balances accuracy and processing speed, aiding optimal model or \nconfiguration selection. \nRobustness: Evaluates performance under challenging scenarios, like occlusions and clutter. \nMemory Footprint: Assesses model memory storage requirements, vital for resource-constrained \ndevices. \nPower Efficiency: Gauges energy consumption, significant for power-limited devices. \nComprehensive evaluation integrates these metrics, tailored to application-specific requirements \nand constraints. \n \n6. Real-Time Object Detection Datasets and Benchmarks \nTo advance real-time object detection, diverse datasets and benchmarks enable robust evaluation","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":9,"lines":{"from":14,"to":28}}}}],["df996143-417e-4db7-8b38-7c1154d13306",{"pageContent":"and constraints. \n \n6. Real-Time Object Detection Datasets and Benchmarks \nTo advance real-time object detection, diverse datasets and benchmarks enable robust evaluation \nand comparison of algorithm performance across challenging scenarios. Prominent datasets include: \nCOCO (Common Objects in Context) Dataset: COCO provides detailed annotations for object \ndetection and instance segmentation, fostering algorithm refinement and research [33]. \nPascal VOC (Visual Object Classes) Dataset: PASCAL VOC supports rigorous object detection \nevaluation [3]. \nKITTI Dataset: KITTI offers real-world data for autonomous driving applications (Geiger [7]). \nChallenges in Benchmark Datasets: Considerations include diversity, annotation quality, biases, \nscale, temporal consistency, domain shift, and evolving technologies. \nEfforts to address these challenges include curated updates, standardized protocols, and diverse \nscenarios for comprehensive evaluation.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":9,"lines":{"from":25,"to":38}}}}],["51f77b54-c277-45e9-bd3d-ed0057980cea",{"pageContent":"Efforts to address these challenges include curated updates, standardized protocols, and diverse \nscenarios for comprehensive evaluation. \n \n7. Real-Time Object Detection Architectures \nSeminal architectures harmonizing real-time efficiency and high-fidelity detection include: \nYOLO (You Only Look Once): A one-stage architecture predicting bounding boxes and class \nprobabilities in a single pass, eliminating region proposal networks ( \nSSD (Single Shot Multibox Detector): Predicts multiple bounding boxes and class scores per \nfeature map location, renowned for real-time efficiency  \nEfficientDet: Balances  computational  efficiency  and  precision  through  compound  scaling  and \nefficient architecture integration  \nCenterNet: Emphasizes object center detection and spatial regression for high-fidelity detection  \nMobileNet-SSD: Merges MobileNet's lightweight architecture with SSD for resource-constrained \ndeployment","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":9,"lines":{"from":37,"to":50}}}}],["b6330e8d-df33-4581-947a-40cbce836872",{"pageContent":"MobileNet-SSD: Merges MobileNet's lightweight architecture with SSD for resource-constrained \ndeployment  \nEfficientDet-D: Tailored for edge devices, it extends real-time detection to low-power hardware \nthrough model compression  \n \n8. Conclusion and Future Directions and Challenges \nReal-time object detection stands as a cornerstone technology within the realms of computer vision \nand artificial intelligence, endowing machines with the capability to instantaneously perceive and \ncomprehend  their  surroundings.  The  amalgamation  of  advanced  deep  learning  algorithms,","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":9,"lines":{"from":49,"to":57}}}}],["eedef14d-e113-4247-bfff-6f07bf4e2db4",{"pageContent":"J. Amr. Uni. 03 (2023) p.267                                                                                                                    Naif Alsharabi  \n \n \n276 \n \nstreamlined architectures, and hardware acceleration mechanisms has engendered a paradigmatic \ntransformation,  catapulting  real-time  object  detection  into  diverse  applications  spanning \nautonomous vehicles, surveillance, robotics, and beyond. \nThe evolution of real-time object detection has been punctuated by an array of challenges, each met \nwith  innovative  solutions  that  have  reshaped  the  landscape.  From  the  advent  of  one-stage \narchitectures like YOLO and SSD to the orchestration of edge computing paradigms and hardware \naccelerators, the journey of real-time object detection is characterized by persistent refinement and \nprogress. \nLooking ahead, the trajectory of real-time object detection is poised to be marked by continued","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":10,"lines":{"from":1,"to":14}}}}],["e7ff08b6-cc36-4f9f-859a-0266ba2f73c8",{"pageContent":"progress. \nLooking ahead, the trajectory of real-time object detection is poised to be marked by continued \ninnovation and exploration of uncharted territories. The integration of real-time 3D object detection, \nmulti-modal  fusion,  and  adaptive  learning  holds  the  promise of  unraveling  new  vistas  of \nunderstanding and interaction between machines and the physical world. \nAmidst this unfolding narrative, the synergy between researchers, practitioners, and industries will \ncontinue to drive the evolution of real-time object detection, ushering in a future where intelligent \nsystems seamlessly navigate, perceive, and interact with the intricacies of their environments. As \nthe dimensions of speed, accuracy, and efficiency converge, real-time object detection stands as a \ntestament to the potency of human ingenuity in crafting technologies that redefine the boundaries of \npossibility. \nThe following table summarizes the recent deep learning Real-Time Object Detection Algorithm","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":10,"lines":{"from":13,"to":24}}}}],["da2c0a89-286c-4525-85c6-dd726409e48a",{"pageContent":"possibility. \nThe following table summarizes the recent deep learning Real-Time Object Detection Algorithm \nlearning Real-Time Object Detection Algorithm \nAlgorithm Year Framework Speed Accuracy Main Features \nYOLO (You Only Look \nOnce) \n2016 Darknet, \nYOLOv3, \nYOLOv4 \nVery Fast Moderate to High Single pass, real-time \nprocessing \nSSD (Single Shot \nMultiBox Detector) \n2016 Caffe, \nTensorFlow \nFast Moderate to High Multi-scale feature maps, \nanchor boxes \nFaster R-CNN (Region \nConvolutional Neural \nNetwork) \n2015 TensorFlow, \nPyTorch \nModerate High Region Proposal Network \n(RPN) for object proposals \nRetinaNet 2017 TensorFlow, \nPyTorch \nModerate High Focal Loss for handling \nclass imbalance \nEfficientDet 2019 TensorFlow, \nPyTorch \nModerate to \nFast \nHigh Scalable and efficient \narchitecture \nCenterNet 2019 PyTorch Fast Moderate to High Detects objects as points and \nregresses to bounding boxes \nDetectron2 2019 PyTorch Moderate to \nFast \nHigh Flexible framework with","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":10,"lines":{"from":23,"to":61}}}}],["354b0f0d-7cc8-46f0-853b-0ed58d92afa5",{"pageContent":"architecture \nCenterNet 2019 PyTorch Fast Moderate to High Detects objects as points and \nregresses to bounding boxes \nDetectron2 2019 PyTorch Moderate to \nFast \nHigh Flexible framework with \nstate-of-the-art models \nYOLOv5 2020 PyTorch Very Fast High Efficient  architecture,  focus \non speed \nHTC   (Hybrid   Task \nCascade) \n2019 TensorFlow, \nPyTorch \nModerate  to \nFast \nHigh Multi-task  framework  for \nimproved accuracy \nSparse R-CNN 2021 PyTorch Fast High Utilizes sparsity for efficient \ninference \nDeformable DETR 2021 PyTorch Fast High Utilizes  deformable  self-\nattention","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":10,"lines":{"from":56,"to":76}}}}],["191e54d9-05fa-4868-a200-ce3ef1ac5d39",{"pageContent":"J. Amr. Uni. 03 (2023) p.267                                                                                                                    Naif Alsharabi  \n \n \n277 \n \n \nNote that \"Speed\" and \"Accuracy\" are relative terms and can vary depending on hardware, software \noptimizations,  and  dataset  used  for  training.  Additionally,  the  field  of  computer  vision  is  rapidly \nevolving,  and  newer  algorithms  might  have  been  developed  since  my  last  knowledge  update. \nAlways refer to the latest research papers and benchmarks for the most up-to-date information. \nFuture directions include real-time 3D object detection, efficient hardware  architectures, multi-\nmodal  fusion,  and  incremental  learning.  Challenges  encompass  handling  complex  scenes, \nadversarial attacks, resource constraints, and data bias, highlighting the need for ongoing research \nand development to advance real-time object detection's accuracy, efficiency, and robustness across","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":11,"lines":{"from":1,"to":14}}}}],["2fc12f45-8cb5-4ccb-aefe-a0e9a1dfcf34",{"pageContent":"and development to advance real-time object detection's accuracy, efficiency, and robustness across \nevolving applications and domains. \n \nReferences: \n[1]S. Guefrachi, M. Jabra, and N. Alsharabi, \"Deep learning based DeepFake video detection,\" in \n2023 International Conference on Smart Computing and Application (ICSCA), Hail, Saudi Arabia, \n2023, pp. 1-8, doi: 10.1109/ICSCA57840.2023.10087584. \n[2] J. Redmon et al., \"You only look once: Unified, real-time object detection,\" in Proceedings of \nthe IEEE conference on computer vision and pattern recognition (CVPR), 2016, pp. 779-788. \n[3] W. Liu et al., \"SSD: Single shot multibox detector,\" in European conference on computer vision, \n2016, pp. 21-37. Springer. \n[4] S.  Ren  et  al.,  \"Faster R-CNN:  Towards  real-time  object  detection  with  region  proposal \nnetworks,\" in Advances in neural information processing systems, 2015, pp. 91-99.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":11,"lines":{"from":14,"to":26}}}}],["ea22df7a-d35c-4f50-a62a-185ef55cb920",{"pageContent":"[4] S.  Ren  et  al.,  \"Faster R-CNN:  Towards  real-time  object  detection  with  region  proposal \nnetworks,\" in Advances in neural information processing systems, 2015, pp. 91-99. \n[5] M. Sandler et al., \"Inverted residuals and linear bottlenecks: Mobile networks for classification, \ndetection and segmentation,\" in Proceedings of the IEEE conference on computer vision and pattern \nrecognition, 2018, pp. 4510-4520. \n[6] M. Everingham et al., \"The pascal visual object classes (VOC) challenge,\" International Journal \nof Computer Vision, vol. 88, no. 2, pp. 303-338, 2010. \n[7] A. Geiger, P. Lenz, and R. Urtasun, \"Are we ready for autonomous driving? the KITTI vision \nbenchmark suite,\" in Conference on Computer Vision and Pattern Recognition (CVPR), 2012. \n[8] L.  C.  Dua  et  al.,  \"Encoder-decoder  with  atrous  separable  convolution  for  semantic  image \nsegmentation,\" in Proceedings of the European conference on computer vision (ECCV), 2018, pp. \n801-818.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":11,"lines":{"from":25,"to":36}}}}],["182660cd-86f2-4c54-bff5-1959c8a527c5",{"pageContent":"segmentation,\" in Proceedings of the European conference on computer vision (ECCV), 2018, pp. \n801-818. \n[9] H.-C. Nguyen, T.-H. Nguyen, R. Scherer, V.-H. Le, \"Unified End-to-End YOLOv5-HR-TCM \nFramework for Automatic 2D/3D Human Pose Estimation for Real-Time Applications,\" Sensors, \nvol. 22, no. 22, p. 5419, 2022. doi: 10.3390/s22145419. \n[10] C.  Cao  et  al.,  \"Real-time  object  detection  in  augmented  reality,\"  IEEE  Transactions  on \nVisualization and Computer Graphics, vol. 24, no. 1, pp. 17-27, 2017. \n[11] H. C. Shin et al., \"Deep convolutional neural networks for computer-aided detection: CNN \narchitectures, dataset characteristics and transfer learning,\" IEEE Transactions on Medical Imaging, \nvol. 35, no. 5, pp. 1285-1298, 2016. \n[12] Y. Duan, X. Chen, R. Houthooft, J. Schulman, P. Abbeel, \"Benchmarking deep reinforcement \nlearning for continuous control,\" in International Conference on Machine Learning, 2016, pp. 1329-\n1338.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":11,"lines":{"from":35,"to":47}}}}],["f54880f5-73bd-4bab-b077-02ddf63438be",{"pageContent":"learning for continuous control,\" in International Conference on Machine Learning, 2016, pp. 1329-\n1338. \n[13] Y.  Zhang,  R.  Grosse,  \"Track,  then  Decide:  Category-Agnostic  Vision-based  Multi-Object \nTracking,\" arXiv preprint arXiv:1806.07235, 2018. \nRepPoints 2021 PyTorch Fast Moderate Representing   object   as \npoints \nYOLOX 2021 PyTorch Very Fast Moderate to High SOTA     speed-accuracy \ntradeoff \nSparse R-CNN 2021 PyTorch Fast High Utilizes sparsity for efficient \ninference","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":11,"lines":{"from":46,"to":55}}}}],["7ed99a25-ea97-492f-8ffa-c66f05de9496",{"pageContent":"J. Amr. Uni. 03 (2023) p.267                                                                                                                    Naif Alsharabi  \n \n \n278 \n \n[14] V. Balntas, E. Riba, D. Ponsa, K. Mikolajczyk, \"Learning local feature descriptors with triplets \nand shallow convolutional neural networks,\" in BMVC, 2016, pp. 1-12. \n[15] H. C. Shin et al., \"Deep convolutional neural networks for computer-aided detection: CNN \narchitectures, dataset characteristics and transfer learning,\" IEEE transactions on medical imaging, \nvol. 35, no. 5, pp. 1285-1298, 2016. \n[16] D. Pavllo, C. Feichtenhofer, D. Grangier, M. Auli, \"3D human pose estimation in video with \ntemporal convolutions and semi-supervised training,\" in Proceedings of the IEEE Conference on \nComputer Vision and Pattern Recognition (CVPR), 2018, pp. 7753-7762. \n[17] Y. Cao, J. Xu, S. Lin, F. Wei, H. Hu, \"Diverse image-to-image translation via disentangled","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":12,"lines":{"from":1,"to":14}}}}],["12ed7a05-dc45-4922-b03e-135a446ba9b6",{"pageContent":"Computer Vision and Pattern Recognition (CVPR), 2018, pp. 7753-7762. \n[17] Y. Cao, J. Xu, S. Lin, F. Wei, H. Hu, \"Diverse image-to-image translation via disentangled \nrepresentations,\" in Advances in Neural Information Processing Systems (NIPS), 2017, pp. 876-\n886. \n[18] M. S. Norouzzadeh et al., \"Automatically identifying, counting, and describing wild animals in \ncamera-trap images with deep learning,\" Proceedings of the National Academy of Sciences, vol. \n115, no. 25, pp. E5716-E5725, 2018. \n[19] P.  Y.  Chen,  C.  C.  Liu,  C.  H.  Chuang,  \"Real-time  Object  Detection  and  Tracking  for \nAutonomous Vehicles,\" arXiv preprint arXiv:2103.05991, 2021. \n[20] C. F. Liew, J. H. Lim, K. W. Chong, \"Deep Learning Surveillance System for Object Detection \nand Classification in Video Surveillance,\" Procedia Computer Science, vol. 105, pp. 35-42, 2017. \n[21] R. S. Mohan and B. R. Babu, \"A Survey on Visual Surveillance for Smart Retailing,\" ACM","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":12,"lines":{"from":13,"to":24}}}}],["3d18d793-317e-4797-91d0-b9b0f8bdaccd",{"pageContent":"and Classification in Video Surveillance,\" Procedia Computer Science, vol. 105, pp. 35-42, 2017. \n[21] R. S. Mohan and B. R. Babu, \"A Survey on Visual Surveillance for Smart Retailing,\" ACM \nComputing Surveys (CSUR), vol. 53, no. 2, pp. 1-34, 2020. \n[22] J.  Zhang,  J.  Zou,  K. He,  \"Multi-Scale  Object  Detection  with  Feature  Fusion  and  Scale \nEqualizing,\"  in  Proceedings  of  the  IEEE/CVF  Conference  on  Computer  Vision  and  Pattern \nRecognition (CVPR), 2020, pp. 9476-9485. \n[23] N. Dalal, B. Triggs, \"Histograms of oriented gradients for human detection,\" in Proceedings of \nthe IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2005, pp. 886-893. \n[24] P. Viola, M. Jones, \"Rapid object detection using a boosted cascade of simple features,\" in \nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2001, \npp. 511-518. \n[25] H.  Bay,  T.  Tuytelaars,  L.  Van  Gool,  \"SURF:  Speeded  up  robust  features,\"  in  European","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":12,"lines":{"from":23,"to":34}}}}],["16dae716-1fd5-4b2c-89fc-25b0e0041cc6",{"pageContent":"pp. 511-518. \n[25] H.  Bay,  T.  Tuytelaars,  L.  Van  Gool,  \"SURF:  Speeded  up  robust  features,\"  in  European \nConference on Computer Vision (ECCV), 2006, pp. 404-417. \n[26] P.  F.  Felzenszwalb,  R.  B.  Girshick,  D.  McAllester,  D.  Ramanan,  \"Object  detection  with \ndiscriminatively trained part-based models,\" IEEE Transactions on Pattern Analysis and Machine \nIntelligence, vol. 32, no. 9, pp. 1627-1645, 2010. \n[27] J. R. Uijlings, K. E. Van De Sande, T. Gevers, A. W. Smeulders, \"Selective search for object \nrecognition,\" International Journal of Computer Vision, vol. 104, no. 2, pp. 154-171, 2013. \n[28] D. G. Lowe, \"Distinctive image features from scale-invariant keypoints,\" International Journal \nof Computer Vision, vol. 60, no. 2, pp. 91-110, 2004. \n[29] H.  Bay,  T.  Tuytelaars,  L.  Van  Gool,  \"SURF:  Speeded  up  robust  features,\"  in  European \nconference on computer vision, 2006, pp. 404-417.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":12,"lines":{"from":33,"to":44}}}}],["b6cd217d-bba7-4f5d-ad90-879747ad2bc2",{"pageContent":"[29] H.  Bay,  T.  Tuytelaars,  L.  Van  Gool,  \"SURF:  Speeded  up  robust  features,\"  in  European \nconference on computer vision, 2006, pp. 404-417. \n[30] R. Lienhart, J. Maydt, \"An extended set of Haar-like features for efficient object detection,\" in \nProceedings of Image Processing, 2003. \n[31] J.  Redmon,  A.  Farhadi,  \"YOLOv3:  An  incremental  improvement,\"  arXiv  preprint \narXiv:1804.02767, 2018. \n[32] N. Sharma, P. D. Shenoy, \"A Survey of Edge Computing Architectures for Real-time Analytics \nof IoT Data,\" Journal of King Saud University-Computer and Information Sciences, 2020. \n[33] T. Y. Lin et al., \"Microsoft COCO: Common Objects in Context,\" in D. Fleet; T. Pajdla, B. \nSchiele, T. Tuytelaars (Eds.), Computer Vision – ECCV 2014, 2014.","metadata":{"source":"C:\\Users\\User\\Projects\\LangChain_Chatbot\\docs_in\\Real-Time_Object_Detection_Overview_Advancements_C.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Author":"Naif A. Alsharabi","Creator":"Microsoft® Word 2013","Producer":"Microsoft® Word 2013","CreationDate":"D:20231026200903+03'00'","ModDate":"D:20231026200903+03'00'"},"metadata":null,"totalPages":12},"loc":{"pageNumber":12,"lines":{"from":43,"to":52}}}}]],{"0":"c7ca8b35-7232-421a-9c75-41b3b94d83df","1":"7ac7433d-88c4-407c-9ab3-3b57f37b75ab","2":"38ab9d22-7305-4630-845f-dd09b457b4b1","3":"d9006d3e-7f59-4c93-b035-66fa203b0329","4":"a7fe010f-6ab3-425f-8aac-2de113d38aef","5":"eb8e340c-12bf-4f8c-a686-738862df44a3","6":"5b7ce767-569c-45a3-948f-17de88c5427f","7":"5613c8d7-020e-4d67-bbf2-9b108d788d7f","8":"8e85918e-4210-45d9-8b03-22d68dafc6a5","9":"b446c9d9-724d-49f4-a18c-11a3b170ebe7","10":"891cdfa0-d0d4-4c97-b11f-cd749edd8633","11":"624e7d3b-f3f9-4362-acf6-b935cb9bb2c5","12":"54e22e5c-360f-49bb-8e0f-c9d845c33392","13":"23b0bca8-27fe-4263-962d-50f85685a0dd","14":"ecab1199-7aaa-4080-b39c-fd10bebc1df6","15":"52874206-8dca-44fa-8538-ff5d380bb7fb","16":"2987c63c-1130-4cb7-a90f-2b77bb706305","17":"c52b4fd8-5b31-4d51-8002-e1866f8b5ea6","18":"7d220bb9-8829-46af-93d6-7f076bbead6c","19":"7b66ed69-cc01-48a0-baf5-a184b754d7a4","20":"8c035d1c-23b8-4161-8142-f19397a92b87","21":"6dcfbc69-0942-4fbd-9fd4-ed70ecff6342","22":"0dcba4f8-d14a-4d1c-8481-4938fa64c4b9","23":"f0ab3cb1-0032-40b0-8d22-c408831e9355","24":"c5c0ee5d-f92b-443d-80f5-ccff8bf425e5","25":"6ba3e1fc-f330-49e3-87de-f1fc1252bf3f","26":"5cf98ee2-b0f3-4795-812c-db94a1508574","27":"4f411c25-4029-4528-9e2d-7b6962406aa5","28":"f7a55481-4f2b-485d-aeaf-2281041a3a30","29":"f1a09851-3fee-4b38-aaf2-28d73cad67ca","30":"529e80db-30cc-4123-b0b2-93dec3382a6a","31":"1f4e0570-e797-484e-a50e-e1a8188f0377","32":"16249c2b-57a5-4233-ac57-866664658c44","33":"0bfa2a55-611f-47cf-8875-31fe67eb1a09","34":"4a48f83b-3393-4d92-84bd-7341c1ca43f0","35":"f026bbf4-f207-4b7c-8e1e-9662dc1da111","36":"8a935b2c-081e-4f48-8855-182503180168","37":"cd985357-ebc3-416c-9b4d-c35a59c4ea40","38":"0090c70b-c32f-4913-b334-054d296fface","39":"8c9375a8-4faa-4b76-8d7c-66cfc575ad11","40":"58edbfa3-fd9d-4cc6-be5b-475ea31dda1c","41":"3e1212b0-1725-4565-8f15-7f3c2929ac89","42":"9e56b406-586e-4df9-bdb8-436825bacaac","43":"1b6425b9-1937-43ca-b943-5daf3d0192c3","44":"18874489-d027-4c4f-9f29-5e2505feaa5d","45":"11967836-32c2-4da7-81ee-7a548114bf4a","46":"8c4c2298-6a30-4d61-a421-0c0efe1eea60","47":"8e48ec68-9f1c-4166-a683-073cf3273429","48":"80db3145-0421-40c3-b0b3-e016c025496b","49":"96c71250-87b9-4335-953e-5d498db93fe0","50":"3c9714a4-1ac2-40dd-bf22-fc7990fccfe6","51":"436f606a-2c8c-4208-b0e9-bf611c314214","52":"41071151-2c51-4225-836c-6af6cde547a9","53":"953b814d-0bed-4ff8-b103-079db817898d","54":"d89bf787-c2f6-4ca8-8f69-70a3f07cb3b6","55":"ba1114d2-4e24-4968-bdf7-2d7c02113de6","56":"d250ca61-cc0d-47da-8d84-becb94f5415b","57":"accc2481-3df4-4f46-9a3f-5fa23561604e","58":"40cfece0-d741-4958-9986-4731f8498d60","59":"d68889e3-d94e-4639-b017-287e49710109","60":"2f0d1d1e-7985-457b-bd08-d370c225c91c","61":"391db8bf-135c-45e3-806a-d20c412df7af","62":"8cf1f8f0-7fc2-4ed1-bde2-c9e9df07c815","63":"4f548604-bd70-4157-aa4b-ad50ab546ad0","64":"5d7cf74d-d1f9-4b73-aa1a-e587878877ac","65":"9134b12c-c7af-44be-b24c-489cabd35a0d","66":"074cc7ff-c4bb-4d05-97bf-a267832df175","67":"799b9d11-b8fb-4e12-8223-e6b489bdfc76","68":"5a904cdd-0cae-4a34-a2dd-9873f564c840","69":"ed8709af-1bca-4d88-b272-ee002e7b189c","70":"2675d56a-2b5e-4045-8d71-ac298b2a28f3","71":"ce353f58-b86f-402a-a9be-9dadf5eb2473","72":"936c185a-8ec8-4d91-8382-d6b90d03f627","73":"e066782c-e66d-46ad-ae2c-e5cc54d7236b","74":"69ce6afe-f32c-4b50-a598-b53517cd93d6","75":"3d3d7c1b-f905-492e-b2b5-3fc09d084247","76":"14b47b7b-9168-4578-b172-a5eb5f75739c","77":"82468cc7-24d2-4a7e-9ab6-be40db6d7147","78":"df5f35e5-af58-4944-9d3f-38ad0a81b65a","79":"d229be17-2ee8-4980-b456-505e11e51820","80":"de195f6b-913f-4731-bb5b-b058ea97967d","81":"b454090e-a1b3-4409-be24-bc787d69eaff","82":"07852fbf-e335-480c-b77d-2497875ed8a0","83":"d84f1ac6-0fa5-4988-854a-35eda5fe3e76","84":"eee9450f-624c-4d10-ba6e-e84f6f25cf87","85":"50ce6cf0-3264-45b3-982d-9b0249534640","86":"70c7e9e6-11a5-4374-9f12-27cebc9ed2bf","87":"55214845-b92b-40f3-9236-57e6200d4395","88":"4f072366-0c01-4d62-a050-413e911ee7a4","89":"cf35ecd1-5380-44a5-aeba-b67790aba52f","90":"82095ae9-f08c-40c1-bc11-73dc9bdba3e9","91":"5fb7ccca-f515-487b-a23d-0cf3e3543615","92":"115c6d91-67a7-4d3a-8b6c-cc26d371751f","93":"40e26b5b-c13e-427c-95e7-248f8cc087b9","94":"ebba686b-efe3-4ddd-8746-b7d11a1384fe","95":"f199a4b1-3057-478c-ad5c-4f34cd6bf021","96":"de8911af-0dd3-4380-a041-2636212bbf43","97":"44ee1990-9a50-4e6b-a1f3-f435b2da166a","98":"f8d4568a-c85c-4797-b2d6-0cefc54f45c6","99":"fa595a57-532e-4d76-800a-6103cc8b5b8a","100":"745b52bd-0c9b-42da-aacd-f1d026b41a3a","101":"c4416f51-659c-4158-bf7c-df9d5ec6629f","102":"23c458c9-a002-4e2d-b7f1-777ba04327f0","103":"81f4e2bd-aed6-4abd-ac92-03169e2361a3","104":"ea8bd9a0-d198-4d1e-be94-7c902834ddda","105":"fa178d3d-c073-4a97-9d52-10f717144783","106":"058b6749-21db-44ea-b1b4-657cb6f585d5","107":"167c5a6d-a80e-415b-8ed7-deb1135a0972","108":"03de37a4-b847-4756-9ec5-d083397f8bb5","109":"0708c79e-d6e3-4794-9c88-bef5a79823f1","110":"096f1e36-776a-420e-947f-4d4948f1d949","111":"61e0360f-f3df-4a47-a6fd-360bc0bbb486","112":"3e64197c-c9f8-4201-853c-2b3504c1792e","113":"47b067a4-b1dc-4432-906a-9c3b5fba1548","114":"1f0e2b3e-8196-4883-a092-2def281442f1","115":"a0bde366-b69b-4544-b852-287e8ae40325","116":"f39025be-976c-4091-93e0-663fd8bf8081","117":"7dfaf66c-7d3d-4011-bba8-1516dfb4e9f1","118":"f117c5ed-af69-4702-9fe3-2ada0dfdcd44","119":"d46ea7a9-299e-4498-a832-1146f1f4fc2b","120":"ded9c0ff-fc73-491e-bd7e-523765761d04","121":"dc1d3c81-d3c3-42c6-8a9f-0bb2546977a4","122":"4abc5523-5596-457c-90ce-c5a7b05d6d3f","123":"9787f160-8493-4070-be3d-fff876f83644","124":"d7042b15-de8c-4e03-ba86-eb0a31b75f46","125":"78b4fc8a-06dc-48a2-b662-c7f004e60481","126":"9e8cd049-1e56-483c-ae15-3de059d07445","127":"619ff864-9f49-40cb-8a36-f9e1ceb0dff4","128":"eed61c70-6871-49f7-afb8-39823d35e599","129":"42b09dbd-a9d9-4949-ad36-252587711624","130":"c9f9ad67-a04b-4fc1-85e0-01fd9367b223","131":"1ef32f26-7bfb-44ee-af04-22e18b93bbd8","132":"37ece6c0-bf62-4d38-ba9b-240a31672d91","133":"c7068c12-a29a-4d31-b07f-a1f4e26838c4","134":"3a98ba32-ad41-4279-b7ce-113e265bf0b8","135":"4b1a9cdd-8f28-4266-bce5-db865f504645","136":"727cd485-6c12-40b5-b369-334026b8cc1f","137":"4193dc9b-4b31-4fef-ac03-006d841bf3f6","138":"ee758998-1218-48e3-bcce-6398d7904ca8","139":"a0d3b88f-0afd-4a6a-a10a-2f79919b7d9b","140":"84a41406-32c6-4d52-b204-c909a4c96199","141":"7cb79dbe-92e5-4ecc-8733-cf2ec4c7f66b","142":"58f8617d-00e1-4a0c-a295-e3499dfa8c8d","143":"ba333abc-7ed3-48f0-a163-ae407f923bc5","144":"56ecd518-9bd5-4337-bf2a-bc8d701d313d","145":"8d073e1c-72d5-433c-8ebf-b4cb7f25f502","146":"6199307e-df77-43d8-9874-5526bc0f39a1","147":"daef2bc2-9d33-42b9-b726-05f9f9019f4a","148":"ab085c25-6a7d-4059-81ea-ece3001c7573","149":"0c3c8e39-2af3-4fcd-a7e7-0d6c0d68381f","150":"a21684be-28db-4efd-b120-479daa8dda36","151":"9cb5af22-9faf-4fa6-a91e-e6eaefd6331c","152":"5d4bffcc-4e36-4c7e-908c-c9f0a27dabc5","153":"6ab3bffc-e585-4395-b423-0693003f7c68","154":"6bea52c3-37a9-4109-a957-eb8c21487689","155":"1264ccb8-d640-4206-98d0-7d45065ceadd","156":"5fea42a0-7639-4222-9962-06cbb588d486","157":"c9e88bf5-a966-4b94-8a3f-61399a7c5a1e","158":"9873260a-65fe-432f-b828-2cb4299ac11d","159":"d4366774-73f0-440c-a2a9-ad0d6408334a","160":"fee6399b-be55-4b6b-b556-bf7c87051cf4","161":"3afe51fc-a493-41d3-9fc9-edf52e0e8760","162":"1486bbca-f0d6-48a2-a004-3d57123032a4","163":"4351a511-2968-45b0-a117-229a18bc0e7f","164":"43ec84c9-4ad8-44f4-9478-97331b5bd4a2","165":"b9f523c6-ef4c-462a-bfc9-385ae90a63b4","166":"ba638558-aab8-41fe-b6b4-a71b0dddd056","167":"e1ccbdb8-247d-45ac-b4b2-753983e63d5c","168":"cefa2bd1-62d9-4094-8de8-59fadb1d5c39","169":"1d0022bc-7e11-48b9-a4b4-b85910749e1e","170":"222e0067-0e5d-43c8-9c74-b73c3e6874b8","171":"15298386-120e-4be6-a80f-f1b399a4e960","172":"5109e251-416a-48f8-a778-589ed016ef93","173":"13499d78-3cd1-45cc-8f11-ff6b9b56241e","174":"632a080a-9bca-438e-8650-f6e2e28105e1","175":"1b4d6c16-4fa7-4dcd-b15c-ebb0d4153064","176":"595251be-16e6-49fd-84c5-c7ad42fc5157","177":"31bc76eb-3c73-4341-9ef6-9fe6b0d89fe7","178":"9ed1ba0f-cbc7-40e9-98f2-e90866ef7e8d","179":"d546f574-33b9-42bc-b526-6d7f1d7c1059","180":"4f636727-e681-4b98-81bf-992b13cd7084","181":"23281b86-4962-42bb-9c1f-e60f078e25f3","182":"03be3fbf-5a70-41ce-9129-bab2f8bfed8d","183":"ac50a87f-55c7-472a-8c20-75d426393265","184":"22d4c96c-1465-48d7-a067-8ff494bbff50","185":"f597cd6e-a426-4042-b465-30ad71602f77","186":"4debe2af-480a-4c36-89d9-8800f75ed9a1","187":"78a0d566-8842-4585-9821-ba44f6ec7ee0","188":"67908ee7-82fc-4d7d-aab6-00d3fbb9747f","189":"92fb6005-5e7d-447d-b7b9-3da9f3bf4117","190":"554769ec-a119-414a-8d03-9e9822412795","191":"514cd5e5-9dfb-40a6-aa9b-8f89afcd01d3","192":"c0b83296-133b-4a30-af97-d162e7236b40","193":"dd78a995-9d9c-4fd0-bce5-b8726550b10a","194":"3644c099-3f62-452b-b9b7-9c5d51fd2bb5","195":"b9ee2654-eecf-43f9-abdc-9d4684f04a40","196":"11225ba7-f2f1-46f3-ae37-a68f3b26efa6","197":"ac6501e3-ae86-46c0-be50-4366fbcb8cbf","198":"47892fb3-47fd-4783-8911-821af66560f6","199":"35549a36-887e-42ca-b5c3-709ae9cc37df","200":"cbb2b9e8-0428-470c-9cef-d65973782915","201":"cf739e7c-e6ca-438d-996c-4a7052049912","202":"a68f35d3-959f-4449-9b60-7e1971b1e9c8","203":"53cb0e83-3981-4b8b-8ef3-a1046dd496dd","204":"07662e27-b4ec-4aa6-bde1-5d771de977a0","205":"c39bf6f0-0694-4780-8386-262b8dccd578","206":"14abc865-e040-4cc4-8634-65181c90f309","207":"884a22c4-aa08-4830-b016-a656fa1f1137","208":"b6495294-b9ca-417e-b0ca-c01977ff5869","209":"291c50ba-9ce4-41ff-8ca6-9819d6560f50","210":"6a3b83a9-afbb-4a44-9db2-786c8114a006","211":"5f0ef8bc-7f41-4568-b1e2-c312d70f3263","212":"c0ef911f-53e8-4a68-919a-980efd1d8a39","213":"c7776c0b-4826-43ea-ac94-f5a2c82a3d57","214":"0da1b9bf-2d76-48a4-be40-cbe507a67edb","215":"fe0181b2-fe9e-4c9f-b2bc-b784e8ca7dac","216":"7762a25c-b0b2-47ae-a29d-32f098318e5a","217":"8d8642e9-2c57-4fa0-8fed-4523a73eec09","218":"f41eb4d6-c55e-4419-bcde-de79bf09bf52","219":"b2443428-c6e2-4ce1-a127-3b6646e07dfa","220":"a13f2857-98ff-4335-ba0e-acc83c4519ac","221":"0f6ea917-01c7-4f21-87ff-27acf7f1a8d1","222":"04f8b5da-2f6d-4690-ae86-ade3e0bb277a","223":"54ddb86c-116e-4df3-bcbf-71b28ceefdac","224":"803fd9cd-2266-462f-b440-c92dafbd8875","225":"5ad35af3-7f60-4f14-8085-e8b6eec25df0","226":"1da482f7-f0df-438b-9ae6-7e0fe686b30a","227":"2a4b7d56-804e-4bf5-aa9c-c82bedebfabd","228":"b7bd2fb2-549e-494e-879a-7a535d9c7d0c","229":"2a27812f-ca59-4a7a-8415-130f3cf83902","230":"c8619035-bb3a-4826-b59c-24b1e526dea5","231":"8f81ada9-12d1-4f97-b40f-079829530b37","232":"8220b64d-5585-478c-a7b3-87af0583c09f","233":"f41c32f5-4c53-49e5-b276-f5162b167cd7","234":"e5bd4304-7d70-49ad-a802-3f3ef8754bc1","235":"5825019b-be6d-4784-aa47-129669054ccf","236":"76a86d0d-ff96-4a85-96bb-e5e769fe1ee6","237":"4e96dc1a-8cff-4797-9170-92496cc957ba","238":"b08bd689-56be-4673-9512-428efaf017fc","239":"8f6d4c45-349c-45e5-ae19-52b08a29f119","240":"1d439033-1535-4d3a-8629-e30b16709e12","241":"9245a262-6e87-4688-9e42-2a07f2f0a7ff","242":"a40d1b72-9873-461b-aadc-205fd0a611af","243":"6c6f927e-dd8a-4108-b3d8-f283e1f098e9","244":"af4e082d-601c-4cd8-a3d8-0613292f8521","245":"017ba726-d07a-4ebc-824c-6264d89140dc","246":"d2673388-e640-4ac4-a779-f747a1cb7271","247":"53f788fe-8cc0-4971-a842-75cacaf75de9","248":"c48f9cfb-6629-4517-9d22-bea8c9277787","249":"ebe7ad6a-1b42-4b30-babf-9cd6227ba476","250":"67055809-550f-4ce2-bcc2-e2f97d1c1800","251":"1799b445-830f-49be-ae27-5dbe97cdac4a","252":"706454a0-11c7-40e7-bc84-f8ead5f4e470","253":"e713fa35-f1cc-4737-97e2-7d7277389e96","254":"7e3a6c03-195e-4361-b0fb-cf651e9ff576","255":"d5b9508d-f4b9-430b-a7c0-9cad6efa4527","256":"95a02cb1-d572-4ddf-876b-977f2eb9a78b","257":"9623c0d2-db7a-40c8-a4ab-f59dca22630b","258":"d828b395-f6db-49ed-8fd4-8548d37dfced","259":"398aa72f-c181-4777-9b1d-01532f3438cb","260":"f843ee6e-a31a-4761-a369-820afd91265a","261":"847b705c-2820-49c6-9674-ad2c6770796f","262":"10523043-5d35-4e84-a84e-71fee6590c06","263":"04d6d4f9-a548-47f8-9c47-91cd5b93ae65","264":"d49fb9fc-d954-46dd-afe4-91f42189cf20","265":"eee32626-882f-4be1-8e7a-58edea6c35fc","266":"1859eb3c-68c6-4486-bfaf-f515e0a59aae","267":"30b44125-39d1-4f0c-a98d-764f13385bab","268":"aa6a6e2f-94f0-47f0-a7a5-fa803494dd83","269":"8246dff4-ee4b-473a-8f32-47b3d35a83ad","270":"1c532b86-752b-4788-8d13-5c12015861e1","271":"a2dcd84c-dece-4ead-bee7-ae4f6155e7b5","272":"55e8f435-b015-4d6f-ad06-75ebfee42817","273":"f6f8da93-6ae1-4405-8bf9-5ed837f85373","274":"4c7823d6-bed9-4c9f-ba96-0e466d00c75d","275":"c672b0d5-ce2a-48e1-bc54-b882d3e0e68b","276":"d27c12ac-7bb6-4c9c-8be9-f0c96ae51be3","277":"d9f17668-d67c-4edf-b601-0eb304bf9a8e","278":"7b27a7a7-f44f-4352-a544-3b6b4b2a00a2","279":"955ac5e0-4a04-48f8-96a9-ad816d6f4bb8","280":"379f6877-9be7-4cb7-8b86-062e4b0ac259","281":"18e65fdd-d57f-411c-be97-692094f66e23","282":"16afe8b4-cf9f-4ec4-a85c-257565925219","283":"7886c202-33f2-439d-8c12-d14c311cb7e2","284":"48215d96-6784-4100-b763-fcf026e1aa30","285":"46ef62b3-2b2c-4043-872f-63ce2b1835ad","286":"326dc970-2d94-4a10-b650-cc46fb74c2af","287":"c39d007c-3889-484a-9d0e-00dc16abef0f","288":"2af0ade8-90f7-4ccd-b7a9-7667c953db84","289":"2e7c27a8-67d2-4a83-96cd-4329e4a2a97a","290":"140eb4d4-8ba9-4d42-b02f-b5f8147332d5","291":"b100ff06-febc-4a90-8481-69b421b8f169","292":"420ab7c5-eb0f-4ea5-8478-6cbab6ddd93e","293":"a4cafcab-3b0a-4034-bc57-d46e26771d74","294":"f02d7503-f6cf-45c5-9d79-6111875ab91b","295":"dc72db94-1199-4771-9202-a7ae044c6b9f","296":"92ed1150-1567-437c-b17d-63af3b6452f7","297":"585a26aa-57eb-4730-9677-6a97b430b9a8","298":"9a3f5354-fdeb-407e-8b7c-cc7c126b524b","299":"e5516ac5-e2b7-49ec-ab83-dc9ad7a6e585","300":"447c54d0-887c-4bf3-9ed7-0e4e7bf2ca11","301":"23e4a8bf-19ed-44f8-80bf-fe2062a0ef45","302":"53592b40-e417-4654-a1d2-920ab72e8238","303":"b74e15ba-693b-463c-844e-063cd2ecd026","304":"441ddb58-8d49-4653-b248-f942537faf01","305":"a76cab46-3ff7-4b7a-9fce-317a5dd78cb4","306":"40149cab-c506-42f7-b862-eb16efc7df0b","307":"b4467e1e-6b7f-4da5-905a-af78617096ba","308":"ee8bf838-f7ed-456d-b92b-dfc9d1f9a42f","309":"45c47256-4137-475f-9b60-952a81bce917","310":"9c682c9b-b227-4eba-b2f4-7a8a2fb6074d","311":"2cc60553-b0d6-4c33-98df-4c93b2285677","312":"96616d37-603d-4f8a-b15a-46e946487a5d","313":"82e34ce3-5ccd-4982-8d4b-35fd78ccd7f6","314":"595ac521-4175-4e9c-a89c-090a629a5504","315":"6a6983ee-69fe-477b-a095-e3fb94c43801","316":"77b10869-6e18-4cc0-be47-038c96818fc3","317":"ea8879af-486a-4ba1-927f-65ed11837b18","318":"075bacd0-8c73-4661-9e55-50fd2efb7d03","319":"4126eedd-f977-4e00-b9a8-ac1dbbfed1a5","320":"f6124ba6-6b2c-4f03-880d-a5890fb32e2e","321":"b4add3f7-7cd8-4cd1-94e9-e118045a2d22","322":"c6d991fb-b5fd-477d-9be4-b2bba28c35a4","323":"16a56379-02fc-4a02-82fc-52a457298712","324":"548dcdee-d904-4285-a0cc-96359ab7a983","325":"0a48cfe9-d607-41f6-b110-7de4ce4c1398","326":"006a6231-f97a-4964-80c1-02777749a5df","327":"17c4746b-cf1a-4722-b681-96517d8a2166","328":"395a4494-b94b-4236-a209-1e85f3b85d9f","329":"b6a688fd-8dfb-486d-a6a1-3fd0746f48dc","330":"3f566fe3-8512-41be-8f18-d3c330ac7f77","331":"fffa8d62-6d2a-47bb-8467-c27cfc359ea8","332":"132ddbd8-7906-43e3-a5c6-7423a1413410","333":"ff6a8378-f01a-4dc3-8c39-5d801baab081","334":"1ce43903-ae7a-428f-8881-569ddd920476","335":"5f8f8a15-2439-408f-83d8-b177441498bd","336":"3f037c6f-313d-4345-a604-28d62a234fd5","337":"c8e0a182-8edd-4896-b842-57f96f2b2f4b","338":"3a334818-3b15-40ea-8327-da41eb95946f","339":"6cd2aacf-ed74-4eef-9f14-16359039b811","340":"401882bb-ff8d-44ec-b708-17dc5b82e55c","341":"768ec1e9-cd43-40fe-834e-7072be402eea","342":"7a47eb0e-e0e2-4af7-b8cb-c73e4ed21d36","343":"bf5bbbfa-fdc6-4f18-a900-fa9e762bd76b","344":"847d749e-868d-4c33-8a28-7002af6038f2","345":"ad1829c8-a150-4469-86ef-347ca20fb476","346":"61c84de8-e471-4c3d-9411-ef7f59ec3465","347":"8da92b36-5005-41f0-8040-2f913d700abd","348":"c7307fcb-e1fe-4b1c-9e1b-3fdb7a96f03e","349":"0fec9f88-7879-4be8-bd32-19e84cb29efa","350":"4a5dd2b6-7f55-465f-822c-520eeb0f3f7e","351":"b5b9ef68-ef7c-4c86-b7ff-0df4f055f2d5","352":"72d185ca-7783-4a57-9265-e3e790008671","353":"90dbbef9-77ff-4bc4-bb75-2b93c2909096","354":"dc23f229-a28c-49c9-9665-b8aad1112fd2","355":"66c675c7-2367-47ec-bc19-3c7ff629b736","356":"670bd788-b5ee-4297-b904-cac813b8653d","357":"6be9c802-2e9e-4e92-80c7-a28108125bfd","358":"a1070db1-5c7d-4ce5-8e67-3099c6dd7a79","359":"dcd23b2b-06bb-4d03-a075-b03bd4de569c","360":"0bcbc417-e47a-4666-8aef-caaa30be74f3","361":"df996143-417e-4db7-8b38-7c1154d13306","362":"51f77b54-c277-45e9-bd3d-ed0057980cea","363":"b6330e8d-df33-4581-947a-40cbce836872","364":"eedef14d-e113-4247-bfff-6f07bf4e2db4","365":"e7ff08b6-cc36-4f9f-859a-0266ba2f73c8","366":"da2c0a89-286c-4525-85c6-dd726409e48a","367":"354b0f0d-7cc8-46f0-853b-0ed58d92afa5","368":"191e54d9-05fa-4868-a200-ce3ef1ac5d39","369":"2fc12f45-8cb5-4ccb-aefe-a0e9a1dfcf34","370":"ea22df7a-d35c-4f50-a62a-185ef55cb920","371":"182660cd-86f2-4c54-bff5-1959c8a527c5","372":"f54880f5-73bd-4bab-b077-02ddf63438be","373":"7ed99a25-ea97-492f-8ffa-c66f05de9496","374":"12ed7a05-dc45-4922-b03e-135a446ba9b6","375":"3d18d793-317e-4797-91d0-b9b0f8bdaccd","376":"16dae716-1fd5-4b2c-89fc-25b0e0041cc6","377":"b6cd217d-bba7-4f5d-ad90-879747ad2bc2"}]